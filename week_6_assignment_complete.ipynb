{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week_6_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOPzWVv/G9TXuYPtcOvrKD6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranay8297/deep-learning-projects/blob/master/week_6_assignment_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook Walkthrough"
      ],
      "metadata": {
        "id": "B0kn5iUb3Du-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT7r3o4q2AP3",
        "outputId": "1b855a78-800e-4127-ed09-06d1cfd49bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('brown')  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "brown.words() # Returns a list of strings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWVg6fIE2D8G",
        "outputId": "bd2edf4a-e3c5-4426-98e3-358fd831910e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClcVZ6kG9s6z",
        "outputId": "6f8b6020-975d-4d98-9730-83862b75c97f"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(gutenberg.sents()), gutenberg.sents()[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i37kenHb2HnH",
        "outputId": "5ba26859-430e-47f4-fb5d-b90c7d3fe191"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98552, ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'])"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gutenberg.fileids()[:4], len(gutenberg.fileids())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpM6dMpe2mTn",
        "outputId": "980d32e0-9519-4be8-8e9e-a703f19847ec"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['austen-emma.txt',\n",
              "  'austen-persuasion.txt',\n",
              "  'austen-sense.txt',\n",
              "  'bible-kjv.txt'],\n",
              " 18)"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gutenberg.sents(fileids='austen-emma.txt') , len(gutenberg.sents(fileids='austen-emma.txt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrODk8yU2TFl",
        "outputId": "bc24d349-bc15-4e53-8252-76e1cc286354"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'], ['VOLUME', 'I'], ...],\n",
              " 7752)"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gutenberg.raw('bible-kjv.txt')[:400]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "qk7ojZnm21j7",
        "outputId": "0d3f56ca-c1d1-4e0a-ff18-33999b2628ae"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[The King James Bible]\\n\\nThe Old Testament of the King James Bible\\n\\nThe First Book of Moses:  Called Genesis\\n\\n\\n1:1 In the beginning God created the heaven and the earth.\\n\\n1:2 And the earth was without form, and void; and darkness was upon\\nthe face of the deep. And the Spirit of God moved upon the face of the\\nwaters.\\n\\n1:3 And God said, Let there be light: and there was light.\\n\\n1:4 And God saw the li'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = gutenberg.raw('bible-kjv.txt')\n",
        "sentences = x.split('\\n\\n')\n",
        "sentences[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vu1NKnH3ldO",
        "outputId": "b1e72f81-489a-45bd-b504-06e48a70955f"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[The King James Bible]',\n",
              " 'The Old Testament of the King James Bible',\n",
              " 'The First Book of Moses:  Called Genesis',\n",
              " '\\n1:1 In the beginning God created the heaven and the earth.',\n",
              " '1:2 And the earth was without form, and void; and darkness was upon\\nthe face of the deep. And the Spirit of God moved upon the face of the\\nwaters.',\n",
              " '1:3 And God said, Let there be light: and there was light.',\n",
              " '1:4 And God saw the light, that it was good: and God divided the light\\nfrom the darkness.',\n",
              " '1:5 And God called the light Day, and the darkness he called Night.\\nAnd the evening and the morning were the first day.',\n",
              " '1:6 And God said, Let there be a firmament in the midst of the waters,\\nand let it divide the waters from the waters.',\n",
              " '1:7 And God made the firmament, and divided the waters which were\\nunder the firmament from the waters which were above the firmament:\\nand it was so.',\n",
              " '1:8 And God called the firmament Heaven. And the evening and the\\nmorning were the second day.',\n",
              " '1:9 And God said, Let the waters under the heaven be gathered together\\nunto one place, and let the dry land appear: and it was so.',\n",
              " '1:10 And God called the dry land Earth; and the gathering together of\\nthe waters called he Seas: and God saw that it was good.',\n",
              " '1:11 And God said, Let the earth bring forth grass, the herb yielding\\nseed, and the fruit tree yielding fruit after his kind, whose seed is\\nin itself, upon the earth: and it was so.',\n",
              " '1:12 And the earth brought forth grass, and herb yielding seed after\\nhis kind, and the tree yielding fruit, whose seed was in itself, after\\nhis kind: and God saw that it was good.',\n",
              " '1:13 And the evening and the morning were the third day.',\n",
              " '1:14 And God said, Let there be lights in the firmament of the heaven\\nto divide the day from the night; and let them be for signs, and for\\nseasons, and for days, and years: 1:15 And let them be for lights in\\nthe firmament of the heaven to give light upon the earth: and it was\\nso.',\n",
              " '1:16 And God made two great lights; the greater light to rule the day,\\nand the lesser light to rule the night: he made the stars also.',\n",
              " '1:17 And God set them in the firmament of the heaven to give light\\nupon the earth, 1:18 And to rule over the day and over the night, and\\nto divide the light from the darkness: and God saw that it was good.',\n",
              " '1:19 And the evening and the morning were the fourth day.']"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s8 = sentences[16]\n",
        "s8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "E7jJ_k-j3kk-",
        "outputId": "5d8aa13b-c479-464d-b24c-ca1a9e5d036d"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1:14 And God said, Let there be lights in the firmament of the heaven\\nto divide the day from the night; and let them be for signs, and for\\nseasons, and for days, and years: 1:15 And let them be for lights in\\nthe firmament of the heaven to give light upon the earth: and it was\\nso.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2qmoQgT32Et",
        "outputId": "15f69c9c-34e1-4066-8132-832f16fe21f8"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(s8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1EArId133K8",
        "outputId": "c42f5f10-5a5f-41ab-e863-f9f27d1915d4"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1:14 And God said, Let there be lights in the firmament of the heaven\\nto divide the day from the night; and let them be for signs, and for\\nseasons, and for days, and years: 1:15 And let them be for lights in\\nthe firmament of the heaven to give light upon the earth: and it was\\nso.']"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in sent_tokenize(s8):\n",
        "    print(word_tokenize(sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIT0ByAD4M_R",
        "outputId": "7cefbc07-434f-4c9e-fb09-00ee04197cf3"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1:14', 'And', 'God', 'said', ',', 'Let', 'there', 'be', 'lights', 'in', 'the', 'firmament', 'of', 'the', 'heaven', 'to', 'divide', 'the', 'day', 'from', 'the', 'night', ';', 'and', 'let', 'them', 'be', 'for', 'signs', ',', 'and', 'for', 'seasons', ',', 'and', 'for', 'days', ',', 'and', 'years', ':', '1:15', 'And', 'let', 'them', 'be', 'for', 'lights', 'in', 'the', 'firmament', 'of', 'the', 'heaven', 'to', 'give', 'light', 'upon', 'the', 'earth', ':', 'and', 'it', 'was', 'so', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in sent_tokenize(s8):\n",
        "    # It's a little in efficient to loop through each word,\n",
        "    # after but sometimes it helps to get better tokens.\n",
        "    print([word.lower() for word in word_tokenize(sent)])\n",
        "    # Alternatively:\n",
        "    #print(list(map(str.lower, word_tokenize(sent))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDHgH1n64Tng",
        "outputId": "657e10ea-20ff-4cd8-8ba6-46dcb8296ac7"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1:14', 'and', 'god', 'said', ',', 'let', 'there', 'be', 'lights', 'in', 'the', 'firmament', 'of', 'the', 'heaven', 'to', 'divide', 'the', 'day', 'from', 'the', 'night', ';', 'and', 'let', 'them', 'be', 'for', 'signs', ',', 'and', 'for', 'seasons', ',', 'and', 'for', 'days', ',', 'and', 'years', ':', '1:15', 'and', 'let', 'them', 'be', 'for', 'lights', 'in', 'the', 'firmament', 'of', 'the', 'heaven', 'to', 'give', 'light', 'upon', 'the', 'earth', ':', 'and', 'it', 'was', 'so', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(s8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKTaXotQ4cg0",
        "outputId": "abf959b8-bed3-426d-8906-e92312ba9172"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1:14', 'And', 'God', 'said', ',', 'Let', 'there', 'be', 'lights', 'in', 'the', 'firmament', 'of', 'the', 'heaven', 'to', 'divide', 'the', 'day', 'from', 'the', 'night', ';', 'and', 'let', 'them', 'be', 'for', 'signs', ',', 'and', 'for', 'seasons', ',', 'and', 'for', 'days', ',', 'and', 'years', ':', '1:15', 'And', 'let', 'them', 'be', 'for', 'lights', 'in', 'the', 'firmament', 'of', 'the', 'heaven', 'to', 'give', 'light', 'upon', 'the', 'earth', ':', 'and', 'it', 'was', 'so', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "single_no9 = webtext.raw('singles.txt').split('\\n')[17]\n",
        "sent_tokenize(single_no9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YulC41V4oQ9",
        "outputId": "5e9e3b9c-25ab-4039-a88c-0e31ea391eb4"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BLONDE HAIR, BLUE EYES Medium build, Im in my early 30s.',\n",
              " 'Am honest, caring, likes fishing, animals, golf, bike riding, TV and DVDs, quiet nights at home, kissing and cuddling up to a special person in my life.',\n",
              " 'Looking for a caring, honest lady for friendship to relationship.']"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG6nA1vf4wnq",
        "outputId": "c76bb58a-bf13-4fd2-ea92-80e55d72188e"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords_en = stopwords.words('english')\n",
        "print(stopwords_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bnDR2_c4rsr",
        "outputId": "5e99e82f-5805-4d01-bbd6-dd5a5f663a72"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "single_no8_tokenized_lowered = list(map(str.lower, word_tokenize(s8)))\n",
        "print(single_no8_tokenized_lowered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U3B3qiJ44d9",
        "outputId": "fcd45702-c66f-4f70-e0a3-04bb168423e4"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1:14', 'and', 'god', 'said', ',', 'let', 'there', 'be', 'lights', 'in', 'the', 'firmament', 'of', 'the', 'heaven', 'to', 'divide', 'the', 'day', 'from', 'the', 'night', ';', 'and', 'let', 'them', 'be', 'for', 'signs', ',', 'and', 'for', 'seasons', ',', 'and', 'for', 'days', ',', 'and', 'years', ':', '1:15', 'and', 'let', 'them', 'be', 'for', 'lights', 'in', 'the', 'firmament', 'of', 'the', 'heaven', 'to', 'give', 'light', 'upon', 'the', 'earth', ':', 'and', 'it', 'was', 'so', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_en = set(stopwords.words('english')) # Set checking is faster in Python than list.\n",
        "\n",
        "# List comprehension.\n",
        "print([word for word in single_no8_tokenized_lowered if word not in stopwords_en])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5zN_n1247mI",
        "outputId": "d7d9ca22-30bd-479e-f8ea-86eea0ed0cdf"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1:14', 'god', 'said', ',', 'let', 'lights', 'firmament', 'heaven', 'divide', 'day', 'night', ';', 'let', 'signs', ',', 'seasons', ',', 'days', ',', 'years', ':', '1:15', 'let', 'lights', 'firmament', 'heaven', 'give', 'light', 'upon', 'earth', ':', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "# It's a string so we have to them into a set type\n",
        "print('From string.punctuation:', type(punctuation), punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ytD6bqR5AvM",
        "outputId": "a05f051f-5b74-467a-ae21-0ebb4321b31c"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From string.punctuation: <class 'str'> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_en_withpunct = stopwords_en.union(set(punctuation))\n",
        "print(stopwords_en_withpunct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9gGcMn15DiP",
        "outputId": "a7beb339-d211-4ea6-9d65-ad2f41b5cfb7"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'few', \"mightn't\", \"'\", 'those', 'or', 'there', 'her', 'too', 'very', \"shouldn't\", '[', 'against', \"aren't\", 'haven', \"wouldn't\", 'to', 'myself', 'he', 'doing', '@', 'some', 'here', 'wasn', 'other', 'out', '.', 'which', 'll', \"she's\", 'itself', \"doesn't\", 'most', 'than', 'don', 'are', '+', 'and', 'down', 'mightn', ':', 'who', \"isn't\", 'after', \"hadn't\", 'won', 'did', 'am', 'how', '$', 'ours', 'ourselves', 'was', 'they', 'can', 'these', \"that'll\", 'so', '%', 'aren', 'has', \"haven't\", \"it's\", 'should', 'you', 'as', '/', 'their', 'just', 'now', 'does', 've', '&', 'same', 'the', 'with', 's', 'into', 'had', 'in', \"you're\", 'for', \"mustn't\", ';', 'couldn', 'didn', '|', '#', '(', 'o', '}', 'more', \"shan't\", 'yourselves', 'during', 'all', ')', 'whom', 'yours', 'this', ',', 'of', 'that', 'hasn', 'himself', 'needn', 'up', 'once', 'wouldn', 'will', 'hers', 'each', \"you'd\", 'shan', 'off', '?', '\\\\', 'but', 'themselves', \"didn't\", 'have', 'shouldn', 'where', 'our', 'were', 'again', 'is', '`', \"wasn't\", 'if', 'through', 'any', 'we', '<', 'between', 'she', 'your', 'weren', 'above', \"should've\", 'while', 'ain', 'over', 'before', \"don't\", 'when', 'd', '>', '{', 'do', 'not', \"won't\", 'having', 'by', 'no', ']', 'below', 'me', 'then', 'under', 'theirs', '^', 'only', '!', 'until', 'such', 'its', 'isn', 'herself', 'why', '_', 'my', 'been', 'yourself', 'y', 'being', 'nor', 't', \"needn't\", 'i', \"you'll\", 'further', '\"', '*', 'm', 'doesn', '-', 'from', 'about', 'own', 'a', 'them', 're', 'it', 'because', 'what', 'both', \"couldn't\", '=', '~', 'hadn', 'mustn', 'his', 'on', \"you've\", \"weren't\", 'be', 'at', 'him', \"hasn't\", 'ma', 'an'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s8_clean_initail = print([word for word in single_no8_tokenized_lowered if word not in stopwords_en_withpunct])\n",
        "s8_clean_initail"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2535bRvK5EFM",
        "outputId": "6e8818cd-55b1-4ff4-dadd-e4feb0cf8f8d"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1:14', 'god', 'said', 'let', 'lights', 'firmament', 'heaven', 'divide', 'day', 'night', 'let', 'signs', 'seasons', 'days', 'years', '1:15', 'let', 'lights', 'firmament', 'heaven', 'give', 'light', 'upon', 'earth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
        "stopwords_json_en = set(stopwords_json['en'])\n",
        "stopwords_nltk_en = set(stopwords.words('english'))\n",
        "stopwords_punct = set(punctuation)\n",
        "# Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
        "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)\n",
        "\n",
        "# Remove the stopwords from `single_no8`.\n",
        "print('With combined stopwords:')\n",
        "print([word for word in single_no8_tokenized_lowered if word not in stoplist_combined])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sE1U1FR5R5z",
        "outputId": "0fdf505f-e3bc-4341-cf69-02510b1b3f3a"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With combined stopwords:\n",
            "['1:14', 'god', 'lights', 'firmament', 'heaven', 'divide', 'day', 'night', 'signs', 'seasons', 'days', 'years', '1:15', 'lights', 'firmament', 'heaven', 'give', 'light', 'earth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FUA2iI-5rvZ",
        "outputId": "68472d85-fde5-487c-8ace-54e9aeb9c83e"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "for word in ['walking', 'walks', 'walked']:\n",
        "    print(porter.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHkaWXeD5kvi",
        "outputId": "a5159f46-18d1-4e2b-959a-5ac31e49fc4e"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walk\n",
            "walk\n",
            "walk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "for word in ['walking', 'walks', 'walked']:\n",
        "    print(wnl.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKf9XcMJ5ok7",
        "outputId": "c995581a-bf5f-4aee-f4dc-a5357c6c274d"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walking\n",
            "walk\n",
            "walked\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def penn2morphy(penntag):\n",
        "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
        "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
        "                  'VB':'v', 'RB':'r'}\n",
        "    try:\n",
        "        return morphy_tag[penntag[:2]]\n",
        "    except:\n",
        "        return 'n' # if mapping isn't found, fall back to Noun.\n",
        "    \n",
        "# `pos_tag` takes the tokenized sentence as input, i.e. list of string,\n",
        "# and returns a tuple of (word, tg), i.e. list of tuples of strings\n",
        "# so we need to get the tag from the 2nd element.\n",
        "\n",
        "walking_tagged = pos_tag(word_tokenize(s8))\n",
        "print(walking_tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-_A6o9j5zm4",
        "outputId": "5cf9c6d1-65c1-46ff-bdf7-858906247cfd"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('1:14', 'CD'), ('And', 'CC'), ('God', 'NNP'), ('said', 'VBD'), (',', ','), ('Let', 'VB'), ('there', 'EX'), ('be', 'VB'), ('lights', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('firmament', 'NN'), ('of', 'IN'), ('the', 'DT'), ('heaven', 'NN'), ('to', 'TO'), ('divide', 'VB'), ('the', 'DT'), ('day', 'NN'), ('from', 'IN'), ('the', 'DT'), ('night', 'NN'), (';', ':'), ('and', 'CC'), ('let', 'VB'), ('them', 'PRP'), ('be', 'VB'), ('for', 'IN'), ('signs', 'NNS'), (',', ','), ('and', 'CC'), ('for', 'IN'), ('seasons', 'NNS'), (',', ','), ('and', 'CC'), ('for', 'IN'), ('days', 'NNS'), (',', ','), ('and', 'CC'), ('years', 'NNS'), (':', ':'), ('1:15', 'CD'), ('And', 'CC'), ('let', 'VB'), ('them', 'PRP'), ('be', 'VB'), ('for', 'IN'), ('lights', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('firmament', 'NN'), ('of', 'IN'), ('the', 'DT'), ('heaven', 'NN'), ('to', 'TO'), ('give', 'VB'), ('light', 'JJ'), ('upon', 'IN'), ('the', 'DT'), ('earth', 'NN'), (':', ':'), ('and', 'CC'), ('it', 'PRP'), ('was', 'VBD'), ('so', 'RB'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in walking_tagged])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qS_1D1058iv",
        "outputId": "6adf1f8b-34d7-4f4e-ea79-6d7d4641b1ec"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1:14', 'and', 'god', 'say', ',', 'let', 'there', 'be', 'light', 'in', 'the', 'firmament', 'of', 'the', 'heaven', 'to', 'divide', 'the', 'day', 'from', 'the', 'night', ';', 'and', 'let', 'them', 'be', 'for', 'sign', ',', 'and', 'for', 'season', ',', 'and', 'for', 'day', ',', 'and', 'year', ':', '1:15', 'and', 'let', 'them', 'be', 'for', 'light', 'in', 'the', 'firmament', 'of', 'the', 'heaven', 'to', 'give', 'light', 'upon', 'the', 'earth', ':', 'and', 'it', 'be', 'so', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def penn2morphy(penntag):\n",
        "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
        "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
        "                  'VB':'v', 'RB':'r'}\n",
        "    try:\n",
        "        return morphy_tag[penntag[:2]]\n",
        "    except:\n",
        "        return 'n' \n",
        "    \n",
        "def lemmatize_sent(text): \n",
        "    # Text input is string, returns lowercased strings.\n",
        "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
        "            for word, tag in pos_tag(word_tokenize(text))]\n",
        "\n",
        "print(lemmatize_sent(s8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07pQXjrX5-dQ",
        "outputId": "07f77957-bac8-4a4b-a7e1-8bc02de0e798"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1:14', 'and', 'god', 'say', ',', 'let', 'there', 'be', 'light', 'in', 'the', 'firmament', 'of', 'the', 'heaven', 'to', 'divide', 'the', 'day', 'from', 'the', 'night', ';', 'and', 'let', 'them', 'be', 'for', 'sign', ',', 'and', 'for', 'season', ',', 'and', 'for', 'day', ',', 'and', 'year', ':', '1:15', 'and', 'let', 'them', 'be', 'for', 'light', 'in', 'the', 'firmament', 'of', 'the', 'heaven', 'to', 'give', 'light', 'upon', 'the', 'earth', ':', 'and', 'it', 'be', 'so', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Original Single no. 8:')\n",
        "print(s8, '\\n')\n",
        "print('Lemmatized and removed stopwords:')\n",
        "print([word for word in lemmatize_sent(s8) \n",
        "       if word not in stoplist_combined\n",
        "       and not word.isdigit() ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Alqo49c16Hmj",
        "outputId": "f829abb7-f3d3-40ae-8b67-7db5f46d2697"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Single no. 8:\n",
            "1:14 And God said, Let there be lights in the firmament of the heaven\n",
            "to divide the day from the night; and let them be for signs, and for\n",
            "seasons, and for days, and years: 1:15 And let them be for lights in\n",
            "the firmament of the heaven to give light upon the earth: and it was\n",
            "so. \n",
            "\n",
            "Lemmatized and removed stopwords:\n",
            "['1:14', 'god', 'light', 'firmament', 'heaven', 'divide', 'day', 'night', 'sign', 'season', 'day', 'year', '1:15', 'light', 'firmament', 'heaven', 'give', 'light', 'earth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Input: str, i.e. document/sentence\n",
        "    # Output: list(str) , i.e. list of lemmas\n",
        "    return [word for word in lemmatize_sent(text) \n",
        "            if word not in stoplist_combined\n",
        "            and not word.isdigit()]"
      ],
      "metadata": {
        "id": "MjFgXNJA6RGt"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Lemmatize and remove stopwords\n",
        "processed_sent1 = preprocess_text(s8)\n",
        "processed_sent2 = preprocess_text(single_no9)"
      ],
      "metadata": {
        "id": "JxalqdW46Vy5"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(processed_sent1, '\\n', processed_sent2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSaBE7na6aS5",
        "outputId": "8e9f71e8-6c97-48cb-ba18-b260c5fb4081"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1:14', 'god', 'light', 'firmament', 'heaven', 'divide', 'day', 'night', 'sign', 'season', 'day', 'year', '1:15', 'light', 'firmament', 'heaven', 'give', 'light', 'earth'] \n",
            " ['blonde', 'hair', 'blue', 'eye', 'medium', 'build', 'im', 'early', 'honest', 'care', 'fishing', 'animal', 'golf', 'bike', 'riding', 'tv', 'dvd', 'quiet', 'night', 'home', 'kiss', 'cuddle', 'special', 'person', 'life', 'caring', 'honest', 'lady', 'friendship', 'relationship']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "with StringIO('\\n'.join([s8, single_no9])) as fin:\n",
        "    # Create the vectorizer\n",
        "    count_vect = CountVectorizer()\n",
        "    count_vect.fit_transform(fin)"
      ],
      "metadata": {
        "id": "viIUbPSH6ivX"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect.vocabulary_"
      ],
      "metadata": {
        "id": "sdv42GLX6k74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "with StringIO('\\n'.join([s8, single_no9])) as fin:\n",
        "    # Override the analyzer totally with our preprocess text\n",
        "    count_vect = CountVectorizer(stop_words=stoplist_combined,\n",
        "                                 tokenizer=word_tokenize)\n",
        "    count_vect.fit_transform(fin)\n",
        "print(count_vect.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QvuaO_27JOb",
        "outputId": "2b153378-6eed-48a7-9e7c-db4f9a67c886"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'1:14': 0, 'god': 21, 'lights': 32, 'firmament': 17, 'heaven': 24, 'divide': 12, 'day': 10, 'night': 35, 'signs': 42, 'seasons': 41, 'days': 11, 'years': 45, '1:15': 1, 'give': 20, 'light': 31, 'earth': 15, 'blonde': 5, 'hair': 23, 'blue': 6, 'eyes': 16, 'medium': 34, 'build': 7, 'im': 27, 'early': 14, '30s': 2, 'honest': 26, 'caring': 8, 'likes': 33, 'fishing': 18, 'animals': 3, 'golf': 22, 'bike': 4, 'riding': 40, 'tv': 44, 'dvds': 13, 'quiet': 38, 'nights': 36, 'home': 25, 'kissing': 28, 'cuddling': 9, 'special': 43, 'person': 37, 'life': 30, 'lady': 29, 'friendship': 19, 'relationship': 39}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", '``', 'ai', 'ca', \"n't\", 'sha', 'wo'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "with StringIO('\\n'.join([s8, single_no9])) as fin:\n",
        "    # Override the analyzer totally with our preprocess text\n",
        "    count_vect = CountVectorizer(analyzer=preprocess_text)\n",
        "    count_vect.fit_transform(fin)\n",
        "print(count_vect.vocabulary_ )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzfkGaLt7K-A",
        "outputId": "e9567e69-47b2-405f-b30f-93b3dd93cfdd"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'1:14': 0, 'god': 20, 'light': 30, 'firmament': 16, 'heaven': 23, 'divide': 11, 'day': 10, 'night': 32, 'sign': 38, 'season': 37, 'year': 41, '1:15': 1, 'give': 19, 'earth': 14, 'blonde': 4, 'hair': 22, 'blue': 5, 'eye': 15, 'medium': 31, 'build': 6, 'im': 26, 'early': 13, 'honest': 25, 'care': 7, 'fishing': 17, 'animal': 2, 'golf': 21, 'bike': 3, 'riding': 36, 'tv': 40, 'dvd': 12, 'quiet': 34, 'home': 24, 'kiss': 27, 'cuddle': 9, 'special': 39, 'person': 33, 'life': 29, 'caring': 8, 'lady': 28, 'friendship': 18, 'relationship': 35}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect.transform([s8, single_no9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOMRWtHz7NSG",
        "outputId": "6d9156dd-68c8-4c12-a63c-7f77cdf7d278"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2x42 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 43 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Walkthrough of Pizza dataset. \n",
        "My implementation of IMDB classification is further down, after this pizza classification"
      ],
      "metadata": {
        "id": "5DK9UIbkFBUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/marycboardman/Random-Acts-of-Pizza/master/train.json\n",
        "!wget https://raw.githubusercontent.com/marycboardman/Random-Acts-of-Pizza/master/test.json\n",
        "!wget https://raw.githubusercontent.com/marycboardman/Random-Acts-of-Pizza/master/sampleSubmission.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvRWtogy7pEa",
        "outputId": "cb470d96-cb5e-469f-a136-6ed447d60a4f"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-03 01:06:35--  https://raw.githubusercontent.com/marycboardman/Random-Acts-of-Pizza/master/train.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12569177 (12M) [text/plain]\n",
            "Saving to: train.json.2\n",
            "\n",
            "train.json.2        100%[===================>]  11.99M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2022-04-03 01:06:35 (138 MB/s) - train.json.2 saved [12569177/12569177]\n",
            "\n",
            "--2022-04-03 01:06:35--  https://raw.githubusercontent.com/marycboardman/Random-Acts-of-Pizza/master/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3019717 (2.9M) [text/plain]\n",
            "Saving to: test.json.1\n",
            "\n",
            "test.json.1         100%[===================>]   2.88M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-04-03 01:06:35 (58.3 MB/s) - test.json.1 saved [3019717/3019717]\n",
            "\n",
            "--2022-04-03 01:06:36--  https://raw.githubusercontent.com/marycboardman/Random-Acts-of-Pizza/master/sampleSubmission.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2022-04-03 01:06:36 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq5zeGuU8I9y",
        "outputId": "1368fa8e-84f2-49af-b19b-f0a9810b439e"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  test.json\ttest.json.1  train.json  train.json.1  train.json.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('./train.json') as fin:\n",
        "    trainjson = json.load(fin)\n"
      ],
      "metadata": {
        "id": "CpBpfHea7zzs"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainjson[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSsIN_6z8PA5",
        "outputId": "89ef9fb4-b8d6-4aa8-ff4e-f2fcd3e08e23"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'giver_username_if_known': 'N/A',\n",
              " 'number_of_downvotes_of_request_at_retrieval': 0,\n",
              " 'number_of_upvotes_of_request_at_retrieval': 1,\n",
              " 'post_was_edited': False,\n",
              " 'request_id': 't3_l25d7',\n",
              " 'request_number_of_comments_at_retrieval': 0,\n",
              " 'request_text': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
              " 'request_text_edit_aware': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
              " 'request_title': 'Request Colorado Springs Help Us Please',\n",
              " 'requester_account_age_in_days_at_request': 0.0,\n",
              " 'requester_account_age_in_days_at_retrieval': 792.4204050925925,\n",
              " 'requester_days_since_first_post_on_raop_at_request': 0.0,\n",
              " 'requester_days_since_first_post_on_raop_at_retrieval': 792.4204050925925,\n",
              " 'requester_number_of_comments_at_request': 0,\n",
              " 'requester_number_of_comments_at_retrieval': 0,\n",
              " 'requester_number_of_comments_in_raop_at_request': 0,\n",
              " 'requester_number_of_comments_in_raop_at_retrieval': 0,\n",
              " 'requester_number_of_posts_at_request': 0,\n",
              " 'requester_number_of_posts_at_retrieval': 1,\n",
              " 'requester_number_of_posts_on_raop_at_request': 0,\n",
              " 'requester_number_of_posts_on_raop_at_retrieval': 1,\n",
              " 'requester_number_of_subreddits_at_request': 0,\n",
              " 'requester_received_pizza': False,\n",
              " 'requester_subreddits_at_request': [],\n",
              " 'requester_upvotes_minus_downvotes_at_request': 0,\n",
              " 'requester_upvotes_minus_downvotes_at_retrieval': 1,\n",
              " 'requester_upvotes_plus_downvotes_at_request': 0,\n",
              " 'requester_upvotes_plus_downvotes_at_retrieval': 1,\n",
              " 'requester_user_flair': None,\n",
              " 'requester_username': 'nickylvst',\n",
              " 'unix_timestamp_of_request': 1317852607.0,\n",
              " 'unix_timestamp_of_request_utc': 1317849007.0}"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('UID:\\t', trainjson[0]['request_id'], '\\n')\n",
        "print('Title:\\t', trainjson[0]['request_title'], '\\n')\n",
        "print('Text:\\t', trainjson[0]['request_text_edit_aware'], '\\n')\n",
        "print('Tag:\\t', trainjson[0]['requester_received_pizza'], end='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy9Ocv7B8S2U",
        "outputId": "48f8212d-337a-45c5-c611-ce80a10646f2"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UID:\t t3_l25d7 \n",
            "\n",
            "Title:\t Request Colorado Springs Help Us Please \n",
            "\n",
            "Text:\t Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated \n",
            "\n",
            "Tag:\t False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.io.json.json_normalize(trainjson) # Pandas magic... \n",
        "df_train = df[['request_id', 'request_title', \n",
        "               'request_text_edit_aware', \n",
        "               'requester_received_pizza']]\n",
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "dO3EJ_lv8YPo",
        "outputId": "aedf6650-f1cd-476a-a42c-68047d46334e"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  request_id                                      request_title  \\\n",
              "0   t3_l25d7            Request Colorado Springs Help Us Please   \n",
              "1   t3_rcb83  [Request] California, No cash and I could use ...   \n",
              "2   t3_lpu5j  [Request] Hungry couple in Dundee, Scotland wo...   \n",
              "3   t3_mxvj3  [Request] In Canada (Ontario), just got home f...   \n",
              "4  t3_1i6486  [Request] Old friend coming to visit. Would LO...   \n",
              "\n",
              "                             request_text_edit_aware  requester_received_pizza  \n",
              "0  Hi I am in need of food for my 4 children we a...                     False  \n",
              "1  I spent the last money I had on gas today. Im ...                     False  \n",
              "2  My girlfriend decided it would be a good idea ...                     False  \n",
              "3  It's cold, I'n hungry, and to be completely ho...                     False  \n",
              "4  hey guys:\\n I love this sub. I think it's grea...                     False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ad95f7d-2350-43d2-9454-4fdb292e1321\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>request_id</th>\n",
              "      <th>request_title</th>\n",
              "      <th>request_text_edit_aware</th>\n",
              "      <th>requester_received_pizza</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>t3_l25d7</td>\n",
              "      <td>Request Colorado Springs Help Us Please</td>\n",
              "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>t3_rcb83</td>\n",
              "      <td>[Request] California, No cash and I could use ...</td>\n",
              "      <td>I spent the last money I had on gas today. Im ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>t3_lpu5j</td>\n",
              "      <td>[Request] Hungry couple in Dundee, Scotland wo...</td>\n",
              "      <td>My girlfriend decided it would be a good idea ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>t3_mxvj3</td>\n",
              "      <td>[Request] In Canada (Ontario), just got home f...</td>\n",
              "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>t3_1i6486</td>\n",
              "      <td>[Request] Old friend coming to visit. Would LO...</td>\n",
              "      <td>hey guys:\\n I love this sub. I think it's grea...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ad95f7d-2350-43d2-9454-4fdb292e1321')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8ad95f7d-2350-43d2-9454-4fdb292e1321 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8ad95f7d-2350-43d2-9454-4fdb292e1321');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('./test.json') as fin:\n",
        "    testjson = json.load(fin)"
      ],
      "metadata": {
        "id": "EkPAb4-08t7F"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('UID:\\t', testjson[0]['request_id'], '\\n')\n",
        "print('Title:\\t', testjson[0]['request_title'], '\\n')\n",
        "print('Text:\\t', testjson[0]['request_text_edit_aware'], '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO-IUmUq80yO",
        "outputId": "b6214dc6-2457-4784-a0c1-2774ad1f296d"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UID:\t t3_i8iy4 \n",
            "\n",
            "Title:\t [request] pregger gf 95 degree house and no food.. promise to pay it forward! Northern Colorado \n",
            "\n",
            "Text:\t Hey all! It's about 95 degrees here and our kitchen is pretty much empty save for some bread and cereal.  My girlfriend/fiance is 8 1/2 months pregnant and we could use a good meal.  We promise to pay it forward when we get money! Thanks so much in advance! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.io.json.json_normalize(testjson)\n",
        "df_test = df[['request_id', 'request_title', \n",
        "               'request_text_edit_aware']]\n",
        "df_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "_hg-bUTm83Go",
        "outputId": "630e2329-0a1f-45ab-a628-dc6f9d338929"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  request_id                                      request_title  \\\n",
              "0   t3_i8iy4  [request] pregger gf 95 degree house and no fo...   \n",
              "1  t3_1mfqi0  [Request] Lost my job day after labour day, st...   \n",
              "2   t3_lclka                (Request) pizza for my kids please?   \n",
              "3  t3_1jdgdj  [Request] Just moved to a new state(Waltham MA...   \n",
              "4   t3_t2qt4  [Request] Two girls in between paychecks, we'v...   \n",
              "\n",
              "                             request_text_edit_aware  \n",
              "0  Hey all! It's about 95 degrees here and our ki...  \n",
              "1  I didn't know a place like this exists! \\n\\nI ...  \n",
              "2  Hi Reddit. Im a single dad having a really rou...  \n",
              "3  Hi I just moved to Waltham MA from my home sta...  \n",
              "4  We're just sitting here near indianapolis on o...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-81f7eb4d-1496-4e36-9efe-203bd6182e85\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>request_id</th>\n",
              "      <th>request_title</th>\n",
              "      <th>request_text_edit_aware</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>t3_i8iy4</td>\n",
              "      <td>[request] pregger gf 95 degree house and no fo...</td>\n",
              "      <td>Hey all! It's about 95 degrees here and our ki...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>t3_1mfqi0</td>\n",
              "      <td>[Request] Lost my job day after labour day, st...</td>\n",
              "      <td>I didn't know a place like this exists! \\n\\nI ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>t3_lclka</td>\n",
              "      <td>(Request) pizza for my kids please?</td>\n",
              "      <td>Hi Reddit. Im a single dad having a really rou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>t3_1jdgdj</td>\n",
              "      <td>[Request] Just moved to a new state(Waltham MA...</td>\n",
              "      <td>Hi I just moved to Waltham MA from my home sta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>t3_t2qt4</td>\n",
              "      <td>[Request] Two girls in between paychecks, we'v...</td>\n",
              "      <td>We're just sitting here near indianapolis on o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81f7eb4d-1496-4e36-9efe-203bd6182e85')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-81f7eb4d-1496-4e36-9efe-203bd6182e85 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-81f7eb4d-1496-4e36-9efe-203bd6182e85');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "# It doesn't really matter what the function name is called\n",
        "# but the `train_test_split` is splitting up the data into \n",
        "# 2 parts according to the `test_size` argument you've set.\n",
        "\n",
        "# When we're splitting up the training data, we're spltting up \n",
        "# into train, valid split. The function name is just a name =)\n",
        "train, valid = train_test_split(df_train, test_size=0.2)"
      ],
      "metadata": {
        "id": "Y-L4JUpO8fyd"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the vectorizer and \n",
        "# override the analyzer totally with the preprocess_text().\n",
        "# Note: the vectorizer is just an 'empty' object now.\n",
        "count_vect = CountVectorizer(analyzer=preprocess_text)\n",
        "\n",
        "# When we use `CounterVectorizer.fit_transform`,\n",
        "# we essentially create the dictionary and \n",
        "# vectorize our input text at the same time.\n",
        "train_set = count_vect.fit_transform(train['request_text_edit_aware'])\n",
        "train_tags = train['requester_received_pizza']\n",
        "\n",
        "# When vectorizing the validation data, we use `CountVectorizer.transform()`.\n",
        "valid_set = count_vect.transform(valid['request_text_edit_aware'])\n",
        "valid_tags = valid['requester_received_pizza']"
      ],
      "metadata": {
        "id": "YLLuVht68hUn"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = count_vect.transform(df_test['request_text_edit_aware'])\n"
      ],
      "metadata": {
        "id": "99qCvWZJ87Pe"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB() \n",
        "\n",
        "# To train the classifier, simple do \n",
        "clf.fit(train_set, train_tags) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dhaao_l88xf",
        "outputId": "60a97cdf-8c98-465a-8229-53821c3a2bd0"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# To predict our tags (i.e. whether requesters get their pizza), \n",
        "# we feed the vectorized `test_set` to .predict()\n",
        "predictions_valid = clf.predict(valid_set)\n",
        "\n",
        "print('Pizza reception accuracy = {}'.format(\n",
        "        accuracy_score(predictions_valid, valid_tags) * 100)\n",
        "     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_hCiKxX8_i7",
        "outputId": "5af49754-83aa-4085-bea5-f89780d349a4"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pizza reception accuracy = 72.52475247524752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect = CountVectorizer(analyzer=preprocess_text)\n",
        "\n",
        "full_train_set = count_vect.fit_transform(df_train['request_text_edit_aware'])\n",
        "full_tags = df_train['requester_received_pizza']\n",
        "\n",
        "# Note: We have to re-vectorize the test set since\n",
        "#       now our vectorizer is different using the full \n",
        "#       training set.\n",
        "test_set = count_vect.transform(df_test['request_text_edit_aware'])\n",
        "\n",
        "# To train the classifier\n",
        "clf = MultinomialNB() \n",
        "clf.fit(full_train_set, full_tags) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv-Jy0S89CSs",
        "outputId": "03660413-d89c-48fe-d525-0e552e6fbcd7"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To predict our tags (i.e. whether requesters get their pizza), \n",
        "# we feed the vectorized `test_set` to .predict()\n",
        "predictions = clf.predict(test_set)"
      ],
      "metadata": {
        "id": "b63I9gGJ9EMy"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "success_rate = sum(df_train['requester_received_pizza']) / len(df_train) * 100\n",
        "print(str('Of {} requests, only {} gets their pizzas,'\n",
        "          ' {}% success rate...'.format(len(df_train), \n",
        "                                        sum(df_train['requester_received_pizza']), \n",
        "                                       success_rate)\n",
        "         )\n",
        "     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fyvny7809LbU",
        "outputId": "bdf48849-3312-44a8-ceda-db5a1e9e4df7"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of 4040 requests, only 994 gets their pizzas, 24.603960396039604% success rate...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "success_rate = sum(predictions) / len(predictions) * 100\n",
        "print(str('Of {} requests, only {} gets their pizzas,'\n",
        "          ' {}% success rate...'.format(len(predictions), \n",
        "                                        sum(predictions), \n",
        "                                       success_rate)\n",
        "         )\n",
        "     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tARm_W-B9NxB",
        "outputId": "a940c300-cc71-49da-9a75-f0930ae93839"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of 1631 requests, only 51 gets their pizzas, 3.126916002452483% success rate...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMDB dataset - Classification"
      ],
      "metadata": {
        "id": "xtF3tTHSCMC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fastai\n",
        "from fastai.text import *"
      ],
      "metadata": {
        "id": "i4sRgAGaCLlX"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading data from fastai\n",
        "path = untar_data(URLs.IMDB_SAMPLE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2PtAYCLoCSJG",
        "outputId": "25594520-bd76-4377-b4b3-48867b33c809"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://files.fast.ai/data/examples/imdb_sample.tgz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              ""
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path.ls()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J0lvl7UCcxv",
        "outputId": "4e9c8944-b022-4d84-be12-885f2e16b15b"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.fastai/data/imdb_sample/texts.csv')]"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the csv data into a pandas dataframe\n",
        "train_df = pd.read_csv(path/'texts.csv')\n",
        "train_df = train_df.drop(columns = ['is_valid'])\n",
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Ud-lweCkCd4n",
        "outputId": "ce14a36e-910c-4263-b2c4-81d7702166bc"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      label                                               text\n",
              "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...\n",
              "1  positive  This is a extremely well-made film. The acting...\n",
              "2  negative  Every once in a long while a movie will come a...\n",
              "3  positive  Name just says it all. I watched this movie wi...\n",
              "4  negative  This movie succeeds at being one of the most u..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ddd40177-ba78-4ccb-93f4-af5f0689286f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>This is a extremely well-made film. The acting...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>Every once in a long while a movie will come a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>Name just says it all. I watched this movie wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>This movie succeeds at being one of the most u...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ddd40177-ba78-4ccb-93f4-af5f0689286f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ddd40177-ba78-4ccb-93f4-af5f0689286f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ddd40177-ba78-4ccb-93f4-af5f0689286f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7cEoAFHEWkS",
        "outputId": "26c253e2-4bba-4414-cd65-d1ff3e50dad7"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting train and valid datasets \n",
        "train, valid = train_test_split(train_df, test_size=0.2)"
      ],
      "metadata": {
        "id": "sSA51NgVCr9k"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, valid"
      ],
      "metadata": {
        "id": "8aXq0qsTDty8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we essentially create the dictionary and \n",
        "# vectorize our input text at the same time.\n",
        "train_set = count_vect.fit_transform(train['text'])\n",
        "train_tags = train['label']\n",
        "\n",
        "# When vectorizing the validation data, we use `CountVectorizer.transform()`.\n",
        "valid_set = count_vect.transform(valid['text'])\n",
        "\n",
        "valid_tags = valid['label']"
      ],
      "metadata": {
        "id": "r4uP0SdlDHQH"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting a classifier to the data - Naive Bayes\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB() \n",
        "\n",
        "# To train the classifier, simple do \n",
        "clf.fit(train_set, train_tags) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky7sjupxDz02",
        "outputId": "48e2d218-b593-4fb1-8aa1-265aa0c3d89e"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To predict our tags (i.e. whether requesters get their pizza), \n",
        "# we feed the vectorized `test_set` to .predict()\n",
        "predictions_valid = clf.predict(valid_set)\n",
        "\n",
        "print('Pizza reception accuracy = {}'.format(\n",
        "        accuracy_score(predictions_valid, valid_tags) * 100)\n",
        "     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiqtrLp-D-1e",
        "outputId": "6575442c-6dd8-4bfd-a192-c5deb3de0ad9"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pizza reception accuracy = 76.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we do not have test set here, I'm considering valid set as final. I got an accuracy score of **76.5%** on the valid set using this Bayes Classification model which is trained on just 800 samples. "
      ],
      "metadata": {
        "id": "o76pySmsEOVG"
      }
    }
  ]
}