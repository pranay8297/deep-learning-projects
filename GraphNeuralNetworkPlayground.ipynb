{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "0-mPsRuDf-Ok",
        "yck1UjDYgBl3",
        "NN1jBdqUKKzf",
        "pxR-bGTxHRr-",
        "WXIJEmlgkOG8"
      ],
      "authorship_tag": "ABX9TyMEMXXT9SUIMrpLkMfA5b/e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranay8297/deep-learning-projects/blob/master/GraphNeuralNetworkPlayground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n",
        "!pip install  networkx\n",
        "!pip install ipdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHp-KzOgdZ-x",
        "outputId": "61870225-9f48-43a4-a487-bd7a77606deb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /home/ec2-user/virtenv/lib/python3.7/site-packages (2.3.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from torch_geometric) (5.9.4)\n",
            "Requirement already satisfied: numpy in /home/ec2-user/virtenv/lib/python3.7/site-packages (from torch_geometric) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /home/ec2-user/virtenv/lib/python3.7/site-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: requests in /home/ec2-user/virtenv/lib/python3.7/site-packages (from torch_geometric) (2.28.2)\n",
            "Requirement already satisfied: pyparsing in /home/ec2-user/virtenv/lib/python3.7/site-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: jinja2 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scipy in /home/ec2-user/virtenv/lib/python3.7/site-packages (from torch_geometric) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /home/ec2-user/virtenv/lib/python3.7/site-packages (from torch_geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from jinja2->torch_geometric) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from requests->torch_geometric) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from requests->torch_geometric) (1.26.15)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
            "You should consider upgrading via the '/home/ec2-user/virtenv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: networkx in /home/ec2-user/virtenv/lib/python3.7/site-packages (2.6.3)\n",
            "^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
            "You should consider upgrading via the '/home/ec2-user/virtenv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: ipdb in /home/ec2-user/virtenv/lib/python3.7/site-packages (0.13.13)\n",
            "Requirement already satisfied: tomli in /home/ec2-user/virtenv/lib/python3.7/site-packages (from ipdb) (2.0.1)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from ipdb) (7.34.0)\n",
            "Requirement already satisfied: decorator in /home/ec2-user/virtenv/lib/python3.7/site-packages (from ipdb) (5.1.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from ipython>=7.31.1->ipdb) (4.8.0)\n",
            "Requirement already satisfied: pygments in /home/ec2-user/virtenv/lib/python3.7/site-packages (from ipython>=7.31.1->ipdb) (2.14.0)\n",
            "Requirement already satisfied: matplotlib-inline in /home/ec2-user/virtenv/lib/python3.7/site-packages (from ipython>=7.31.1->ipdb) (0.1.6)\n",
            "Requirement already satisfied: setuptools>=18.5 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from ipython>=7.31.1->ipdb) (47.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from ipython>=7.31.1->ipdb) (0.18.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from ipython>=7.31.1->ipdb) (5.9.0)\n",
            "Requirement already satisfied: pickleshare in /home/ec2-user/virtenv/lib/python3.7/site-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
            "Requirement already satisfied: backcall in /home/ec2-user/virtenv/lib/python3.7/site-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from ipython>=7.31.1->ipdb) (3.0.38)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/virtenv/lib/python3.7/site-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /home/ec2-user/virtenv/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb) (0.2.6)\n",
            "^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
            "You should consider upgrading via the '/home/ec2-user/virtenv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4aeTbTBjwbT7"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import add_self_loops, degree\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ipdb import set_trace as st\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ITER - 1"
      ],
      "metadata": {
        "id": "0-mPsRuDf-Ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Planetoid(root='/tmp/Cora', name='Cora')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQRgr7BtdsA_",
        "outputId": "d41f1fbd-1dc5-49c2-8fdf-18d7cff366aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.__dict__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaBe7ktxdtZk",
        "outputId": "71d39bc9-12a3-4986-a0dd-59e950582355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Cora',\n",
              " 'split': 'public',\n",
              " 'root': '/tmp/Cora',\n",
              " 'transform': None,\n",
              " 'pre_transform': None,\n",
              " 'pre_filter': None,\n",
              " 'log': True,\n",
              " '_indices': None,\n",
              " '_data': Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708]),\n",
              " 'slices': None,\n",
              " '_data_list': None}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.num_node_features, dataset.num_edge_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzoSXUA-d6mJ",
        "outputId": "0ed6ca8c-12fa-47da-dd1d-c3525b632267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1433, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset._data.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhRjQaW8eETm",
        "outputId": "e4bcdd3a-fae4-4537-81d8-58d7e62d63cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges = dataset.data.edge_index"
      ],
      "metadata": {
        "id": "yyZ6JsN24xdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edges.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj_dGBYNVk4I",
        "outputId": "4e4f54aa-01e0-480d-dfa9-06ff6935245d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10556])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges = dataset.data.edge_index.numpy()"
      ],
      "metadata": {
        "id": "PnTFu2WsV5NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.min(edges[0] == edges[0, :]), np.min(edges[1] == edges[1, :])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spot8iZgW2KF",
        "outputId": "f7630780-0b56-4bd9-bab0-875f52964cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edge_list = [(x, y) for x, y in zip(edges[0], edges[1])]"
      ],
      "metadata": {
        "id": "OYMGXjJYW2zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_list"
      ],
      "metadata": {
        "id": "xqaLfUYFXZJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = [i for i in range(np.max(edge_list))]"
      ],
      "metadata": {
        "id": "Ljj-RSehXd7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = nx.Graph()"
      ],
      "metadata": {
        "id": "eMFRTk48XvLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph.add_nodes_from(nodes)\n",
        "graph.add_edges_from(edge_list)\n"
      ],
      "metadata": {
        "id": "jz6vMzs4Xxx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = dataset.data.y"
      ],
      "metadata": {
        "id": "ecX7lK1SYBGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(111)\n",
        "options = {\n",
        "            'node_size': 30,\n",
        "            'width': 0.2,\n",
        "}\n",
        "nx.draw(graph, with_labels=False, node_color=labels.tolist(), cmap=plt.cm.tab10, font_weight='bold', **options)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "DzqbQPPkX5lD",
        "outputId": "f6256709-a57b-4f43-cf28-4f42e80b8be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_dataset(dataset):\n",
        "    edges_raw = dataset.data.edge_index.numpy()\n",
        "    edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
        "    labels = dataset.data.y.numpy()\n",
        "\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(list(range(np.max(edges_raw))))\n",
        "    G.add_edges_from(edges)\n",
        "    plt.subplot(111)\n",
        "    options = {\n",
        "                'node_size': 30,\n",
        "                'width': 0.2,\n",
        "    }\n",
        "    nx.draw(G, with_labels=False, node_color=labels.tolist(), cmap=plt.cm.tab10, font_weight='bold', **options)\n",
        "    plt.show()\n",
        "# plot_dataset(dataset)"
      ],
      "metadata": {
        "id": "qnBv__TCX9XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 classes - Single Layer Class and a Neural Network class that has multiple layers\n",
        "class GCNConv(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation\n",
        "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Step 1: Add self-loops\n",
        "\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "\n",
        "        # Step 2: Multiply with weights\n",
        "        x = self.lin(x)\n",
        "\n",
        "        # Step 3: Calculate the normalization\n",
        "        row, col = edge_index\n",
        "        deg = degree(row, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        # Step 4: Propagate the embeddings to the next layer\n",
        "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x,\n",
        "                              norm=norm)\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        # Normalize node features.\n",
        "\n",
        "        return x_j\n"
      ],
      "metadata": {
        "id": "kyqtPjGFY-nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, dataset):\n",
        "    super(Net, self).__init__()\n",
        "    self.c1 = GCNConv(dataset.num_node_features, 24)\n",
        "    self.bn = nn.BatchNorm1d(24)\n",
        "    self.c2 = GCNConv(24, dataset.num_classes)\n",
        "    self.bn_2 = nn.BatchNorm1d(dataset.num_classes)\n",
        "  \n",
        "  def forward(self, dataset):\n",
        "    x, edge_index = dataset.data.x, dataset.data.edge_index\n",
        "\n",
        "    h1 = F.relu(self.bn(self.c1(x, edge_index)))\n",
        "    h1 = F.dropout(h1, p = 0.5, training = self.training)\n",
        "    h2 = F.dropout(self.bn_2(self.c2(h1, edge_index)), p = 0.5, training = self.training)\n",
        "    return F.log_softmax(h2, dim = 1)\n"
      ],
      "metadata": {
        "id": "Mfpbqtn5cTHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = GCNConv(dataset.num_node_features, 24)"
      ],
      "metadata": {
        "id": "mE-UNOJCakI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, edge_index = dataset.x, dataset.edge_index"
      ],
      "metadata": {
        "id": "ECIQ3hP1aqa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = layer(x, edge_index)"
      ],
      "metadata": {
        "id": "nuLzRAa-ayZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l42FJnXlcMX0",
        "outputId": "6509bc5d-6e44-4fe6-f26c-44d2a8ef4952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2708, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(dataset)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-2)\n",
        "\n",
        "def test(data, train=True):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    pred = model(data).max(dim=1)[1]\n",
        "\n",
        "    if train:\n",
        "        correct += pred[data.train_mask].eq(data.y[data.train_mask]).sum().item()\n",
        "        return correct / (len(data.y[data.train_mask]))\n",
        "    else:\n",
        "        correct += pred[data.test_mask].eq(data.y[data.test_mask]).sum().item()\n",
        "        return correct / (len(data.y[data.test_mask]))\n",
        "\n",
        "def train(data, plot=False):\n",
        "    train_accuracies, test_accuracies = list(), list()\n",
        "    for epoch in range(100):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_acc = test(data)\n",
        "            test_acc = test(data, train=False)\n",
        "\n",
        "            train_accuracies.append(train_acc)\n",
        "            test_accuracies.append(test_acc)\n",
        "            print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: {:.5f}, Test Acc: {:.5f}'.\n",
        "                  format(epoch, loss, train_acc, test_acc))\n",
        "\n",
        "    if plot:\n",
        "        plt.plot(train_accuracies, label=\"Train accuracy\")\n",
        "        plt.plot(test_accuracies, label=\"Validation accuracy\")\n",
        "        plt.xlabel(\"# Epoch\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "K9SYK32dfGDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(dataset, plot = True)"
      ],
      "metadata": {
        "id": "lxsu3Q1sfVQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ITER - 2"
      ],
      "metadata": {
        "id": "yck1UjDYgBl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "2 functions \n",
        "1. update function - gaama\n",
        "2. message function - fi\n",
        "\n",
        "Spec: \n",
        "What aggregation sceheme to use. weather its mean, add or max?\n",
        "Edge Conv: Xik = sum(mlp(XiK-1 , (XiK-1) - (XjK-1)))\n",
        "'''"
      ],
      "metadata": {
        "id": "pHzceqFsgIQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EdgeConv(MessagePassing):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(EdgeConv, self).__init__(aggr = 'add')\n",
        "    self.mlp = nn.Sequential(nn.Linear(in_channels*2, out_channels),\n",
        "                             nn.BatchNorm1d(out_channels),\n",
        "                             nn.ReLU(), \n",
        "                             nn.Linear(out_channels, out_channels),\n",
        "                             nn.BatchNorm1d(out_channels))\n",
        "    \n",
        "  def forward(self, x, edge_index):\n",
        "    return self.propagate(edge_index, x = x)\n",
        "  \n",
        "  def message(self, x_i, x_j):\n",
        "    inp = torch.cat([x_i, (x_j - x_i)], dim = 1)\n",
        "    return self.mlp(inp)"
      ],
      "metadata": {
        "id": "Scp7dOgLxX0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(1, 10)\n",
        "b = torch.randn(1, 10)\n",
        "c = torch.cat([a, b], dim = 1)\n",
        "\n",
        "c.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbBKM3sC2Yvy",
        "outputId": "96b427d8-308a-4f98-d608-03140d2220c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edge_conv = EdgeConv(dataset.num_node_features, 128)"
      ],
      "metadata": {
        "id": "W4UiSKtp4Vi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EdgeNet(nn.Module):\n",
        "  def __init__(self, dataset):\n",
        "    super(EdgeNet, self).__init__()\n",
        "    self.c1 = EdgeConv(dataset.num_node_features, 128)\n",
        "    self.c2 = GCNConv(128, dataset.num_classes)\n",
        "  \n",
        "  def forward(self, dataset):\n",
        "    x, edge_index = dataset.data.x, dataset.data.edge_index\n",
        "\n",
        "    h1 = self.c1(x, edge_index)\n",
        "    h1 = F.dropout(h1, p = 0.5, training = self.training)\n",
        "    h2 = F.dropout(self.c2(h1, edge_index), p = 0.5, training = self.training)\n",
        "    return F.log_softmax(h2, dim = 1)"
      ],
      "metadata": {
        "id": "RzdjUGBj5qjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EdgeNet(dataset)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-2)"
      ],
      "metadata": {
        "id": "KEYfhKhh5Gds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(dataset, plot = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DkOTHGDr5QCv",
        "outputId": "16de082a-675d-4a50-e9e0-f79a9020ac3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Loss: 58.05602, Train Acc: 0.49286, Test Acc: 0.34900\n",
            "Epoch: 001, Loss: 43.36913, Train Acc: 0.52857, Test Acc: 0.31500\n",
            "Epoch: 002, Loss: 50.69139, Train Acc: 0.57143, Test Acc: 0.31200\n",
            "Epoch: 003, Loss: 41.02229, Train Acc: 0.61429, Test Acc: 0.36400\n",
            "Epoch: 004, Loss: 29.34721, Train Acc: 0.65000, Test Acc: 0.39400\n",
            "Epoch: 005, Loss: 20.61322, Train Acc: 0.63571, Test Acc: 0.40500\n",
            "Epoch: 006, Loss: 7.98051, Train Acc: 0.65714, Test Acc: 0.39000\n",
            "Epoch: 007, Loss: 17.08380, Train Acc: 0.64286, Test Acc: 0.41900\n",
            "Epoch: 008, Loss: 49.49514, Train Acc: 0.65714, Test Acc: 0.39700\n",
            "Epoch: 009, Loss: 26.77965, Train Acc: 0.68571, Test Acc: 0.38900\n",
            "Epoch: 010, Loss: 21.99355, Train Acc: 0.65000, Test Acc: 0.38700\n",
            "Epoch: 011, Loss: 21.39690, Train Acc: 0.58571, Test Acc: 0.38400\n",
            "Epoch: 012, Loss: 36.78561, Train Acc: 0.55714, Test Acc: 0.38500\n",
            "Epoch: 013, Loss: 70.36671, Train Acc: 0.58571, Test Acc: 0.39300\n",
            "Epoch: 014, Loss: 34.05000, Train Acc: 0.61429, Test Acc: 0.41400\n",
            "Epoch: 015, Loss: 2.43582, Train Acc: 0.62143, Test Acc: 0.44200\n",
            "Epoch: 016, Loss: 2.17929, Train Acc: 0.64286, Test Acc: 0.44900\n",
            "Epoch: 017, Loss: 17.87963, Train Acc: 0.66429, Test Acc: 0.46300\n",
            "Epoch: 018, Loss: 45.08458, Train Acc: 0.69286, Test Acc: 0.46600\n",
            "Epoch: 019, Loss: 9.27827, Train Acc: 0.75000, Test Acc: 0.47100\n",
            "Epoch: 020, Loss: 47.98757, Train Acc: 0.77143, Test Acc: 0.45500\n",
            "Epoch: 021, Loss: 3.63739, Train Acc: 0.80714, Test Acc: 0.45300\n",
            "Epoch: 022, Loss: 13.61820, Train Acc: 0.83571, Test Acc: 0.45900\n",
            "Epoch: 023, Loss: 27.14381, Train Acc: 0.85000, Test Acc: 0.45100\n",
            "Epoch: 024, Loss: 24.60699, Train Acc: 0.88571, Test Acc: 0.45500\n",
            "Epoch: 025, Loss: 22.16290, Train Acc: 0.88571, Test Acc: 0.46300\n",
            "Epoch: 026, Loss: 11.53677, Train Acc: 0.90714, Test Acc: 0.47300\n",
            "Epoch: 027, Loss: 23.12464, Train Acc: 0.87857, Test Acc: 0.47000\n",
            "Epoch: 028, Loss: 18.85653, Train Acc: 0.88571, Test Acc: 0.45700\n",
            "Epoch: 029, Loss: 9.68815, Train Acc: 0.88571, Test Acc: 0.45300\n",
            "Epoch: 030, Loss: 1.34047, Train Acc: 0.87143, Test Acc: 0.45800\n",
            "Epoch: 031, Loss: 20.06050, Train Acc: 0.87143, Test Acc: 0.45900\n",
            "Epoch: 032, Loss: 3.47910, Train Acc: 0.87857, Test Acc: 0.45700\n",
            "Epoch: 033, Loss: 22.55711, Train Acc: 0.89286, Test Acc: 0.46000\n",
            "Epoch: 034, Loss: 10.29799, Train Acc: 0.90000, Test Acc: 0.46800\n",
            "Epoch: 035, Loss: 3.46269, Train Acc: 0.91429, Test Acc: 0.47700\n",
            "Epoch: 036, Loss: 36.32815, Train Acc: 0.93571, Test Acc: 0.48300\n",
            "Epoch: 037, Loss: 2.14685, Train Acc: 0.95714, Test Acc: 0.48800\n",
            "Epoch: 038, Loss: 28.03862, Train Acc: 0.97143, Test Acc: 0.49600\n",
            "Epoch: 039, Loss: 23.17275, Train Acc: 0.97857, Test Acc: 0.49500\n",
            "Epoch: 040, Loss: 27.86034, Train Acc: 0.98571, Test Acc: 0.50600\n",
            "Epoch: 041, Loss: 14.91651, Train Acc: 0.98571, Test Acc: 0.51400\n",
            "Epoch: 042, Loss: 12.80873, Train Acc: 0.98571, Test Acc: 0.52800\n",
            "Epoch: 043, Loss: 55.90694, Train Acc: 0.98571, Test Acc: 0.52700\n",
            "Epoch: 044, Loss: 12.09605, Train Acc: 0.96429, Test Acc: 0.53600\n",
            "Epoch: 045, Loss: 61.58188, Train Acc: 0.96429, Test Acc: 0.52300\n",
            "Epoch: 046, Loss: 2.34401, Train Acc: 0.95000, Test Acc: 0.50600\n",
            "Epoch: 047, Loss: 15.48700, Train Acc: 0.95714, Test Acc: 0.50600\n",
            "Epoch: 048, Loss: 1.07558, Train Acc: 0.95000, Test Acc: 0.50200\n",
            "Epoch: 049, Loss: 40.32886, Train Acc: 0.95000, Test Acc: 0.50500\n",
            "Epoch: 050, Loss: 32.56684, Train Acc: 0.94286, Test Acc: 0.50400\n",
            "Epoch: 051, Loss: 14.99372, Train Acc: 0.93571, Test Acc: 0.50900\n",
            "Epoch: 052, Loss: 41.09079, Train Acc: 0.95000, Test Acc: 0.51400\n",
            "Epoch: 053, Loss: 28.34230, Train Acc: 0.95714, Test Acc: 0.51800\n",
            "Epoch: 054, Loss: 26.06663, Train Acc: 0.96429, Test Acc: 0.51400\n",
            "Epoch: 055, Loss: 17.05902, Train Acc: 0.98571, Test Acc: 0.51300\n",
            "Epoch: 056, Loss: 23.34657, Train Acc: 0.99286, Test Acc: 0.52100\n",
            "Epoch: 057, Loss: 2.10240, Train Acc: 0.99286, Test Acc: 0.52300\n",
            "Epoch: 058, Loss: 1.40384, Train Acc: 0.99286, Test Acc: 0.53100\n",
            "Epoch: 059, Loss: 13.03387, Train Acc: 0.99286, Test Acc: 0.53400\n",
            "Epoch: 060, Loss: 5.33912, Train Acc: 0.99286, Test Acc: 0.53200\n",
            "Epoch: 061, Loss: 11.34915, Train Acc: 0.99286, Test Acc: 0.53200\n",
            "Epoch: 062, Loss: 39.70705, Train Acc: 0.99286, Test Acc: 0.53100\n",
            "Epoch: 063, Loss: 15.78058, Train Acc: 0.99286, Test Acc: 0.53200\n",
            "Epoch: 064, Loss: 6.18677, Train Acc: 0.99286, Test Acc: 0.53100\n",
            "Epoch: 065, Loss: 2.34742, Train Acc: 0.99286, Test Acc: 0.53100\n",
            "Epoch: 066, Loss: 0.96283, Train Acc: 0.99286, Test Acc: 0.53200\n",
            "Epoch: 067, Loss: 1.77698, Train Acc: 0.99286, Test Acc: 0.52800\n",
            "Epoch: 068, Loss: 11.87816, Train Acc: 0.99286, Test Acc: 0.53500\n",
            "Epoch: 069, Loss: 4.45098, Train Acc: 0.99286, Test Acc: 0.54300\n",
            "Epoch: 070, Loss: 3.12439, Train Acc: 0.99286, Test Acc: 0.54900\n",
            "Epoch: 071, Loss: 5.24133, Train Acc: 0.99286, Test Acc: 0.54800\n",
            "Epoch: 072, Loss: 0.92401, Train Acc: 0.99286, Test Acc: 0.54500\n",
            "Epoch: 073, Loss: 3.98756, Train Acc: 0.99286, Test Acc: 0.54500\n",
            "Epoch: 074, Loss: 4.92922, Train Acc: 0.99286, Test Acc: 0.54700\n",
            "Epoch: 075, Loss: 1.09205, Train Acc: 0.99286, Test Acc: 0.54800\n",
            "Epoch: 076, Loss: 6.53340, Train Acc: 0.99286, Test Acc: 0.55200\n",
            "Epoch: 077, Loss: 1.06396, Train Acc: 0.99286, Test Acc: 0.55600\n",
            "Epoch: 078, Loss: 10.81557, Train Acc: 0.99286, Test Acc: 0.55800\n",
            "Epoch: 079, Loss: 0.78351, Train Acc: 0.99286, Test Acc: 0.56000\n",
            "Epoch: 080, Loss: 1.64528, Train Acc: 0.99286, Test Acc: 0.56200\n",
            "Epoch: 081, Loss: 16.11495, Train Acc: 1.00000, Test Acc: 0.57000\n",
            "Epoch: 082, Loss: 4.42878, Train Acc: 1.00000, Test Acc: 0.57700\n",
            "Epoch: 083, Loss: 0.77051, Train Acc: 1.00000, Test Acc: 0.57800\n",
            "Epoch: 084, Loss: 5.63202, Train Acc: 1.00000, Test Acc: 0.57700\n",
            "Epoch: 085, Loss: 0.80273, Train Acc: 1.00000, Test Acc: 0.57300\n",
            "Epoch: 086, Loss: 2.21246, Train Acc: 1.00000, Test Acc: 0.57300\n",
            "Epoch: 087, Loss: 1.39277, Train Acc: 0.99286, Test Acc: 0.57800\n",
            "Epoch: 088, Loss: 8.52985, Train Acc: 0.98571, Test Acc: 0.57900\n",
            "Epoch: 089, Loss: 0.71780, Train Acc: 0.98571, Test Acc: 0.57800\n",
            "Epoch: 090, Loss: 3.12834, Train Acc: 0.98571, Test Acc: 0.57800\n",
            "Epoch: 091, Loss: 4.50153, Train Acc: 0.98571, Test Acc: 0.57900\n",
            "Epoch: 092, Loss: 5.85033, Train Acc: 0.98571, Test Acc: 0.58300\n",
            "Epoch: 093, Loss: 2.76986, Train Acc: 0.98571, Test Acc: 0.57900\n",
            "Epoch: 094, Loss: 9.01079, Train Acc: 0.99286, Test Acc: 0.57600\n",
            "Epoch: 095, Loss: 8.86850, Train Acc: 0.99286, Test Acc: 0.57300\n",
            "Epoch: 096, Loss: 5.52491, Train Acc: 0.99286, Test Acc: 0.56400\n",
            "Epoch: 097, Loss: 1.79416, Train Acc: 0.99286, Test Acc: 0.55800\n",
            "Epoch: 098, Loss: 1.05792, Train Acc: 0.99286, Test Acc: 0.55300\n",
            "Epoch: 099, Loss: 1.59555, Train Acc: 1.00000, Test Acc: 0.54700\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Classification"
      ],
      "metadata": {
        "id": "NN1jBdqUKKzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import TUDataset\n",
        "dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
        "\n",
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:150]\n",
        "test_dataset = dataset[150:]\n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 16, shuffle = True)"
      ],
      "metadata": {
        "id": "UlI-tlAGA73d"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "tgBJ6ro7KIYw",
        "outputId": "2577553f-cc42-478d-bf49-34f85692cdf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4559cc8fd2c4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTUDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/TUDataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'MUTAG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'TUDataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKNLg8JvKUmy",
        "outputId": "3c23b063-da5a-46b9-92ac-3262ac3418e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[0]"
      ],
      "metadata": {
        "id": "H9gTN2B6KWqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(data):\n",
        "    edges_raw = data.edge_index.numpy()\n",
        "    edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
        "    labels = data.y.numpy()\n",
        "\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(list(range(np.max(edges_raw))))\n",
        "    G.add_edges_from(edges)\n",
        "    plt.subplot(111)\n",
        "    options = {\n",
        "                'node_size': 30,\n",
        "                'width': 0.2,\n",
        "    }\n",
        "    nx.draw(G, with_labels=False,  cmap=plt.cm.tab10, font_weight='bold', **options)\n",
        "    plt.show()\n",
        "# plot_dataset(dataset)"
      ],
      "metadata": {
        "id": "VXXi3GKnKX_U"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "KOmStyBNK10r",
        "outputId": "19fa25fd-8852-4104-87c1-07424fad13e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.edge_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdQ-fzefbM5A",
        "outputId": "d13b98da-bcbf-4423-f4a3-f67926914c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  0,  1,  1,  2,  2,  3,  3,  3,  4,  4,  4,  5,  5,  6,  6,  7,  7,\n",
              "          8,  8,  8,  9,  9,  9, 10, 10, 11, 11, 12, 12, 12, 13, 13, 14, 14, 14,\n",
              "         15, 16],\n",
              "        [ 1,  5,  0,  2,  1,  3,  2,  4,  9,  3,  5,  6,  0,  4,  4,  7,  6,  8,\n",
              "          7,  9, 13,  3,  8, 10,  9, 11, 10, 12, 11, 13, 14,  8, 12, 12, 15, 16,\n",
              "         14, 14]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kRLfQS_kMHte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xfS80c71M6u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "OKxzjzuvNGAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdJ3YeZlNJQy",
        "outputId": "30b91d58-f7b8-4ac7-aebe-90235f34a621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataBatch(edge_index=[2, 712], x=[321, 7], edge_attr=[712, 4], y=[16], batch=[321], ptr=[17])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch.batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fYuirDvOFct",
        "outputId": "b841a075-fcb1-47ab-a6ed-344f8468c0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
              "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
              "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
              "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
              "         4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
              "         5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
              "         6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
              "         7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
              "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,\n",
              "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10,\n",
              "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11,\n",
              "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
              "        11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13,\n",
              "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14,\n",
              "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15,\n",
              "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch.edge_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srul9FvlOKFu",
        "outputId": "db7effc1-0890-405d-d0a3-e37486923178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   1,  ..., 318, 319, 320],\n",
              "        [  1,   5,   0,  ..., 320, 318, 318]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool"
      ],
      "metadata": {
        "id": "G8ciQymccoCz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = F.sigmoid(self.lin(x))\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = GCN(hidden_channels=64)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCiZ2M5kco5S",
        "outputId": "714d83e3-dcaf-47b1-ca5b-2c65c7603ea2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(7, 64)\n",
            "  (conv2): GCNConv(64, 64)\n",
            "  (conv3): GCNConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "faP_MM_xdOpQ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(hidden_channels=64)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model.to(device)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "        loss = criterion(out, data.y)  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "def test(loader):\n",
        "     model.eval()\n",
        "     correct = 0\n",
        "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "        data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)  \n",
        "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n",
        "\n",
        "for epoch in range(1, 171):\n",
        "    train()\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "MIBrUzlsdDes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ed66083-e672-4cb9-c04d-cef4b9cc92f3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6667, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.7467, Test Acc: 0.7632\n",
            "Epoch: 007, Train Acc: 0.7333, Test Acc: 0.7632\n",
            "Epoch: 008, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 009, Train Acc: 0.7267, Test Acc: 0.7632\n",
            "Epoch: 010, Train Acc: 0.7200, Test Acc: 0.8421\n",
            "Epoch: 011, Train Acc: 0.7000, Test Acc: 0.7895\n",
            "Epoch: 012, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 013, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 014, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 015, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 016, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 017, Train Acc: 0.6733, Test Acc: 0.7368\n",
            "Epoch: 018, Train Acc: 0.7267, Test Acc: 0.7632\n",
            "Epoch: 019, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 020, Train Acc: 0.7667, Test Acc: 0.7368\n",
            "Epoch: 021, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 022, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 023, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 024, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 025, Train Acc: 0.7467, Test Acc: 0.7368\n",
            "Epoch: 026, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 027, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 028, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 029, Train Acc: 0.7533, Test Acc: 0.8421\n",
            "Epoch: 030, Train Acc: 0.7400, Test Acc: 0.7368\n",
            "Epoch: 031, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 032, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 033, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 034, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 035, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 036, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 037, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 038, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 039, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 040, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 041, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 042, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 043, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 044, Train Acc: 0.7400, Test Acc: 0.7632\n",
            "Epoch: 045, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 046, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 047, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 048, Train Acc: 0.7667, Test Acc: 0.7368\n",
            "Epoch: 049, Train Acc: 0.7400, Test Acc: 0.7632\n",
            "Epoch: 050, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 051, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 052, Train Acc: 0.7667, Test Acc: 0.7368\n",
            "Epoch: 053, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 054, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 055, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 056, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 057, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 058, Train Acc: 0.7667, Test Acc: 0.7368\n",
            "Epoch: 059, Train Acc: 0.7667, Test Acc: 0.8684\n",
            "Epoch: 060, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 061, Train Acc: 0.7667, Test Acc: 0.7368\n",
            "Epoch: 062, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 063, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 064, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 065, Train Acc: 0.7533, Test Acc: 0.8684\n",
            "Epoch: 066, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 067, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 068, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 069, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 070, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 071, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 072, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 073, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 074, Train Acc: 0.7667, Test Acc: 0.7368\n",
            "Epoch: 075, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 076, Train Acc: 0.7667, Test Acc: 0.7368\n",
            "Epoch: 077, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 078, Train Acc: 0.7467, Test Acc: 0.7368\n",
            "Epoch: 079, Train Acc: 0.7800, Test Acc: 0.7368\n",
            "Epoch: 080, Train Acc: 0.7467, Test Acc: 0.7632\n",
            "Epoch: 081, Train Acc: 0.7800, Test Acc: 0.7368\n",
            "Epoch: 082, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 083, Train Acc: 0.7800, Test Acc: 0.7368\n",
            "Epoch: 084, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 085, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 086, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 087, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 088, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 089, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 090, Train Acc: 0.7800, Test Acc: 0.7368\n",
            "Epoch: 091, Train Acc: 0.7800, Test Acc: 0.7368\n",
            "Epoch: 092, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 093, Train Acc: 0.7667, Test Acc: 0.7368\n",
            "Epoch: 094, Train Acc: 0.7800, Test Acc: 0.7368\n",
            "Epoch: 095, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 096, Train Acc: 0.7667, Test Acc: 0.7368\n",
            "Epoch: 097, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 098, Train Acc: 0.7667, Test Acc: 0.7368\n",
            "Epoch: 099, Train Acc: 0.7667, Test Acc: 0.7368\n",
            "Epoch: 100, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 101, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 102, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 103, Train Acc: 0.7867, Test Acc: 0.7895\n",
            "Epoch: 104, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 105, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 106, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 107, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 108, Train Acc: 0.7667, Test Acc: 0.7368\n",
            "Epoch: 109, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 110, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 111, Train Acc: 0.7867, Test Acc: 0.8158\n",
            "Epoch: 112, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 113, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 114, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 115, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 116, Train Acc: 0.7667, Test Acc: 0.7368\n",
            "Epoch: 117, Train Acc: 0.7800, Test Acc: 0.7368\n",
            "Epoch: 118, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 119, Train Acc: 0.7867, Test Acc: 0.8158\n",
            "Epoch: 120, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 121, Train Acc: 0.7933, Test Acc: 0.8421\n",
            "Epoch: 122, Train Acc: 0.7933, Test Acc: 0.8421\n",
            "Epoch: 123, Train Acc: 0.7800, Test Acc: 0.8421\n",
            "Epoch: 124, Train Acc: 0.8133, Test Acc: 0.7368\n",
            "Epoch: 125, Train Acc: 0.8133, Test Acc: 0.7368\n",
            "Epoch: 126, Train Acc: 0.8200, Test Acc: 0.8158\n",
            "Epoch: 127, Train Acc: 0.8000, Test Acc: 0.7368\n",
            "Epoch: 128, Train Acc: 0.8200, Test Acc: 0.8158\n",
            "Epoch: 129, Train Acc: 0.8067, Test Acc: 0.7368\n",
            "Epoch: 130, Train Acc: 0.8200, Test Acc: 0.7632\n",
            "Epoch: 131, Train Acc: 0.8133, Test Acc: 0.7368\n",
            "Epoch: 132, Train Acc: 0.7933, Test Acc: 0.8421\n",
            "Epoch: 133, Train Acc: 0.7867, Test Acc: 0.7105\n",
            "Epoch: 134, Train Acc: 0.7800, Test Acc: 0.7632\n",
            "Epoch: 135, Train Acc: 0.8000, Test Acc: 0.8158\n",
            "Epoch: 136, Train Acc: 0.7933, Test Acc: 0.7368\n",
            "Epoch: 137, Train Acc: 0.8067, Test Acc: 0.7368\n",
            "Epoch: 138, Train Acc: 0.8067, Test Acc: 0.7632\n",
            "Epoch: 139, Train Acc: 0.8000, Test Acc: 0.7368\n",
            "Epoch: 140, Train Acc: 0.8133, Test Acc: 0.7632\n",
            "Epoch: 141, Train Acc: 0.8067, Test Acc: 0.7368\n",
            "Epoch: 142, Train Acc: 0.8200, Test Acc: 0.7632\n",
            "Epoch: 143, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 144, Train Acc: 0.8133, Test Acc: 0.7368\n",
            "Epoch: 145, Train Acc: 0.8200, Test Acc: 0.7368\n",
            "Epoch: 146, Train Acc: 0.8200, Test Acc: 0.7368\n",
            "Epoch: 147, Train Acc: 0.7933, Test Acc: 0.8158\n",
            "Epoch: 148, Train Acc: 0.7933, Test Acc: 0.8158\n",
            "Epoch: 149, Train Acc: 0.7933, Test Acc: 0.7105\n",
            "Epoch: 150, Train Acc: 0.8133, Test Acc: 0.7368\n",
            "Epoch: 151, Train Acc: 0.8133, Test Acc: 0.7368\n",
            "Epoch: 152, Train Acc: 0.8200, Test Acc: 0.7368\n",
            "Epoch: 153, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 154, Train Acc: 0.8133, Test Acc: 0.7368\n",
            "Epoch: 155, Train Acc: 0.8133, Test Acc: 0.7368\n",
            "Epoch: 156, Train Acc: 0.8133, Test Acc: 0.7632\n",
            "Epoch: 157, Train Acc: 0.8200, Test Acc: 0.7368\n",
            "Epoch: 158, Train Acc: 0.8200, Test Acc: 0.7368\n",
            "Epoch: 159, Train Acc: 0.8133, Test Acc: 0.7632\n",
            "Epoch: 160, Train Acc: 0.8133, Test Acc: 0.7368\n",
            "Epoch: 161, Train Acc: 0.8133, Test Acc: 0.7368\n",
            "Epoch: 162, Train Acc: 0.8200, Test Acc: 0.7368\n",
            "Epoch: 163, Train Acc: 0.8133, Test Acc: 0.7368\n",
            "Epoch: 164, Train Acc: 0.8200, Test Acc: 0.7368\n",
            "Epoch: 165, Train Acc: 0.8200, Test Acc: 0.7368\n",
            "Epoch: 166, Train Acc: 0.8200, Test Acc: 0.7368\n",
            "Epoch: 167, Train Acc: 0.8200, Test Acc: 0.7368\n",
            "Epoch: 168, Train Acc: 0.8200, Test Acc: 0.7368\n",
            "Epoch: 169, Train Acc: 0.8133, Test Acc: 0.7105\n",
            "Epoch: 170, Train Acc: 0.8200, Test Acc: 0.7105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, Union\n",
        "\n",
        "from torch import Tensor\n",
        "\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.dense.linear import Linear\n",
        "from torch_geometric.typing import (\n",
        "    Adj,\n",
        "    OptPairTensor,\n",
        "    OptTensor,\n",
        "    Size,\n",
        "    SparseTensor,\n",
        ")\n",
        "from torch_geometric.utils import spmm\n",
        "\n",
        "\n",
        "class GraphConv(MessagePassing):\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: Union[int, Tuple[int, int]],\n",
        "        out_channels: int,\n",
        "        aggr: str = 'add',\n",
        "        bias: bool = True,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(aggr=aggr, **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        if isinstance(in_channels, int):\n",
        "            in_channels = (in_channels, in_channels)\n",
        "\n",
        "        self.lin_rel = Linear(in_channels[0], out_channels, bias=bias)\n",
        "        self.lin_root = Linear(in_channels[1], out_channels, bias=False)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        super().reset_parameters()\n",
        "        self.lin_rel.reset_parameters()\n",
        "        self.lin_root.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
        "                edge_weight: OptTensor = None, size: Size = None) -> Tensor:\n",
        "\n",
        "        if isinstance(x, Tensor):\n",
        "            x: OptPairTensor = (x, x)\n",
        "\n",
        "        # propagate_type: (x: OptPairTensor, edge_weight: OptTensor)\n",
        "        out = self.propagate(edge_index, x=x, edge_weight=edge_weight, size=size)\n",
        "        out = self.lin_rel(out)\n",
        "\n",
        "        x_r = x[1]\n",
        "        if x_r is not None:\n",
        "            out = out + self.lin_root(x_r)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j: Tensor, edge_weight: OptTensor) -> Tensor:\n",
        "        st()\n",
        "        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: OptPairTensor) -> Tensor:\n",
        "        return spmm(adj_t, x[0], reduce=self.aggr)"
      ],
      "metadata": {
        "id": "vGyp3YrNgRY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc = GraphConv(dataset.num_node_features, 32)"
      ],
      "metadata": {
        "id": "dY2gYoqLjN_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3-ZbRrnjTcb",
        "outputId": "463403b6-d713-4ff3-9d7b-16fdbd3730b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataBatch(edge_index=[2, 712], x=[321, 7], edge_attr=[712, 4], y=[16], batch=[321], ptr=[17])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc(batch.x, batch.edge_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkwvbyIwjYIK",
        "outputId": "57084ff7-ca60-4da8-a42d-9fb1ccacc58c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m<ipython-input-35-64f3b2e98466>\u001b[0m(63)\u001b[0;36mmessage\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     62 \u001b[0;31m        \u001b[0mst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 63 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mx_j\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0medge_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_j\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     64 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> ll\n",
            "\u001b[1;32m     61 \u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_j\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     62 \u001b[0m        \u001b[0mst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m---> 63 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mx_j\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0medge_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_j\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m     64 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "ipdb> x_j\n",
            "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
            "ipdb> x_j.shape\n",
            "torch.Size([712, 7])\n",
            "ipdb> exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Attention"
      ],
      "metadata": {
        "id": "pxR-bGTxHRr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.init as init\n",
        "from torch_geometric.utils import softmax\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "import torch\n",
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
        "\n",
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:150]\n",
        "test_dataset = dataset[150:]\n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 16, shuffle = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epnJqF20XVQT",
        "outputId": "bda60260-086c-45b5-eb5f-0d9fb9594f5a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
            "Extracting data/TUDataset/MUTAG/MUTAG.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdgjtf-dHXPK",
        "outputId": "a73a73d7-331c-4cd7-cbe6-cb5a21231354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
            "Extracting data/TUDataset/MUTAG/MUTAG.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "JHi7KC4SSGb4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch.x, batch.edge_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tURHeRp6SJ4D",
        "outputId": "b3683c21-2787-4417-9a09-b58f729ca1c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 1.,  ..., 0., 0., 0.]]),\n",
              " tensor([[  0,   0,   1,  ..., 318, 319, 320],\n",
              "         [  1,   5,   0,  ..., 320, 318, 318]]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch.x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yhpd_k8lU-fx",
        "outputId": "1ccce180-ea99-4f08-d14d-2a56cf5a02d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([321, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.num_node_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F71oDgcgVCH2",
        "outputId": "2cdd9520-deef-4be5-a566-4ac781cef328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_layer(in_channels, out_channels):\n",
        "  return nn.Sequential(nn.Linear(in_channels, out_channels), \n",
        "                      nn.LazyBatchNorm1d(out_channels),\n",
        "                      nn.ReLU())\n",
        "  \n",
        "class MyGatConv(MessagePassing):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(MyGatConv, self).__init__(aggr = 'add')\n",
        "    self.w = linear_layer(in_channels, out_channels)\n",
        "    self.in_c = in_channels\n",
        "    self.out_c = out_channels\n",
        "    self.leaky_r = nn.LeakyReLU(negative_slope=0.2)\n",
        "    self.attn = nn.Parameter(torch.Tensor(out_channels*2, 1))\n",
        "    init.kaiming_normal_(self.w[0].weight, nonlinearity='relu')\n",
        "    init.kaiming_normal_(self.attn, nonlinearity='relu')\n",
        "    \n",
        "  def forward(self, x, edge_index):\n",
        "\n",
        "    z = self.w(x)\n",
        "    out = self.propagate(edge_index, x = z)\n",
        "    return F.relu(out)\n",
        "  \n",
        "  def message(self, x_i, x_j, edge_index_i):\n",
        "\n",
        "    # calculate LeakyReLu(a . (Zi || Zj))\n",
        "    combined_weights = torch.cat([x_i, x_j], dim = 1)\n",
        "    inter_attns = self.leaky_r(combined_weights@self.attn)\n",
        "    soft_max_coeffs = softmax(inter_attns, edge_index_i)\n",
        "    return soft_max_coeffs * x_j\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "aY3FZCS1SOfz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = MyGatConv(dataset.num_node_features, 16)"
      ],
      "metadata": {
        "id": "YZ-WqePIU7sS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db5bbe0-acef-42a4-d5de-fa352aa88ad9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(test.state_dict(), './test.pth')"
      ],
      "metadata": {
        "id": "DhASWh8aW6Wy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_1 = MyGatConv(dataset.num_node_features, 16)\n",
        "test_1.load_state_dict(torch.load('./test.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cemn9ua0Je5",
        "outputId": "4b482e25-3873-433f-809a-12c85ba1b920"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(batch.x, batch.edge_index)"
      ],
      "metadata": {
        "id": "r-CuuzdPVKQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphAttentionNetwork(nn.Module):\n",
        "  def __init__(self, dataset):\n",
        "    super(GraphAttentionNetwork, self).__init__()\n",
        "    self.c1 = MyGatConv(dataset.num_node_features, 128)\n",
        "    self.c2 = MyGatConv(128, 16)\n",
        "    self.final_layer = nn.Sequential(nn.Linear(16, dataset.num_classes), nn.BatchNorm1d(dataset.num_classes))\n",
        "  \n",
        "  def forward(self, data):\n",
        "    x, edge_index = data.x, data.edge_index\n",
        "\n",
        "    h1 = self.c1(x, edge_index)\n",
        "    h1 = F.dropout(h1, p = 0.5, training = self.training)\n",
        "    h2 = F.dropout(self.c2(h1, edge_index), p = 0.5, training = self.training)\n",
        "    h2 = global_mean_pool(h2, batch = data.batch)\n",
        "    return F.log_softmax(self.final_layer(h2), dim = 1)"
      ],
      "metadata": {
        "id": "ns8Cbc38bN4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GraphAttentionNetwork(dataset)\n",
        "yhat = model(batch)\n",
        "yhat.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3G644YocXxS",
        "outputId": "3543b868-f0f5-4e3c-f2de-9d13f6d179c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyyjpMiqdsHC",
        "outputId": "ec790bc0-baac-4357-abeb-f299ea7ce5c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GraphAttentionNetwork(dataset)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model.to(device)\n",
        "\n",
        "def train(train_loader):\n",
        "    model.train()\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        data.to(device)\n",
        "        out = model(data)  # Perform a single forward pass.\n",
        "        loss = criterion(out, data.y)  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "def test(loader):\n",
        "     model.eval()\n",
        "     correct = 0\n",
        "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "        data.to(device)\n",
        "        out = model(data)  \n",
        "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n",
        "\n",
        "for epoch in range(1, 171):\n",
        "    train(train_loader)\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDFtHb1mboo4",
        "outputId": "764ddcac-edf8-4878-d380-40c0a06b466f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.7000, Test Acc: 0.7368\n",
            "Epoch: 003, Train Acc: 0.7133, Test Acc: 0.8421\n",
            "Epoch: 004, Train Acc: 0.7067, Test Acc: 0.6579\n",
            "Epoch: 005, Train Acc: 0.6933, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 008, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 009, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 010, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 012, Train Acc: 0.7067, Test Acc: 0.8158\n",
            "Epoch: 013, Train Acc: 0.7067, Test Acc: 0.8158\n",
            "Epoch: 014, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 015, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 016, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 017, Train Acc: 0.7067, Test Acc: 0.8158\n",
            "Epoch: 018, Train Acc: 0.6800, Test Acc: 0.7895\n",
            "Epoch: 019, Train Acc: 0.6800, Test Acc: 0.7895\n",
            "Epoch: 020, Train Acc: 0.7200, Test Acc: 0.8421\n",
            "Epoch: 021, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 022, Train Acc: 0.7333, Test Acc: 0.8158\n",
            "Epoch: 023, Train Acc: 0.6867, Test Acc: 0.7895\n",
            "Epoch: 024, Train Acc: 0.6800, Test Acc: 0.7632\n",
            "Epoch: 025, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 026, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 027, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 028, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 029, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 030, Train Acc: 0.7400, Test Acc: 0.7895\n",
            "Epoch: 031, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 032, Train Acc: 0.7333, Test Acc: 0.7632\n",
            "Epoch: 033, Train Acc: 0.7400, Test Acc: 0.7895\n",
            "Epoch: 034, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 035, Train Acc: 0.7400, Test Acc: 0.8421\n",
            "Epoch: 036, Train Acc: 0.7333, Test Acc: 0.7368\n",
            "Epoch: 037, Train Acc: 0.7333, Test Acc: 0.7632\n",
            "Epoch: 038, Train Acc: 0.7333, Test Acc: 0.8158\n",
            "Epoch: 039, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 040, Train Acc: 0.7200, Test Acc: 0.8158\n",
            "Epoch: 041, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 042, Train Acc: 0.6933, Test Acc: 0.8158\n",
            "Epoch: 043, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 044, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 045, Train Acc: 0.7400, Test Acc: 0.7895\n",
            "Epoch: 046, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 047, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 048, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 049, Train Acc: 0.7267, Test Acc: 0.7368\n",
            "Epoch: 050, Train Acc: 0.7200, Test Acc: 0.7368\n",
            "Epoch: 051, Train Acc: 0.7267, Test Acc: 0.8421\n",
            "Epoch: 052, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 053, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 054, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 055, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 056, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 057, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 058, Train Acc: 0.7333, Test Acc: 0.8158\n",
            "Epoch: 059, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 060, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 061, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 062, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 063, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 064, Train Acc: 0.7200, Test Acc: 0.8158\n",
            "Epoch: 065, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 066, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 067, Train Acc: 0.7200, Test Acc: 0.7368\n",
            "Epoch: 068, Train Acc: 0.7200, Test Acc: 0.7368\n",
            "Epoch: 069, Train Acc: 0.7200, Test Acc: 0.8158\n",
            "Epoch: 070, Train Acc: 0.7200, Test Acc: 0.8158\n",
            "Epoch: 071, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 072, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 073, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 074, Train Acc: 0.7200, Test Acc: 0.8158\n",
            "Epoch: 075, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 076, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 077, Train Acc: 0.7200, Test Acc: 0.8158\n",
            "Epoch: 078, Train Acc: 0.7267, Test Acc: 0.8421\n",
            "Epoch: 079, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 080, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 081, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 082, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 083, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 084, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 085, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 086, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 087, Train Acc: 0.7200, Test Acc: 0.8421\n",
            "Epoch: 088, Train Acc: 0.7200, Test Acc: 0.8421\n",
            "Epoch: 089, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 090, Train Acc: 0.7000, Test Acc: 0.7632\n",
            "Epoch: 091, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 092, Train Acc: 0.7267, Test Acc: 0.7632\n",
            "Epoch: 093, Train Acc: 0.7400, Test Acc: 0.7632\n",
            "Epoch: 094, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 095, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 096, Train Acc: 0.7400, Test Acc: 0.7632\n",
            "Epoch: 097, Train Acc: 0.7467, Test Acc: 0.7632\n",
            "Epoch: 098, Train Acc: 0.6867, Test Acc: 0.7895\n",
            "Epoch: 099, Train Acc: 0.6867, Test Acc: 0.7895\n",
            "Epoch: 100, Train Acc: 0.7267, Test Acc: 0.8421\n",
            "Epoch: 101, Train Acc: 0.7067, Test Acc: 0.8158\n",
            "Epoch: 102, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 103, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 104, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 105, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 106, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 107, Train Acc: 0.7200, Test Acc: 0.8421\n",
            "Epoch: 108, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 109, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 110, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 111, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 112, Train Acc: 0.7333, Test Acc: 0.7368\n",
            "Epoch: 113, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 114, Train Acc: 0.7067, Test Acc: 0.8158\n",
            "Epoch: 115, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 116, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 117, Train Acc: 0.7133, Test Acc: 0.8421\n",
            "Epoch: 118, Train Acc: 0.6933, Test Acc: 0.8158\n",
            "Epoch: 119, Train Acc: 0.7200, Test Acc: 0.8158\n",
            "Epoch: 120, Train Acc: 0.7067, Test Acc: 0.8158\n",
            "Epoch: 121, Train Acc: 0.7067, Test Acc: 0.8158\n",
            "Epoch: 122, Train Acc: 0.7067, Test Acc: 0.8421\n",
            "Epoch: 123, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 124, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 125, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 126, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 127, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 128, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 129, Train Acc: 0.7000, Test Acc: 0.8158\n",
            "Epoch: 130, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 131, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 132, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 133, Train Acc: 0.7200, Test Acc: 0.8158\n",
            "Epoch: 134, Train Acc: 0.7267, Test Acc: 0.8421\n",
            "Epoch: 135, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 136, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 137, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 138, Train Acc: 0.7067, Test Acc: 0.8158\n",
            "Epoch: 139, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 140, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 141, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 142, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 143, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 144, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 145, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 146, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 147, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 148, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 149, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 150, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 151, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 152, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 153, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 154, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 155, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 156, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 157, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 158, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 159, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 160, Train Acc: 0.7200, Test Acc: 0.7368\n",
            "Epoch: 161, Train Acc: 0.7067, Test Acc: 0.8158\n",
            "Epoch: 162, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 163, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 164, Train Acc: 0.7000, Test Acc: 0.7895\n",
            "Epoch: 165, Train Acc: 0.7000, Test Acc: 0.8158\n",
            "Epoch: 166, Train Acc: 0.7133, Test Acc: 0.8158\n",
            "Epoch: 167, Train Acc: 0.7200, Test Acc: 0.7368\n",
            "Epoch: 168, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 169, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 170, Train Acc: 0.7200, Test Acc: 0.7895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwccrOdXeU6u",
        "outputId": "15f9bcda-a513-4296-c7f9-f33324f5c94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GraphAttentionNetwork(\n",
              "  (c1): MyGatConv()\n",
              "  (c2): MyGatConv()\n",
              "  (final_layer): Sequential(\n",
              "    (0): Linear(in_features=16, out_features=2, bias=True)\n",
              "    (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5V8s5lLeZdp",
        "outputId": "cb4f0b1f-c342-4937-9243-549c8185f5ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataBatch(edge_index=[2, 712], x=[321, 7], edge_attr=[712, 4], y=[16], batch=[321], ptr=[17])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model(batch)"
      ],
      "metadata": {
        "id": "uHwxem7deXEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat.argmax(dim = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjZfKU-oecX8",
        "outputId": "5fc9ae18-b3e9-447b-a140-28d262cd4efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch.y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMgy7iGzefaC",
        "outputId": "bb2e0666-f583-49d3-c440-3d8f16d0c66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Edge Update Attention"
      ],
      "metadata": {
        "id": "WXIJEmlgkOG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GAT v2\n",
        "import torch.nn.init as init\n",
        "from torch_geometric.utils import softmax\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "import torch\n",
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
        "\n",
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:150]\n",
        "test_dataset = dataset[150:]\n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 16, shuffle = True)"
      ],
      "metadata": {
        "id": "INqBGtXykNue"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JpD2xFpV5J2w"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def linear_layer(in_channels, out_channels):\n",
        "#   return nn.Sequential(nn.Linear(in_channels, out_channels), \n",
        "#                       nn.LayerNorm(out_channels),\n",
        "#                       nn.ReLU())"
      ],
      "metadata": {
        "id": "LXughkKihD9E"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attn_heads(in_channels, out_channels):\n",
        "  return nn.Sequential(nn.Linear(in_channels, out_channels), \n",
        "                      nn.LayerNorm(out_channels), nn.Tanh())"
      ],
      "metadata": {
        "id": "ILLXUJaALsUP"
      },
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_layer(in_channels, out_channels):\n",
        "  return nn.Sequential(nn.Linear(in_channels, out_channels), \n",
        "                      nn.BatchNorm1d(out_channels),\n",
        "                      nn.LeakyReLU(1e-02))\n",
        "  \n",
        "def attn_heads(in_channels, out_channels):\n",
        "  return nn.Sequential(nn.Linear(in_channels, out_channels), \n",
        "                      nn.LayerNorm(out_channels), nn.Tanh())\n",
        "\n",
        "class ResLayer(nn.Module):\n",
        "  def __init__(self, in_c, out_c):\n",
        "    super().__init__()\n",
        "    self.l1 = linear_layer(in_c, out_c)\n",
        "    self.l2 = linear_layer(out_c, out_c)\n",
        "    self.l3 = linear_layer(out_c, out_c)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.l1(x)\n",
        "    return self.l3(self.l2(x)) + x"
      ],
      "metadata": {
        "id": "mUL-XiLH8lW2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "vTfh93dk_rMp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATv2Conv(MessagePassing):\n",
        "\n",
        "  def __init__(self, n_in, n_out):\n",
        "    super().__init__(aggr = 'add')\n",
        "    self.inital_transform = linear_layer(n_in, n_out)\n",
        "    self.attn_layer = linear_layer(2*n_out, 1)\n",
        "    self.n_in = n_in\n",
        "    self.n_out = n_out\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    z = self.inital_transform(x)\n",
        "    out = self.propagate(edge_index, x = z)\n",
        "    return F.relu(out)\n",
        "  \n",
        "  def message(self, x_i, x_j, edge_index_i):\n",
        "    attn_scores = self.attn_layer(torch.cat([x_i, x_j], dim = 1))\n",
        "\n",
        "    attn_scores_n = softmax(attn_scores, edge_index_i)\n",
        "\n",
        "    scaled_product = attn_scores_n * x_j\n",
        "\n",
        "    return scaled_product"
      ],
      "metadata": {
        "id": "t5XdTby5_uK9"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = GATv2Conv(dataset.num_node_features, 16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kOE51WNC4xY",
        "outputId": "5924a0dc-0df6-4425-dda9-858d51981c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_out = test(batch.x, batch.edge_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHjQ762mDAoZ",
        "outputId": "e21938a3-5cb9-484e-cbe0-b1ce9eb64b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 336, in set_trace\n",
            "    sys.settrace(self.trace_dispatch)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m<ipython-input-6-c897ed2fab69>\u001b[0m(18)\u001b[0;36mmessage\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     17 \u001b[0;31m    \u001b[0mst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 18 \u001b[0;31m    \u001b[0mattn_scores_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     19 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> ll\n",
            "\u001b[1;32m     15 \u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     16 \u001b[0m    \u001b[0mattn_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     17 \u001b[0m    \u001b[0mst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m---> 18 \u001b[0;31m    \u001b[0mattn_scores_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m     19 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     20 \u001b[0m    \u001b[0mscaled_product\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_scores_n\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_j\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     21 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     22 \u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mscaled_product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "ipdb> torch.cat([x_i, x_j], dim = 1).shape\n",
            "torch.Size([712, 32])\n",
            "ipdb> exit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 361, in set_quit\n",
            "    sys.settrace(None)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmD-RDp6ElPq",
        "outputId": "ec3fb803-b3c4-43ce-9820-dd8dbd6229cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([321, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GATv2ConvE(MessagePassing):\n",
        "\n",
        "  def __init__(self, n_in, n_out, e_in, e_out):\n",
        "    super().__init__(aggr = 'add')\n",
        "    self.n_transform = ResLayer(n_in, n_out)\n",
        "    self.e_transform = ResLayer(e_in, e_out)\n",
        "\n",
        "    # Node\n",
        "    self.n_attn = ResLayer(2*n_out + e_out, 1)\n",
        "    self.e_attn = ResLayer(2*n_out + e_out, 1)\n",
        "\n",
        "    # For Edge\n",
        "    self.n1_attn = ResLayer(e_out + 2*n_out, 1)\n",
        "    self.n2_attn = ResLayer(e_out + 2*n_out, 1)\n",
        "\n",
        "    self.n_message = ResLayer(e_out + n_out , n_out)\n",
        "    self.e_update = ResLayer(e_out + 2*n_out, e_out)\n",
        "    self.n_update = ResLayer(e_out + n_out, n_out)\n",
        "    self.n_in = n_in\n",
        "    self.n_out = n_out\n",
        "    self.e_in = e_in\n",
        "    self.e_out = e_out\n",
        "\n",
        "  def forward(self, x, edge_index, edge_attr):\n",
        "    # st()\n",
        "    x = self.n_transform(x)\n",
        "    edge_attr = self.e_transform(edge_attr)\n",
        "\n",
        "    node_features = self.propagate(edge_index, x = x, edge_attr = edge_attr)\n",
        "    edge_features = self.edge_updater(edge_index, x = x, edge_attr = edge_attr)\n",
        "    # st()\n",
        "    return node_features, edge_features\n",
        "  \n",
        "  def edge_update(self, edge_index, x_i, x_j, edge_attr):\n",
        "    # 2. Edge Message Calculation\n",
        "    # st()\n",
        "    input = torch.cat([x_i, x_j, edge_attr], dim = 1)\n",
        "    n1_attn = self.n1_attn(input)\n",
        "    n2_attn = self.n2_attn(input)\n",
        "\n",
        "    edge_message = torch.cat([n1_attn*x_i, n2_attn*x_j, edge_attr], dim = 1)\n",
        "    return self.e_update(edge_message) + edge_attr\n",
        "  \n",
        "  def message(self, x_i, x_j, edge_index, edge_attr):\n",
        "\n",
        "    input = torch.cat([x_i, x_j, edge_attr], dim = 1)\n",
        "    # 1. Node Message Calculation\n",
        "    neighbour_attn = self.n_attn(input)\n",
        "    edge_attn = self.e_attn(input)\n",
        "    \n",
        "    # 1.1 Attention Softmax\n",
        "    neighbour_attn = softmax(neighbour_attn, edge_index[0])\n",
        "    edge_attn = softmax(edge_attn, edge_index[0])\n",
        "\n",
        "    # 1.2 Gather the message\n",
        "    node_message = torch.cat([neighbour_attn*x_j, edge_attn*edge_attr], dim = 1)\n",
        "\n",
        "    return node_message\n",
        "  \n",
        "  def update(self, aggregated_output):\n",
        "    return self.n_update(aggregated_output)"
      ],
      "metadata": {
        "id": "U8tfqYWbDEpb"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATv2ConvE_1(MessagePassing):\n",
        "\n",
        "  def __init__(self, n_in, n_out, e_in, e_out):\n",
        "    super().__init__(aggr = 'add')\n",
        "    self.n_transform = ResLayer(n_in, n_out)\n",
        "    self.e_transform = ResLayer(e_in, e_out)\n",
        "\n",
        "    # Node\n",
        "    self.n_attn = ResLayer(2*n_out + e_out, 1)\n",
        "    self.e_attn = ResLayer(2*n_out + e_out, 1)\n",
        "\n",
        "    # For Edge\n",
        "    self.n1_attn = ResLayer(e_out + 2*n_out, 1)\n",
        "    self.n2_attn = ResLayer(e_out + 2*n_out, 1)\n",
        "\n",
        "    self.n_message = ResLayer(e_out + n_out , n_out)\n",
        "    self.e_update = ResLayer(e_out + 2*n_out, e_out)\n",
        "    self.n_update = ResLayer(n_out, n_out)\n",
        "    self.n_in = n_in\n",
        "    self.n_out = n_out\n",
        "    self.e_in = e_in\n",
        "    self.e_out = e_out\n",
        "\n",
        "  def forward(self, x, edge_index, edge_attr):\n",
        "    \n",
        "    # st()\n",
        "    x = self.n_transform(x)\n",
        "    edge_attr = self.e_transform(edge_attr)\n",
        "\n",
        "    node_features = self.propagate(edge_index, x = x, edge_attr = edge_attr)\n",
        "    edge_features = self.edge_updater(edge_index, x = x, edge_attr = edge_attr)\n",
        "\n",
        "    return node_features, edge_features\n",
        "  \n",
        "  def edge_update(self, edge_index, x_i, x_j, edge_attr):\n",
        "    \n",
        "    # 2. Edge Message Calculation\n",
        "    input = torch.cat([x_i, x_j, edge_attr], dim = 1)\n",
        "    n1_attn = self.n1_attn(input)\n",
        "    n2_attn = self.n2_attn(input)\n",
        "\n",
        "    edge_message = torch.cat([n1_attn*x_i, n2_attn*x_j, edge_attr], dim = 1)\n",
        "    return self.e_update(edge_message) + edge_attr\n",
        "  \n",
        "  def message(self, x_i, x_j, edge_index, edge_attr):\n",
        "\n",
        "    # 1. Node Message Calculation\n",
        "    input = torch.cat([x_i, x_j, edge_attr], dim = 1)\n",
        "    neighbour_attn = self.n_attn(input)\n",
        "    edge_attn = self.e_attn(input)\n",
        "    \n",
        "    # 1.1 Attention Softmax\n",
        "    neighbour_attn = softmax(neighbour_attn, edge_index[0])\n",
        "    edge_attn = softmax(edge_attn, edge_index[0])\n",
        "\n",
        "    # 1.2 Gather the message\n",
        "    node_message = torch.cat([neighbour_attn*x_j, edge_attn*edge_attr], dim = 1)\n",
        "    return self.n_message(node_message)\n",
        "  \n",
        "  def update(self, aggregated_output):\n",
        "    return self.n_update(aggregated_output)"
      ],
      "metadata": {
        "id": "V9r0istGfpJX"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATv2ConvEV2(MessagePassing):\n",
        "\n",
        "  def __init__(self, n_in, n_out, e_in, e_out):\n",
        "    super().__init__(aggr = 'add')\n",
        "    self.n_transform = ResLayer(n_in, n_out)\n",
        "    self.e_transform = ResLayer(e_in, e_out)\n",
        "\n",
        "    # Node\n",
        "    self.n_attn = attn_heads(2*n_out + e_out, 1)\n",
        "    self.e_attn = attn_heads(2*n_out + e_out, 1)\n",
        "    self.s_attn = attn_heads(2*n_out + e_out, 1)\n",
        "    self.n_message = ResLayer(e_out + 2*n_out , n_out)\n",
        "\n",
        "    # For Edge\n",
        "    self.n1_attn = ResLayer(e_out + 2*n_out, 1)\n",
        "    self.n2_attn = ResLayer(e_out + 2*n_out, 1)\n",
        "    self.s_e_attn = ResLayer(e_out + 2*n_out, 1)\n",
        "\n",
        "    # For Node Update\n",
        "    self.u_m_attn = attn_heads(2*n_out, 1)\n",
        "    self.u_s_attn = attn_heads(2*n_out, 1)\n",
        "    self.n_update = ResLayer(2*n_out, n_out)\n",
        "\n",
        "    self.e_update = ResLayer(e_out + 2*n_out, e_out)\n",
        "\n",
        "    self.n_in = n_in\n",
        "    self.n_out = n_out\n",
        "    self.e_in = e_in\n",
        "    self.e_out = e_out\n",
        "\n",
        "  def forward(self, x, edge_index, edge_attr):\n",
        "    # st()\n",
        "    x = self.n_transform(x)\n",
        "    edge_attr = self.e_transform(edge_attr)\n",
        "\n",
        "    node_features = self.propagate(edge_index, x = x, edge_attr = edge_attr)\n",
        "    edge_features = self.edge_updater(edge_index, x = x, edge_attr = edge_attr)\n",
        "\n",
        "    return node_features, edge_features\n",
        "  \n",
        "  def edge_update(self, edge_index, x_i, x_j, edge_attr):\n",
        "    # 2. Edge Message Calculation\n",
        "\n",
        "    input = torch.cat([x_i, x_j, edge_attr], dim = 1)\n",
        "    n1_attn = self.n1_attn(input)\n",
        "    n2_attn = self.n2_attn(input)\n",
        "    self_attn = self.s_e_attn(input)\n",
        "\n",
        "    edge_message = torch.cat([n1_attn*x_i, n2_attn*x_j, self_attn*edge_attr], dim = 1)\n",
        "    return self.e_update(edge_message) + edge_attr\n",
        "  \n",
        "  def message(self, x_i, x_j, edge_index, edge_attr):\n",
        "\n",
        "    input = torch.cat([x_i, x_j, edge_attr], dim = 1)\n",
        "    # 1. Node Message Calculation\n",
        "    \n",
        "    neighbour_attn = self.n_attn(input)\n",
        "    edge_attn = self.e_attn(input)\n",
        "    self_attn = self.s_attn(input)\n",
        "    \n",
        "    # 1.1 Attention Softmax\n",
        "    neighbour_attn = softmax(neighbour_attn, edge_index[0])\n",
        "    edge_attn = softmax(edge_attn, edge_index[0])\n",
        "    self_attn = softmax(self_attn, edge_index[0])\n",
        "\n",
        "    # 1.2 Gather the message\n",
        "    node_message = torch.cat([self_attn*x_i, neighbour_attn*x_j, edge_attn*edge_attr], dim = 1)\n",
        "\n",
        "    op = self.n_message(node_message)\n",
        "    return op\n",
        "  \n",
        "  def update(self, aggregated_output, x):\n",
        "    '''\n",
        "    input = aggregated_output || x\n",
        "    message_attn = msg_attn(input)\n",
        "    self_attn = self_attn(input)\n",
        "\n",
        "    h` = update(message_attn*aggregated_output || self_attn*x)\n",
        "    return h`\n",
        "    '''\n",
        "\n",
        "    input = torch.cat([aggregated_output, x], dim = 1)\n",
        "    m_attn = self.u_m_attn(input)\n",
        "    s_attn = self.u_s_attn(input)\n",
        "\n",
        "    return self.n_update(torch.cat([m_attn*aggregated_output, s_attn*x], dim = 1))\n",
        "  "
      ],
      "metadata": {
        "id": "eluiw2bAgfjY"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = GATv2ConvEV2(dataset.num_node_features, 16, dataset.num_edge_features, 16)"
      ],
      "metadata": {
        "id": "UFHY-M8CWdDv"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))\n",
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF4hlYexX6bs",
        "outputId": "86da7310-ab01-4249-9ce7-5a05302b48cf"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataBatch(edge_index=[2, 704], x=[319, 7], edge_attr=[704, 4], y=[16], batch=[319], ptr=[17])"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(batch.x, batch.edge_index, batch.edge_attr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG9ubI4VHDko",
        "outputId": "9d0bd04d-eeb7-4ae4-e16f-001ae4a20637"
      },
      "execution_count": 210,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m/tmp/ipykernel_25465/2770043420.py\u001b[0m(82)\u001b[0;36mupdate\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     81 \u001b[0;31m    \u001b[0mst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 82 \u001b[0;31m    \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maggregated_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     83 \u001b[0;31m    \u001b[0mm_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> c\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1.2454, 2.2018, 1.7808,  ..., 0.0000, 0.8820, 1.2514],\n",
              "         [1.2454, 2.2018, 1.7808,  ..., 0.0000, 0.8820, 1.2514],\n",
              "         [1.2454, 2.2017, 1.7809,  ..., 0.0000, 0.8821, 1.2512],\n",
              "         ...,\n",
              "         [1.2455, 2.2017, 1.7809,  ..., 0.0000, 0.8819, 1.2513],\n",
              "         [1.2455, 2.2017, 1.7809,  ..., 0.0000, 0.8819, 1.2513],\n",
              "         [1.2456, 2.2018, 1.7808,  ..., 0.0000, 0.8818, 1.2514]],\n",
              "        grad_fn=<AddBackward0>),\n",
              " tensor([[1.6999, 0.5070, 1.2359,  ..., 1.6414, 0.7072, 3.8029],\n",
              "         [1.6999, 0.5070, 1.2359,  ..., 1.6414, 0.7072, 3.8029],\n",
              "         [1.6999, 0.5070, 1.2359,  ..., 1.6414, 0.7072, 3.8029],\n",
              "         ...,\n",
              "         [2.2075, 0.5070, 1.2711,  ..., 2.2489, 0.7730, 4.5514],\n",
              "         [1.6614, 0.5070, 1.2359,  ..., 2.0305, 0.7072, 4.5659],\n",
              "         [1.6614, 0.5070, 1.2359,  ..., 2.0305, 0.7072, 4.5659]],\n",
              "        grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadGATv2ConvE(nn.Module):\n",
        "  def __init__(self, n_in, n_out, e_in, e_out, n_heads = 6):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([GATv2ConvE(n_in, n_out, e_in, e_out) for i in range(n_heads)])\n",
        "    self.node_layer = ResLayer(n_out*n_heads, n_out)\n",
        "    self.edge_layer = ResLayer(e_out*n_heads, e_out)\n",
        "  \n",
        "  def forward(self, x, edge_index, edge_attr):\n",
        "    yhat = [i(x, edge_index, edge_attr) for i in self.heads]\n",
        "    node_attrs, edge_attrs = [], []\n",
        "    for i in yhat:\n",
        "      node_attrs.append(i[0])\n",
        "      edge_attrs.append(i[1])\n",
        "    # st()\n",
        "    node_attrs = torch.cat(node_attrs, dim = 1)\n",
        "    edge_attrs = torch.cat(edge_attrs, dim = 1)\n",
        "\n",
        "    return self.node_layer(node_attrs), self.edge_layer(edge_attrs)"
      ],
      "metadata": {
        "id": "tJPlm7nQCgLl"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATV2(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, dataset = dataset, regression = False):\n",
        "        super(GATV2, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GATv2ConvEV2(dataset.num_node_features, hidden_channels, dataset.num_edge_features, hidden_channels)\n",
        "        self.conv2 = GATv2ConvEV2(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        self.conv3 = GATv2ConvEV2(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        self.regression = regression\n",
        "        if regression:\n",
        "          self.lin = nn.Sequential(nn.Linear(hidden_channels, 1))\n",
        "        else:\n",
        "          self.lin = ResLayer(hidden_channels, dataset.num_classes)  \n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x, edge_attr = self.conv1(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv2(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv3(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.2)\n",
        "       \n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.4, training=self.training)\n",
        "        if self.regression:\n",
        "          x = self.lin(x)\n",
        "        else:\n",
        "          x = F.sigmoid(self.lin(x))\n",
        "\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "# print(model)"
      ],
      "metadata": {
        "id": "QO01agTt4IOi"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-Z-0NFU5NzN",
        "outputId": "782151f6-5cf2-4d63-884c-d423db1b61e8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat.argmax(dim = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btEn0Cjr8KQP",
        "outputId": "2ce4442e-f153-4d34-f1f7-4e845496598b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZTQbteh58kT",
        "outputId": "a8652db3-c0a1-4e84-93ac-fe0a8db905ba"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataBatch(edge_index=[2, 712], x=[321, 7], edge_attr=[712, 4], y=[16], batch=[321], ptr=[17])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "luHYJOLc5f1N"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GATV2(hidden_channels=64)\n",
        "# model = GCN(hidden_channels=64)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-03)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model.to(device)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        # out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "def test(loader):\n",
        "     model.eval()\n",
        "     correct = 0\n",
        "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "        data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        # out = model(data.x, data.edge_index, data.batch)\n",
        "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n",
        "\n",
        "for epoch in range(1, 171):\n",
        "    train()\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "M5-MZh_w5Tfk",
        "outputId": "2478a516-9a54-44d0-f348-da562337d44d"
      },
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_25465/3348266798.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m171\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_25465/3348266798.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Iterate in batches over the training dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# out = model(data.x, data.edge_index, data.batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute the loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/virtenv/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_25465/3437815918.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, batch)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# 1. Obtain node embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/virtenv/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_25465/3952908844.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# st()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0medge_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/virtenv/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_25465/2351886127.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/virtenv/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/virtenv/lib64/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/virtenv/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/virtenv/lib64/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (416x46 and 7x64)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ESOL Dataset**"
      ],
      "metadata": {
        "id": "MMyq9dSyXPXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BASIC - BEST - 0.73, AVG - 0.85 | \n",
        "# New Attns - Best - 0.73, AVG - 0.82"
      ],
      "metadata": {
        "id": "ag2VEKYQjKyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GATV2(hidden_channels=64, dataset= train_dataset, regression= True)"
      ],
      "metadata": {
        "id": "HxgJK2_AXPEq"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS2f9cFcXpKk",
        "outputId": "ae85f348-9cd9-4dc2-943c-425a9a888158"
      },
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GATV2(\n",
              "  (conv1): GATv2ConvEV2()\n",
              "  (conv2): GATv2ConvEV2()\n",
              "  (conv3): GATv2ConvEV2()\n",
              "  (lin): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.conv1.n_transform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JmDtF1gjUuG",
        "outputId": "63b06942-73c5-4dd4-bcd2-b9e2aa57c42f"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResLayer(\n",
              "  (l1): Sequential(\n",
              "    (0): Linear(in_features=46, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): LeakyReLU(negative_slope=0.01)\n",
              "  )\n",
              "  (l2): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): LeakyReLU(negative_slope=0.01)\n",
              "  )\n",
              "  (l3): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): LeakyReLU(negative_slope=0.01)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "6aj2ERvzXqE4"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-jiiCGUYQjM",
        "outputId": "ea327339-0d1b-4cff-caf7-a6472525c0cc"
      },
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MolDataBatch(x=[472, 46], edge_index=[2, 958], edge_attr=[958, 10], cluster_index=[472], fra_edge_index=[2, 834], fra_edge_attr=[834, 10], y=[32, 1], batch=[472], ptr=[33])"
            ]
          },
          "metadata": {},
          "execution_count": 263
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)"
      ],
      "metadata": {
        "id": "MM1ae0l0YRM2"
      },
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBbRy3TsYazl",
        "outputId": "3c4d1ad1-ae5c-4024-c201-531df8e9b2b8"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6eoO-jIfwY3",
        "outputId": "1126f137-9462-4b18-ad8c-c06725ba81ef"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = GATV2(hidden_channels=64)\n",
        "# model = GCN(hidden_channels=64)\n",
        "model = GATV2(hidden_channels=64, dataset= train_dataset, regression= True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 6e-03, weight_decay=1e-02)\n",
        "criterion = torch.nn.MSELoss()\n",
        "model.to(device)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        # out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "    rmses = []\n",
        "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "        data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        out.squeeze()\n",
        "        rmse = torch.sqrt(torch.mean((out - data.y)**2))\n",
        "        rmses.append(rmse.item())\n",
        "    return np.mean(rmses)\n",
        "\n",
        "for epoch in range(1, 250):\n",
        "    train()\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg9wX3Y_Zudz",
        "outputId": "f1600232-bd73-4cff-991c-2b26bf85c5c9"
      },
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Acc: 3.9756, Test Acc: 4.0009\n",
            "Epoch: 002, Train Acc: 1.6540, Test Acc: 1.8575\n",
            "Epoch: 003, Train Acc: 1.8926, Test Acc: 2.1119\n",
            "Epoch: 004, Train Acc: 1.2685, Test Acc: 1.2966\n",
            "Epoch: 005, Train Acc: 1.4428, Test Acc: 1.5574\n",
            "Epoch: 006, Train Acc: 1.4587, Test Acc: 1.6731\n",
            "Epoch: 007, Train Acc: 1.2469, Test Acc: 1.3278\n",
            "Epoch: 008, Train Acc: 1.1754, Test Acc: 1.2512\n",
            "Epoch: 009, Train Acc: 1.1130, Test Acc: 1.1842\n",
            "Epoch: 010, Train Acc: 1.2042, Test Acc: 1.1848\n",
            "Epoch: 011, Train Acc: 1.0840, Test Acc: 1.1135\n",
            "Epoch: 012, Train Acc: 1.3092, Test Acc: 1.4145\n",
            "Epoch: 013, Train Acc: 1.1065, Test Acc: 1.1230\n",
            "Epoch: 014, Train Acc: 1.3195, Test Acc: 1.4678\n",
            "Epoch: 015, Train Acc: 1.2035, Test Acc: 1.1821\n",
            "Epoch: 016, Train Acc: 1.1509, Test Acc: 1.2166\n",
            "Epoch: 017, Train Acc: 1.2317, Test Acc: 1.1780\n",
            "Epoch: 018, Train Acc: 1.1168, Test Acc: 1.0785\n",
            "Epoch: 019, Train Acc: 1.1224, Test Acc: 1.1490\n",
            "Epoch: 020, Train Acc: 1.1482, Test Acc: 1.2710\n",
            "Epoch: 021, Train Acc: 1.0292, Test Acc: 1.1096\n",
            "Epoch: 022, Train Acc: 1.0784, Test Acc: 1.0291\n",
            "Epoch: 023, Train Acc: 1.1037, Test Acc: 1.1091\n",
            "Epoch: 024, Train Acc: 1.6158, Test Acc: 1.5750\n",
            "Epoch: 025, Train Acc: 1.1724, Test Acc: 1.1950\n",
            "Epoch: 026, Train Acc: 1.1631, Test Acc: 1.2218\n",
            "Epoch: 027, Train Acc: 1.4190, Test Acc: 1.2970\n",
            "Epoch: 028, Train Acc: 1.1229, Test Acc: 1.2220\n",
            "Epoch: 029, Train Acc: 1.0275, Test Acc: 1.1904\n",
            "Epoch: 030, Train Acc: 1.0891, Test Acc: 1.1066\n",
            "Epoch: 031, Train Acc: 1.3367, Test Acc: 1.4980\n",
            "Epoch: 032, Train Acc: 0.9297, Test Acc: 0.9763\n",
            "Epoch: 033, Train Acc: 1.2252, Test Acc: 1.4283\n",
            "Epoch: 034, Train Acc: 1.1274, Test Acc: 1.1314\n",
            "Epoch: 035, Train Acc: 1.3621, Test Acc: 1.4241\n",
            "Epoch: 036, Train Acc: 1.1848, Test Acc: 1.2635\n",
            "Epoch: 037, Train Acc: 1.0754, Test Acc: 1.0903\n",
            "Epoch: 038, Train Acc: 1.1912, Test Acc: 1.1440\n",
            "Epoch: 039, Train Acc: 1.3740, Test Acc: 1.2481\n",
            "Epoch: 040, Train Acc: 0.9241, Test Acc: 0.9619\n",
            "Epoch: 041, Train Acc: 1.2042, Test Acc: 1.1900\n",
            "Epoch: 042, Train Acc: 1.0848, Test Acc: 1.0695\n",
            "Epoch: 043, Train Acc: 1.1487, Test Acc: 1.0735\n",
            "Epoch: 044, Train Acc: 1.1514, Test Acc: 1.1864\n",
            "Epoch: 045, Train Acc: 0.9851, Test Acc: 1.0568\n",
            "Epoch: 046, Train Acc: 0.9782, Test Acc: 1.1591\n",
            "Epoch: 047, Train Acc: 1.7254, Test Acc: 1.6454\n",
            "Epoch: 048, Train Acc: 1.1365, Test Acc: 1.4173\n",
            "Epoch: 049, Train Acc: 1.0939, Test Acc: 1.0636\n",
            "Epoch: 050, Train Acc: 1.2170, Test Acc: 1.1849\n",
            "Epoch: 051, Train Acc: 1.1358, Test Acc: 1.1082\n",
            "Epoch: 052, Train Acc: 0.9170, Test Acc: 0.9659\n",
            "Epoch: 053, Train Acc: 0.9056, Test Acc: 0.9406\n",
            "Epoch: 054, Train Acc: 1.0971, Test Acc: 1.0926\n",
            "Epoch: 055, Train Acc: 1.5482, Test Acc: 1.7367\n",
            "Epoch: 056, Train Acc: 0.9215, Test Acc: 1.0234\n",
            "Epoch: 057, Train Acc: 1.1599, Test Acc: 1.2931\n",
            "Epoch: 058, Train Acc: 0.9537, Test Acc: 1.0077\n",
            "Epoch: 059, Train Acc: 0.9278, Test Acc: 0.9816\n",
            "Epoch: 060, Train Acc: 0.9564, Test Acc: 0.9676\n",
            "Epoch: 061, Train Acc: 0.8894, Test Acc: 0.9640\n",
            "Epoch: 062, Train Acc: 1.0178, Test Acc: 1.0681\n",
            "Epoch: 063, Train Acc: 0.9242, Test Acc: 1.0119\n",
            "Epoch: 064, Train Acc: 0.8559, Test Acc: 0.9153\n",
            "Epoch: 065, Train Acc: 1.3416, Test Acc: 1.5004\n",
            "Epoch: 066, Train Acc: 0.8848, Test Acc: 1.0004\n",
            "Epoch: 067, Train Acc: 0.9062, Test Acc: 0.9340\n",
            "Epoch: 068, Train Acc: 1.2083, Test Acc: 1.2753\n",
            "Epoch: 069, Train Acc: 0.8833, Test Acc: 1.0038\n",
            "Epoch: 070, Train Acc: 0.8309, Test Acc: 0.9572\n",
            "Epoch: 071, Train Acc: 1.1446, Test Acc: 1.1359\n",
            "Epoch: 072, Train Acc: 0.8380, Test Acc: 0.9138\n",
            "Epoch: 073, Train Acc: 0.9075, Test Acc: 0.9394\n",
            "Epoch: 074, Train Acc: 1.1381, Test Acc: 1.0937\n",
            "Epoch: 075, Train Acc: 1.1807, Test Acc: 1.1629\n",
            "Epoch: 076, Train Acc: 0.9307, Test Acc: 0.9995\n",
            "Epoch: 077, Train Acc: 1.1554, Test Acc: 1.1673\n",
            "Epoch: 078, Train Acc: 1.3872, Test Acc: 1.5531\n",
            "Epoch: 079, Train Acc: 0.9246, Test Acc: 1.1223\n",
            "Epoch: 080, Train Acc: 0.7996, Test Acc: 0.9455\n",
            "Epoch: 081, Train Acc: 1.1864, Test Acc: 1.0966\n",
            "Epoch: 082, Train Acc: 1.1336, Test Acc: 1.1214\n",
            "Epoch: 083, Train Acc: 1.5779, Test Acc: 1.8647\n",
            "Epoch: 084, Train Acc: 1.0271, Test Acc: 1.1397\n",
            "Epoch: 085, Train Acc: 1.4972, Test Acc: 1.3711\n",
            "Epoch: 086, Train Acc: 1.0309, Test Acc: 1.1136\n",
            "Epoch: 087, Train Acc: 0.9344, Test Acc: 1.1567\n",
            "Epoch: 088, Train Acc: 0.8199, Test Acc: 0.9082\n",
            "Epoch: 089, Train Acc: 0.8569, Test Acc: 1.0041\n",
            "Epoch: 090, Train Acc: 1.0285, Test Acc: 0.9996\n",
            "Epoch: 091, Train Acc: 0.9483, Test Acc: 1.0557\n",
            "Epoch: 092, Train Acc: 1.0050, Test Acc: 1.0306\n",
            "Epoch: 093, Train Acc: 0.7955, Test Acc: 0.9799\n",
            "Epoch: 094, Train Acc: 0.8105, Test Acc: 0.9353\n",
            "Epoch: 095, Train Acc: 0.8470, Test Acc: 0.9921\n",
            "Epoch: 096, Train Acc: 0.8196, Test Acc: 0.9163\n",
            "Epoch: 097, Train Acc: 0.9434, Test Acc: 1.1368\n",
            "Epoch: 098, Train Acc: 1.4474, Test Acc: 1.7009\n",
            "Epoch: 099, Train Acc: 0.8648, Test Acc: 0.9090\n",
            "Epoch: 100, Train Acc: 0.9085, Test Acc: 0.9813\n",
            "Epoch: 101, Train Acc: 0.8124, Test Acc: 0.8669\n",
            "Epoch: 102, Train Acc: 0.8911, Test Acc: 1.0813\n",
            "Epoch: 103, Train Acc: 0.8656, Test Acc: 0.8980\n",
            "Epoch: 104, Train Acc: 1.2085, Test Acc: 1.1901\n",
            "Epoch: 105, Train Acc: 1.1265, Test Acc: 1.4174\n",
            "Epoch: 106, Train Acc: 0.8601, Test Acc: 0.9686\n",
            "Epoch: 107, Train Acc: 0.8098, Test Acc: 1.0314\n",
            "Epoch: 108, Train Acc: 1.0818, Test Acc: 1.0262\n",
            "Epoch: 109, Train Acc: 0.9924, Test Acc: 0.9530\n",
            "Epoch: 110, Train Acc: 1.9872, Test Acc: 2.2053\n",
            "Epoch: 111, Train Acc: 0.8042, Test Acc: 0.9412\n",
            "Epoch: 112, Train Acc: 0.8796, Test Acc: 0.9459\n",
            "Epoch: 113, Train Acc: 1.0255, Test Acc: 0.9841\n",
            "Epoch: 114, Train Acc: 1.5417, Test Acc: 1.8445\n",
            "Epoch: 115, Train Acc: 1.2009, Test Acc: 1.5576\n",
            "Epoch: 116, Train Acc: 1.3724, Test Acc: 1.3470\n",
            "Epoch: 117, Train Acc: 1.1949, Test Acc: 1.1921\n",
            "Epoch: 118, Train Acc: 0.7928, Test Acc: 0.9441\n",
            "Epoch: 119, Train Acc: 1.1043, Test Acc: 1.1312\n",
            "Epoch: 120, Train Acc: 1.5320, Test Acc: 1.4748\n",
            "Epoch: 121, Train Acc: 0.8890, Test Acc: 0.9081\n",
            "Epoch: 122, Train Acc: 0.8308, Test Acc: 0.9531\n",
            "Epoch: 123, Train Acc: 0.8624, Test Acc: 1.0888\n",
            "Epoch: 124, Train Acc: 0.9533, Test Acc: 1.2256\n",
            "Epoch: 125, Train Acc: 0.8989, Test Acc: 1.2299\n",
            "Epoch: 126, Train Acc: 0.8253, Test Acc: 0.9171\n",
            "Epoch: 127, Train Acc: 1.0062, Test Acc: 1.0271\n",
            "Epoch: 128, Train Acc: 0.9099, Test Acc: 1.1220\n",
            "Epoch: 129, Train Acc: 1.3727, Test Acc: 1.5940\n",
            "Epoch: 130, Train Acc: 0.8884, Test Acc: 1.0830\n",
            "Epoch: 131, Train Acc: 0.8579, Test Acc: 0.8890\n",
            "Epoch: 132, Train Acc: 1.4013, Test Acc: 1.5426\n",
            "Epoch: 133, Train Acc: 0.9301, Test Acc: 0.9589\n",
            "Epoch: 134, Train Acc: 0.8789, Test Acc: 1.0351\n",
            "Epoch: 135, Train Acc: 1.6851, Test Acc: 1.6109\n",
            "Epoch: 136, Train Acc: 1.6113, Test Acc: 1.9516\n",
            "Epoch: 137, Train Acc: 0.9501, Test Acc: 1.2220\n",
            "Epoch: 138, Train Acc: 0.8953, Test Acc: 1.0277\n",
            "Epoch: 139, Train Acc: 1.2066, Test Acc: 1.5293\n",
            "Epoch: 140, Train Acc: 0.8272, Test Acc: 1.0922\n",
            "Epoch: 141, Train Acc: 1.2672, Test Acc: 1.2887\n",
            "Epoch: 142, Train Acc: 0.8586, Test Acc: 0.9284\n",
            "Epoch: 143, Train Acc: 1.1806, Test Acc: 1.1797\n",
            "Epoch: 144, Train Acc: 0.8018, Test Acc: 0.9759\n",
            "Epoch: 145, Train Acc: 0.7935, Test Acc: 0.8649\n",
            "Epoch: 146, Train Acc: 1.0965, Test Acc: 1.1327\n",
            "Epoch: 147, Train Acc: 0.8525, Test Acc: 1.1030\n",
            "Epoch: 148, Train Acc: 0.9157, Test Acc: 0.9098\n",
            "Epoch: 149, Train Acc: 0.9144, Test Acc: 0.9468\n",
            "Epoch: 150, Train Acc: 0.8019, Test Acc: 0.9185\n",
            "Epoch: 151, Train Acc: 0.8536, Test Acc: 1.0538\n",
            "Epoch: 152, Train Acc: 1.0110, Test Acc: 1.0594\n",
            "Epoch: 153, Train Acc: 0.7927, Test Acc: 0.9777\n",
            "Epoch: 154, Train Acc: 0.8058, Test Acc: 0.8804\n",
            "Epoch: 155, Train Acc: 0.8701, Test Acc: 1.0356\n",
            "Epoch: 156, Train Acc: 0.9203, Test Acc: 1.1577\n",
            "Epoch: 157, Train Acc: 0.8125, Test Acc: 1.0138\n",
            "Epoch: 158, Train Acc: 0.7784, Test Acc: 0.8961\n",
            "Epoch: 159, Train Acc: 0.8395, Test Acc: 0.8967\n",
            "Epoch: 160, Train Acc: 1.4769, Test Acc: 1.7490\n",
            "Epoch: 161, Train Acc: 0.9346, Test Acc: 1.0956\n",
            "Epoch: 162, Train Acc: 1.2919, Test Acc: 1.2973\n",
            "Epoch: 163, Train Acc: 1.0187, Test Acc: 1.0944\n",
            "Epoch: 164, Train Acc: 0.8830, Test Acc: 0.8866\n",
            "Epoch: 165, Train Acc: 0.9676, Test Acc: 0.9779\n",
            "Epoch: 166, Train Acc: 0.9125, Test Acc: 1.1613\n",
            "Epoch: 167, Train Acc: 0.7704, Test Acc: 0.9141\n",
            "Epoch: 168, Train Acc: 0.9630, Test Acc: 0.9130\n",
            "Epoch: 169, Train Acc: 1.3625, Test Acc: 1.6154\n",
            "Epoch: 170, Train Acc: 1.1139, Test Acc: 1.1304\n",
            "Epoch: 171, Train Acc: 1.0140, Test Acc: 1.2578\n",
            "Epoch: 172, Train Acc: 0.7901, Test Acc: 0.9945\n",
            "Epoch: 173, Train Acc: 0.7987, Test Acc: 0.9563\n",
            "Epoch: 174, Train Acc: 0.7669, Test Acc: 0.9681\n",
            "Epoch: 175, Train Acc: 0.8454, Test Acc: 0.9342\n",
            "Epoch: 176, Train Acc: 0.8899, Test Acc: 0.9464\n",
            "Epoch: 177, Train Acc: 0.9108, Test Acc: 1.1626\n",
            "Epoch: 178, Train Acc: 0.8156, Test Acc: 0.9540\n",
            "Epoch: 179, Train Acc: 0.8901, Test Acc: 1.0866\n",
            "Epoch: 180, Train Acc: 0.9274, Test Acc: 0.9602\n",
            "Epoch: 181, Train Acc: 0.8734, Test Acc: 0.9781\n",
            "Epoch: 182, Train Acc: 1.0964, Test Acc: 1.0732\n",
            "Epoch: 183, Train Acc: 0.8271, Test Acc: 0.9890\n",
            "Epoch: 184, Train Acc: 0.7256, Test Acc: 0.9011\n",
            "Epoch: 185, Train Acc: 0.8270, Test Acc: 0.8888\n",
            "Epoch: 186, Train Acc: 0.8944, Test Acc: 1.1362\n",
            "Epoch: 187, Train Acc: 0.9744, Test Acc: 1.0330\n",
            "Epoch: 188, Train Acc: 1.0212, Test Acc: 1.2782\n",
            "Epoch: 189, Train Acc: 1.2625, Test Acc: 1.2785\n",
            "Epoch: 190, Train Acc: 0.7873, Test Acc: 0.8672\n",
            "Epoch: 191, Train Acc: 0.8919, Test Acc: 0.9533\n",
            "Epoch: 192, Train Acc: 0.9560, Test Acc: 0.9795\n",
            "Epoch: 193, Train Acc: 0.8545, Test Acc: 0.9667\n",
            "Epoch: 194, Train Acc: 0.8223, Test Acc: 0.9558\n",
            "Epoch: 195, Train Acc: 1.4410, Test Acc: 1.6956\n",
            "Epoch: 196, Train Acc: 0.8092, Test Acc: 0.9620\n",
            "Epoch: 197, Train Acc: 0.9240, Test Acc: 0.9461\n",
            "Epoch: 198, Train Acc: 0.7616, Test Acc: 0.9315\n",
            "Epoch: 199, Train Acc: 0.7953, Test Acc: 0.8714\n",
            "Epoch: 200, Train Acc: 1.5428, Test Acc: 1.7694\n",
            "Epoch: 201, Train Acc: 1.0008, Test Acc: 0.9752\n",
            "Epoch: 202, Train Acc: 0.9558, Test Acc: 0.9436\n",
            "Epoch: 203, Train Acc: 1.1248, Test Acc: 1.1994\n",
            "Epoch: 204, Train Acc: 1.4692, Test Acc: 1.7156\n",
            "Epoch: 205, Train Acc: 1.0060, Test Acc: 1.0029\n",
            "Epoch: 206, Train Acc: 0.9076, Test Acc: 0.9468\n",
            "Epoch: 207, Train Acc: 0.9521, Test Acc: 1.1464\n",
            "Epoch: 208, Train Acc: 0.8159, Test Acc: 0.9096\n",
            "Epoch: 209, Train Acc: 0.9162, Test Acc: 0.9852\n",
            "Epoch: 210, Train Acc: 0.7610, Test Acc: 0.8864\n",
            "Epoch: 211, Train Acc: 0.8683, Test Acc: 1.0135\n",
            "Epoch: 212, Train Acc: 0.8120, Test Acc: 0.9869\n",
            "Epoch: 213, Train Acc: 1.0111, Test Acc: 1.1619\n",
            "Epoch: 214, Train Acc: 1.0262, Test Acc: 1.2552\n",
            "Epoch: 215, Train Acc: 1.1139, Test Acc: 1.1098\n",
            "Epoch: 216, Train Acc: 1.3316, Test Acc: 1.5633\n",
            "Epoch: 217, Train Acc: 1.2093, Test Acc: 1.2415\n",
            "Epoch: 218, Train Acc: 1.1778, Test Acc: 1.4026\n",
            "Epoch: 219, Train Acc: 1.7367, Test Acc: 1.9919\n",
            "Epoch: 220, Train Acc: 0.8001, Test Acc: 0.9469\n",
            "Epoch: 221, Train Acc: 0.9091, Test Acc: 1.0744\n",
            "Epoch: 222, Train Acc: 1.5366, Test Acc: 1.4902\n",
            "Epoch: 223, Train Acc: 0.9787, Test Acc: 1.0650\n",
            "Epoch: 224, Train Acc: 0.7977, Test Acc: 0.9161\n",
            "Epoch: 225, Train Acc: 1.2679, Test Acc: 1.5338\n",
            "Epoch: 226, Train Acc: 1.0566, Test Acc: 1.3004\n",
            "Epoch: 227, Train Acc: 0.9664, Test Acc: 1.0186\n",
            "Epoch: 228, Train Acc: 0.9045, Test Acc: 0.9910\n",
            "Epoch: 229, Train Acc: 0.9492, Test Acc: 0.9754\n",
            "Epoch: 230, Train Acc: 1.0028, Test Acc: 1.0366\n",
            "Epoch: 231, Train Acc: 0.8100, Test Acc: 0.9442\n",
            "Epoch: 232, Train Acc: 1.0422, Test Acc: 1.1284\n",
            "Epoch: 233, Train Acc: 0.7999, Test Acc: 0.8909\n",
            "Epoch: 234, Train Acc: 0.8209, Test Acc: 0.9619\n",
            "Epoch: 235, Train Acc: 0.8389, Test Acc: 1.0213\n",
            "Epoch: 236, Train Acc: 1.8485, Test Acc: 1.6898\n",
            "Epoch: 237, Train Acc: 0.9856, Test Acc: 1.0899\n",
            "Epoch: 238, Train Acc: 1.5360, Test Acc: 1.7345\n",
            "Epoch: 239, Train Acc: 0.7929, Test Acc: 0.9919\n",
            "Epoch: 240, Train Acc: 1.3743, Test Acc: 1.3358\n",
            "Epoch: 241, Train Acc: 0.8646, Test Acc: 0.9997\n",
            "Epoch: 242, Train Acc: 1.6839, Test Acc: 1.6173\n",
            "Epoch: 243, Train Acc: 0.9491, Test Acc: 1.1092\n",
            "Epoch: 244, Train Acc: 0.8227, Test Acc: 0.9251\n",
            "Epoch: 245, Train Acc: 0.8339, Test Acc: 0.9663\n",
            "Epoch: 246, Train Acc: 0.9101, Test Acc: 0.9074\n",
            "Epoch: 247, Train Acc: 0.7775, Test Acc: 0.8897\n",
            "Epoch: 248, Train Acc: 0.9835, Test Acc: 1.1545\n",
            "Epoch: 249, Train Acc: 1.0185, Test Acc: 1.0452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qriGNnp_TgqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HiGNN Experimentation"
      ],
      "metadata": {
        "id": "kiOeJ7LevY6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from config import get_config\n",
        "from utils import create_logger, seed_set\n",
        "from utils import NoamLR, build_scheduler, build_optimizer, get_metric_func\n",
        "from utils import load_checkpoint, save_best_checkpoint, load_best_result\n",
        "from dataset import build_loader\n",
        "from loss import bulid_loss\n",
        "from model import build_model\n",
        "from dataset import *"
      ],
      "metadata": {
        "id": "wjAWsz7d3Vr7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "DeYeG7Ij6DO5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset import load_dataset_random"
      ],
      "metadata": {
        "id": "APXNhWM15XDU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import DataLoader\n",
        "import os\n",
        "import torch.nn.init as init\n",
        "from torch_geometric.utils import softmax\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "import torch"
      ],
      "metadata": {
        "id": "tBIIcCmJPcBN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Sequential, Parameter, Bilinear\n",
        "\n",
        "from torch_scatter import scatter\n",
        "from torch_geometric.nn import global_add_pool, GATConv\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.inits import glorot, reset\n",
        "from torch_geometric.nn.pool.pool import pool_batch\n",
        "from torch_geometric.nn.pool.consecutive import consecutive_cluster\n"
      ],
      "metadata": {
        "id": "b3gTvQJ2QKXN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ../data/deleney"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnVgeO1R5vCc",
        "outputId": "8346aa9c-d100-42d4-c48c-7f5178ae1e72"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delaney.csv  processed\traw\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv('../data/deleney/delaney.csv').head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ujPHQMSy5908",
        "outputId": "0c9bc5b9-e036-4d97-9fb1-d66b73e8941d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              smiles  logSolubility\n",
              "0  OCC3OC(OCC2OC(OC(C#N)c1ccccc1)C(O)C(O)C2O)C(O)...          -0.77\n",
              "1                             Cc1occc1C(=O)Nc2ccccc2          -3.30\n",
              "2                               CC(C)=CCCC(C)=CC(=O)          -2.06\n",
              "3                 c1ccc2c(c1)ccc3c2ccc4c5ccccc5ccc43          -7.87\n",
              "4                                            c1ccsc1          -1.33"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>smiles</th>\n",
              "      <th>logSolubility</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OCC3OC(OCC2OC(OC(C#N)c1ccccc1)C(O)C(O)C2O)C(O)...</td>\n",
              "      <td>-0.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cc1occc1C(=O)Nc2ccccc2</td>\n",
              "      <td>-3.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CC(C)=CCCC(C)=CC(=O)</td>\n",
              "      <td>-2.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>c1ccc2c(c1)ccc3c2ccc4c5ccccc5ccc43</td>\n",
              "      <td>-7.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c1ccsc1</td>\n",
              "      <td>-1.33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ../data/deleney/raw/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZS9hErt6i8X",
        "outputId": "8777e02f-cfd2-48c4-eb68-440b63426185"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deleney.csv\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logger = logging.getLogger('random_logger')"
      ],
      "metadata": {
        "id": "GF4uy2gOON3z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info('Indian')"
      ],
      "metadata": {
        "id": "iEB7fI_bOX1D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xatw0DTbFGUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset_random(path, dataset, seed, task_type, tasks=None, logger=None):\n",
        "    save_path = path + 'processed/train_valid_test_{}_seed_{}.ckpt'.format(dataset, seed)\n",
        "    if os.path.isfile(save_path):\n",
        "        trn, val, test = torch.load(save_path)\n",
        "        return trn, val, test\n",
        "    pyg_dataset = MolDataset(root=path, dataset=dataset, task_type=task_type, tasks=tasks, logger=logger)\n",
        "    del pyg_dataset.data.smiles\n",
        "\n",
        "    # Seed randomness\n",
        "    random = Random(seed)\n",
        "    indices = list(range(len(pyg_dataset)))\n",
        "    random.seed(seed)\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    train_size = int(0.8 * len(pyg_dataset))\n",
        "    val_size = int(0.1 * len(pyg_dataset))\n",
        "    test_size = len(pyg_dataset) - train_size - val_size\n",
        "\n",
        "    trn_id, val_id, test_id = indices[:train_size], \\\n",
        "                              indices[train_size:(train_size + val_size)], \\\n",
        "                              indices[(train_size + val_size):]\n",
        "\n",
        "    trn, val, test = pyg_dataset[torch.LongTensor(trn_id)], \\\n",
        "                     pyg_dataset[torch.LongTensor(val_id)], \\\n",
        "                     pyg_dataset[torch.LongTensor(test_id)]\n",
        "\n",
        "    logger.info(f'Total smiles = {len(pyg_dataset):,} | '\n",
        "                f'train smiles = {train_size:,} | '\n",
        "                f'val smiles = {val_size:,} | '\n",
        "                f'test smiles = {test_size:,}')\n",
        "\n",
        "    assert task_type == 'classification' or 'regression'\n",
        "    if task_type == 'classification':\n",
        "        weights = []\n",
        "        for i in range(len(tasks)):\n",
        "            validId = np.where((pyg_dataset.data.y[:, i] == 0) | (pyg_dataset.data.y[:, i] == 1))[0]\n",
        "            pos_len = (pyg_dataset.data.y[:, i][validId].sum()).item()\n",
        "            neg_len = len(pyg_dataset.data.y[:, i][validId]) - pos_len\n",
        "            weights.append([(neg_len + pos_len) / neg_len, (neg_len + pos_len) / pos_len])\n",
        "        trn.weights = weights\n",
        "\n",
        "    else:\n",
        "        trn.weights = None\n",
        "\n",
        "    torch.save([trn, val, test], save_path)\n",
        "    return load_dataset_random(path, dataset, seed, task_type, tasks)\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = load_dataset_random('../data/deleney/',\n",
        "                                                                         \"deleney\",\n",
        "                                                                         2022,\n",
        "                                                                         'regression',\n",
        "                                                                         'logSolubility', \n",
        "                                                                          logger)\n",
        "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size = 32)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4n35e3-O_Ua",
        "outputId": "7d8b258a-0894-400f-d8e1-ce965ccf6411"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/ec2-user/virtenv/lib64/python3.7/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quick Experiment"
      ],
      "metadata": {
        "id": "1VN_VNEw43yu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RtjcLG9w46uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "5VQJQm7uPdhN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TIkRbZoPf4D",
        "outputId": "a9f79788-1dae-4c1c-f846-f77bde708117"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MolDataBatch(x=[422, 45], edge_index=[2, 870], edge_attr=[870, 10], cluster_index=[422], fra_edge_index=[2, 780], fra_edge_attr=[780, 10], y=[32, 1], batch=[422], ptr=[33])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch.fra_edge_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqAQoD3zXrfU",
        "outputId": "7e52882b-7e32-42fb-fdc3-68b5c3bbcbec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,   1,   1,  ..., 419, 420, 421],\n",
              "        [  1,   0,   2,  ..., 421, 419, 419]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_layer(n_in, n_out):\n",
        "    return nn.Sequential(nn.Linear(n_in, n_out), nn.LeakyReLU(p = 0.2), nn.LayerNorm(n_out))\n",
        "\n",
        "class ResLayer(nn.Module):\n",
        "    def __init__(self, n_in, n_out):\n",
        "\n",
        "        self.l1 = linear_layer(n_in, n_out)\n",
        "        self.l2 = linear_layer(n_out, n_out)\n",
        "        self.l3 = linear_layer(n_out, n_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        return self.l3(self.l2(x)) + x\n",
        "\n",
        "class FeatureAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction):\n",
        "        super().__init__()\n",
        "        self.mlp = Sequential(\n",
        "            Linear(channels, channels // reduction),\n",
        "            nn.ReLU(inplace=True),\n",
        "            Linear(channels // reduction, channels, bias=False),\n",
        "        )\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        reset(self.mlp)\n",
        "\n",
        "    def forward(self, x, batch, size=None):\n",
        "        max_result = scatter(x, batch, dim=0, dim_size=size, reduce='max')\n",
        "        sum_result = scatter(x, batch, dim=0, dim_size=size, reduce='sum')\n",
        "        max_out = self.mlp(max_result)\n",
        "        sum_out = self.mlp(sum_result)\n",
        "        y = torch.sigmoid(max_out + sum_out)\n",
        "        y = y[batch]\n",
        "        return x * y"
      ],
      "metadata": {
        "id": "oytbS6o1PixM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NTNConv(MessagePassing):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, slices, dropout, edge_dim=None, **kwargs):\n",
        "        kwargs.setdefault('aggr', 'add')\n",
        "        super(NTNConv, self).__init__(node_dim=0, **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.slices = slices\n",
        "        self.dropout = dropout\n",
        "        self.edge_dim = edge_dim\n",
        "\n",
        "        self.weight_node = Parameter(torch.Tensor(in_channels,\n",
        "                                                  out_channels))\n",
        "        if edge_dim is not None:\n",
        "            self.weight_edge = Parameter(torch.Tensor(edge_dim,\n",
        "                                                      out_channels))\n",
        "        else:\n",
        "            self.weight_edge = self.register_parameter('weight_edge', None)\n",
        "\n",
        "        self.bilinear = Bilinear(out_channels, out_channels, slices, bias=False)\n",
        "\n",
        "        if self.edge_dim is not None:\n",
        "            self.linear = Linear(3 * out_channels, slices)\n",
        "        else:\n",
        "            self.linear = Linear(2 * out_channels, slices)\n",
        "\n",
        "        self._alpha = None\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot(self.weight_node)\n",
        "        glorot(self.weight_edge)\n",
        "        self.bilinear.reset_parameters()\n",
        "        self.linear.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr=None, return_attention_weights=None):\n",
        "\n",
        "        x = torch.matmul(x, self.weight_node)\n",
        "\n",
        "        if self.weight_edge is not None:\n",
        "            assert edge_attr is not None\n",
        "            edge_attr = torch.matmul(edge_attr, self.weight_edge)\n",
        "\n",
        "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
        "\n",
        "        alpha = self._alpha\n",
        "        self._alpha = None\n",
        "\n",
        "        if isinstance(return_attention_weights, bool):\n",
        "            assert alpha is not None\n",
        "            return out, (edge_index, alpha)\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "    def message(self, x_i, x_j, edge_attr):\n",
        "        score = self.bilinear(x_i, x_j)\n",
        "        if edge_attr is not None:\n",
        "            vec = torch.cat((x_i, edge_attr, x_j), 1)\n",
        "            block_score = self.linear(vec)  # bias already included\n",
        "        else:\n",
        "            vec = torch.cat((x_i, x_j), 1)\n",
        "            block_score = self.linear(vec)\n",
        "        scores = score + block_score\n",
        "        alpha = torch.tanh(scores)\n",
        "        self._alpha = alpha\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "\n",
        "        dim_split = self.out_channels // self.slices\n",
        "        out = torch.max(x_j, edge_attr).view(-1, self.slices, dim_split)\n",
        "\n",
        "        out = out * alpha.view(-1, self.slices, 1)\n",
        "        out = out.view(-1, self.out_channels)\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {}, slices={})'.format(self.__class__.__name__,\n",
        "                                              self.in_channels,\n",
        "                                              self.out_channels, self.slices)"
      ],
      "metadata": {
        "id": "GbeTA0rqP5tW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HiGNN(torch.nn.Module):\n",
        "    \"\"\"Hierarchical informative graph neural network for molecular representation.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, edge_dim, num_layers,\n",
        "                 slices, dropout, f_att=False, r=4, brics=True, cl=False, save_path = None, load_path = None):\n",
        "        super(HiGNN, self).__init__()\n",
        "\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.f_att = f_att\n",
        "        self.brics = brics\n",
        "        self.cl = cl\n",
        "\n",
        "        self.save_path = save_path\n",
        "        self.load_path = load_path\n",
        "\n",
        "        self.pre_trained = True if load_path else False\n",
        "\n",
        "        # atom feature transformation\n",
        "        self.lin_a = Linear(in_channels, hidden_channels)\n",
        "        self.lin_b = Linear(edge_dim, hidden_channels)\n",
        "\n",
        "        # convs block\n",
        "        self.atom_convs = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            conv = NTNConv(hidden_channels, hidden_channels, slices=slices,\n",
        "                           dropout=dropout, edge_dim=hidden_channels)\n",
        "            self.atom_convs.append(conv)\n",
        "\n",
        "        self.lin_gate = Linear(3 * hidden_channels, hidden_channels)\n",
        "\n",
        "        if self.f_att:\n",
        "            self.feature_att = FeatureAttention(channels=hidden_channels, reduction=r)\n",
        "\n",
        "        if self.brics:\n",
        "            # mol-fra attention\n",
        "            self.cross_att = GATConv(hidden_channels, hidden_channels, heads=4,\n",
        "                                     dropout=dropout, add_self_loops=False,\n",
        "                                     negative_slope=0.01, concat=False)\n",
        "\n",
        "        if self.brics:\n",
        "            self.out = Linear(2 * hidden_channels, out_channels)\n",
        "        else:\n",
        "            self.out = Linear(hidden_channels, out_channels)\n",
        "\n",
        "        if self.cl:\n",
        "            self.lin_project = Linear(hidden_channels, int(hidden_channels/2))\n",
        "\n",
        "        if self.pre_trained:\n",
        "            self.load_model(load_model)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def save_model(self, save_path):\n",
        "\n",
        "        model_dict = {\n",
        "            'atom_convs': self.atom_convs.state_dict(),\n",
        "            'lin_gate': self.lin_gate.state_dict(),\n",
        "            'lin_a': self.lin_a.state_dict(),\n",
        "            'lin_b': self.lin_b.state_dict(),\n",
        "        }\n",
        "\n",
        "        if self.f_att:\n",
        "            model_dict['feature_att'] =  self.feature_att.state_dict()\n",
        "\n",
        "        if self.brics:\n",
        "            model_dict['cross_att'] = self.cross_att.state_dict()\n",
        "\n",
        "        torch.save(model_dict, save_path)\n",
        "\n",
        "    def load_model(self, load_path):\n",
        "\n",
        "        model_dict = torch.load(load_path)\n",
        "        self.atom_convs.load_state_dict(model_dict['atom_convs'])\n",
        "        self.lin_gate.load_state_dict(model_dict['lin_gate'])\n",
        "        self.lin_a.load_state_dict(model_dict['lin_a'])\n",
        "        self.lin_b.load_state_dict(model_dict['lin_b'])\n",
        "\n",
        "        if self.f_att:\n",
        "            self.feature_att.load_state_dict(model_dict['feature_att'])\n",
        "\n",
        "        if self.brics:\n",
        "            self.cross_att.load_state_dict(model_dict['cross_att'])\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "\n",
        "        self.lin_a.reset_parameters()\n",
        "        self.lin_b.reset_parameters()\n",
        "\n",
        "        for conv in self.atom_convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "        self.lin_gate.reset_parameters()\n",
        "\n",
        "        if self.f_att:\n",
        "            self.feature_att.reset_parameters()\n",
        "\n",
        "        if self.brics:\n",
        "            self.cross_att.reset_parameters()\n",
        "\n",
        "        self.out.reset_parameters()\n",
        "\n",
        "        if self.cl:\n",
        "            self.lin_project.reset_parameters()\n",
        "\n",
        "    def forward(self, data):\n",
        "        # get mol input\n",
        "        x = data.x\n",
        "        edge_index = data.edge_index\n",
        "        edge_attr = data.edge_attr\n",
        "        batch = data.batch\n",
        "\n",
        "        x = F.relu(self.lin_a(x))  # (N, 46) -> (N, hidden_channels)\n",
        "        edge_attr = F.relu(self.lin_b(edge_attr))  # (N, 10) -> (N, hidden_channels)\n",
        "\n",
        "        # mol conv block\n",
        "        for i in range(0, self.num_layers):\n",
        "            h = F.relu(self.atom_convs[i](x, edge_index, edge_attr))\n",
        "            beta = self.lin_gate(torch.cat([x, h, x - h], 1)).sigmoid()\n",
        "            x = beta * x + (1 - beta) * h\n",
        "            if self.f_att:\n",
        "                x = self.feature_att(x, batch)\n",
        "\n",
        "        mol_vec = global_add_pool(x, batch).relu_()\n",
        "\n",
        "        if self.brics:\n",
        "            # get fragment input\n",
        "            fra_x = data.x\n",
        "            fra_edge_index = data.fra_edge_index\n",
        "            fra_edge_attr = data.fra_edge_attr\n",
        "            cluster = data.cluster_index\n",
        "\n",
        "            fra_x = F.relu(self.lin_a(fra_x))  # (N, 46) -> (N, hidden_channels)\n",
        "            fra_edge_attr = F.relu(self.lin_b(fra_edge_attr))  # (N, 10) -> (N, hidden_channels)\n",
        "\n",
        "            # fragment convs block\n",
        "            for i in range(0, self.num_layers):\n",
        "                fra_h = F.relu(self.atom_convs[i](fra_x, fra_edge_index, fra_edge_attr))\n",
        "                beta = self.lin_gate(torch.cat([fra_x, fra_h, fra_x - fra_h], 1)).sigmoid()\n",
        "                fra_x = beta * fra_x + (1 - beta) * fra_h\n",
        "                if self.f_att:\n",
        "                    fra_x = self.feature_att(fra_x, cluster)\n",
        "            st()\n",
        "            fra_x = global_add_pool(fra_x, cluster).relu_()\n",
        "\n",
        "            # get fragment batch\n",
        "            cluster, perm = consecutive_cluster(cluster)\n",
        "            fra_batch = pool_batch(perm, data.batch)\n",
        "\n",
        "            # molecule-fragment attention\n",
        "            row = torch.arange(fra_batch.size(0), device=batch.device)\n",
        "            mol_fra_index = torch.stack([row, fra_batch], dim=0)\n",
        "            fra_vec = self.cross_att((fra_x, mol_vec), mol_fra_index).relu_()\n",
        "\n",
        "            vectors_concat = list()\n",
        "            vectors_concat.append(mol_vec)\n",
        "            vectors_concat.append(fra_vec)\n",
        "\n",
        "            out = torch.cat(vectors_concat, 1)\n",
        "\n",
        "            # molecule-fragment contrastive\n",
        "            if self.cl:\n",
        "                out = F.dropout(out, p=self.dropout, training=self.training)\n",
        "                return self.out(out), self.lin_project(mol_vec).relu_(), self.lin_project(fra_vec).relu_()\n",
        "            else:\n",
        "                out = F.dropout(out, p=self.dropout, training=self.training)\n",
        "                return self.out(out)\n",
        "\n",
        "        else:\n",
        "            assert self.cl is False\n",
        "            out = F.dropout(mol_vec, p=self.dropout, training=self.training)\n",
        "            return self.out(out)\n"
      ],
      "metadata": {
        "id": "plyDNfrCP_rj"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HiGNN(train_dataset.num_node_features, 64, 1, train_dataset.num_edge_features, 4,\n",
        "                 4, dropout = 0.3, f_att=True, r=4, brics=True, cl=False, save_path = None, load_path = None)"
      ],
      "metadata": {
        "id": "pF1-DrLlcosE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4DTrZxsc5lq",
        "outputId": "cfaf6bbc-111a-47b5-be6d-f005beda6fbd"
      },
      "execution_count": 41,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m/tmp/ipykernel_17458/2043465054.py\u001b[0m(149)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    148 \u001b[0;31m            \u001b[0mst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 149 \u001b[0;31m            \u001b[0mfra_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_add_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfra_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    150 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m/tmp/ipykernel_17458/2043465054.py\u001b[0m(152)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    151 \u001b[0;31m            \u001b[0;31m# get fragment batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 152 \u001b[0;31m            \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsecutive_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    153 \u001b[0;31m            \u001b[0mfra_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m/tmp/ipykernel_17458/2043465054.py\u001b[0m(153)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    152 \u001b[0;31m            \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsecutive_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 153 \u001b[0;31m            \u001b[0mfra_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    154 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m/tmp/ipykernel_17458/2043465054.py\u001b[0m(156)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    155 \u001b[0;31m            \u001b[0;31m# molecule-fragment attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 156 \u001b[0;31m            \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfra_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    157 \u001b[0;31m            \u001b[0mmol_fra_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfra_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> fra_batch\n",
            "tensor([ 0,  1,  1,  1,  2,  2,  2,  3,  4,  5,  6,  6,  6,  7,  7,  7,  8,  9,\n",
            "        10, 10, 11, 11, 11, 12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 15, 15, 15,\n",
            "        16, 16, 16, 16, 17, 17, 18, 18, 19, 20, 20, 21, 22, 22, 22, 22, 22, 22,\n",
            "        23, 23, 24, 24, 24, 25, 25, 25, 25, 26, 26, 27, 28, 29, 30, 30, 30, 30,\n",
            "        30, 30, 31, 31, 31])\n",
            "ipdb> fra_batch.shape\n",
            "torch.Size([77])\n",
            "ipdb> \n",
            "torch.Size([77])\n",
            "ipdb> n\n",
            "> \u001b[0;32m/tmp/ipykernel_17458/2043465054.py\u001b[0m(157)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    156 \u001b[0;31m            \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfra_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 157 \u001b[0;31m            \u001b[0mmol_fra_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfra_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    158 \u001b[0;31m            \u001b[0mfra_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_att\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfra_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol_fra_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> row\n",
            "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
            "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
            "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
            "        72, 73, 74, 75, 76])\n",
            "ipdb> n\n",
            "> \u001b[0;32m/tmp/ipykernel_17458/2043465054.py\u001b[0m(158)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    157 \u001b[0;31m            \u001b[0mmol_fra_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfra_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 158 \u001b[0;31m            \u001b[0mfra_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_att\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfra_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol_fra_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    159 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> mol_fra_index.shape\n",
            "torch.Size([2, 77])\n",
            "ipdb> mol_fra_index\n",
            "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
            "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
            "         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
            "         72, 73, 74, 75, 76],\n",
            "        [ 0,  1,  1,  1,  2,  2,  2,  3,  4,  5,  6,  6,  6,  7,  7,  7,  8,  9,\n",
            "         10, 10, 11, 11, 11, 12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 15, 15, 15,\n",
            "         16, 16, 16, 16, 17, 17, 18, 18, 19, 20, 20, 21, 22, 22, 22, 22, 22, 22,\n",
            "         23, 23, 24, 24, 24, 25, 25, 25, 25, 26, 26, 27, 28, 29, 30, 30, 30, 30,\n",
            "         30, 30, 31, 31, 31]])\n",
            "ipdb> fra_x.shape\n",
            "torch.Size([77, 64])\n",
            "ipdb> mol_vec.shape\n",
            "torch.Size([32, 64])\n",
            "ipdb> exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-02, weight_decay = 1e-02)\n",
        "criterion = torch.nn.MSELoss()\n",
        "model.to(device)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        data.to(device)\n",
        "        out = model(data)\n",
        "        # out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "    rmses = []\n",
        "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "        data.to(device)\n",
        "        out = model(data)\n",
        "        out.squeeze()\n",
        "        rmse = torch.sqrt(torch.mean((out - data.y)**2))\n",
        "        rmses.append(rmse.item())\n",
        "    return np.mean(rmses)\n",
        "\n",
        "for epoch in range(1, 171):\n",
        "    train()\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "f1njvzwoc-2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OWN CODE EXPERIMENTS WITH ESOL DATASET**"
      ],
      "metadata": {
        "id": "FIxQsaMMHxwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "jNLl8fZCIVzC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_layer(in_channels, out_channels):\n",
        "  return nn.Sequential(nn.Linear(in_channels, out_channels), \n",
        "                      nn.BatchNorm1d(out_channels),\n",
        "                      nn.LeakyReLU(1e-02))\n",
        "  \n",
        "def attn_heads(in_channels, out_channels):\n",
        "  return nn.Sequential(nn.Linear(in_channels, out_channels), nn.Tanh())\n",
        "\n",
        "class ResLayer(nn.Module):\n",
        "  def __init__(self, in_c, out_c):\n",
        "    super().__init__()\n",
        "    self.l1 = linear_layer(in_c, out_c)\n",
        "    self.l2 = linear_layer(out_c, out_c)\n",
        "    self.l3 = linear_layer(out_c, out_c)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.l1(x)\n",
        "    return self.l3(self.l2(x)) + x"
      ],
      "metadata": {
        "id": "xrLdc8E6HDt6"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATv2ConvEV2(MessagePassing):\n",
        "\n",
        "  def __init__(self, n_in, n_out, e_in, e_out):\n",
        "    super().__init__(aggr = 'add')\n",
        "    self.n_transform = ResLayer(n_in, n_out)\n",
        "    self.e_transform = ResLayer(e_in, e_out)\n",
        "\n",
        "    # Node\n",
        "    self.n_attn = attn_heads(2*n_out + e_out, 1)\n",
        "    self.e_attn = attn_heads(2*n_out + e_out, 1)\n",
        "    self.s_attn = attn_heads(2*n_out + e_out, 1)\n",
        "    self.n_message = ResLayer(e_out + 2*n_out , n_out)\n",
        "\n",
        "    # For Edge\n",
        "    self.n1_attn = ResLayer(e_out + 2*n_out, 1)\n",
        "    self.n2_attn = ResLayer(e_out + 2*n_out, 1)\n",
        "    self.s_e_attn = ResLayer(e_out + 2*n_out, 1)\n",
        "\n",
        "    # For Node Update\n",
        "    self.u_m_attn = attn_heads(2*n_out, 1)\n",
        "    self.u_s_attn = attn_heads(2*n_out, 1)\n",
        "    self.n_update = ResLayer(2*n_out, n_out)\n",
        "\n",
        "    self.e_update = ResLayer(e_out + 2*n_out, e_out)\n",
        "\n",
        "    self.n_in = n_in\n",
        "    self.n_out = n_out\n",
        "    self.e_in = e_in\n",
        "    self.e_out = e_out\n",
        "\n",
        "  def forward(self, x, edge_index, edge_attr):\n",
        "    # st()\n",
        "    x = self.n_transform(x)\n",
        "    edge_attr = self.e_transform(edge_attr)\n",
        "\n",
        "    node_features = self.propagate(edge_index, x = x, edge_attr = edge_attr)\n",
        "    edge_features = self.edge_updater(edge_index, x = x, edge_attr = edge_attr)\n",
        "\n",
        "    return node_features, edge_features\n",
        "  \n",
        "  def edge_update(self, edge_index, x_i, x_j, edge_attr):\n",
        "    # 2. Edge Message Calculation\n",
        "\n",
        "    input = torch.cat([x_i, x_j, edge_attr], dim = 1)\n",
        "    n1_attn = self.n1_attn(input)\n",
        "    n2_attn = self.n2_attn(input)\n",
        "    self_attn = self.s_e_attn(input)\n",
        "\n",
        "    edge_message = torch.cat([n1_attn*x_i, n2_attn*x_j, self_attn*edge_attr], dim = 1)\n",
        "    return self.e_update(edge_message) + edge_attr\n",
        "  \n",
        "  def message(self, x_i, x_j, edge_index, edge_attr):\n",
        "\n",
        "    input = torch.cat([x_i, x_j, edge_attr], dim = 1)\n",
        "    # 1. Node Message Calculation\n",
        "    \n",
        "    neighbour_attn = self.n_attn(input)\n",
        "    edge_attn = self.e_attn(input)\n",
        "    self_attn = self.s_attn(input)\n",
        "    \n",
        "    # 1.1 Attention Softmax\n",
        "    neighbour_attn = softmax(neighbour_attn, edge_index[0])\n",
        "    edge_attn = softmax(edge_attn, edge_index[0])\n",
        "    self_attn = softmax(self_attn, edge_index[0])\n",
        "\n",
        "    # 1.2 Gather the message\n",
        "    node_message = torch.cat([self_attn*x_i, neighbour_attn*x_j, edge_attn*edge_attr], dim = 1)\n",
        "\n",
        "    op = self.n_message(node_message)\n",
        "    return op\n",
        "  \n",
        "  def update(self, aggregated_output, x):\n",
        "    '''\n",
        "    input = aggregated_output || x\n",
        "    message_attn = msg_attn(input)\n",
        "    self_attn = self_attn(input)\n",
        "\n",
        "    h` = update(message_attn*aggregated_output || self_attn*x)\n",
        "    return h`\n",
        "    '''\n",
        "\n",
        "    input = torch.cat([aggregated_output, x], dim = 1)\n",
        "    m_attn = self.u_m_attn(input)\n",
        "    s_attn = self.u_s_attn(input)\n",
        "\n",
        "    return self.n_update(torch.cat([m_attn*aggregated_output, s_attn*x], dim = 1))\n",
        "  "
      ],
      "metadata": {
        "id": "3DrphAWmGyFi"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATV2(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, dataset = train_dataset, regression = False):\n",
        "        super(GATV2, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GATv2ConvEV2(dataset.num_node_features, hidden_channels, dataset.num_edge_features, hidden_channels)\n",
        "        self.conv2 = GATv2ConvEV2(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        self.conv3 = GATv2ConvEV2(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        self.conv4 = GATv2ConvEV2(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        self.regression = regression\n",
        "        if regression:\n",
        "          self.lin = nn.Sequential(nn.Linear(hidden_channels, 1))\n",
        "        else:\n",
        "          self.lin = ResLayer(hidden_channels, dataset.num_classes)  \n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x, edge_attr = self.conv1(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv2(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv3(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv4(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.2), F.dropout(edge_attr, p = 0.2)\n",
        "       \n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        if self.regression:\n",
        "          x = self.lin(x)\n",
        "        else:\n",
        "          x = F.sigmoid(self.lin(x))\n",
        "\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "# print(model)"
      ],
      "metadata": {
        "id": "7orw4kleHPvV"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GATV2(hidden_channels=64, dataset= train_dataset, regression= True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-03, weight_decay = 1e-03)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode = 'min',\n",
        "            factor = 0.7,\n",
        "            patience = 10,\n",
        "            min_lr = 1e-04\n",
        ")\n",
        "criterion = torch.nn.MSELoss()\n",
        "model.to(device)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        # out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "        train_losses.append(loss.item())\n",
        "    \n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            data.to(device)\n",
        "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "            loss = criterion(out, data.y)\n",
        "            val_losses.append(loss.item())\n",
        "        lr_scheduler.step(np.mean(val_losses))\n",
        "    \n",
        "    print(f'Train RMSE: {np.mean(np.sqrt(train_losses))}, Valid Losses: {np.mean(np.sqrt(val_losses))}')\n",
        "\n",
        "\n",
        "for epoch in range(1, 250):\n",
        "    train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMhzCqeQHYpO",
        "outputId": "3031abf0-d975-40f3-d193-3dd4a9fe69dd"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train RMSE: 2.993682006651612, Valid Losses: 2.4043797577130395\n",
            "Train RMSE: 1.9306624882965726, Valid Losses: 1.8005640091299697\n",
            "Train RMSE: 1.6296609466490886, Valid Losses: 1.560132269348957\n",
            "Train RMSE: 1.4475995749382655, Valid Losses: 1.3290248845744363\n",
            "Train RMSE: 1.3550906248505865, Valid Losses: 1.2019043465459558\n",
            "Train RMSE: 1.2838099278845816, Valid Losses: 1.178157868759243\n",
            "Train RMSE: 1.2672858836303853, Valid Losses: 1.1400192380227576\n",
            "Train RMSE: 1.2049269980777262, Valid Losses: 1.15298811139987\n",
            "Train RMSE: 1.1960482218903963, Valid Losses: 1.0377773682573859\n",
            "Train RMSE: 1.1744690118063388, Valid Losses: 0.9021600199427882\n",
            "Train RMSE: 1.1802060640661687, Valid Losses: 1.027458092201556\n",
            "Train RMSE: 1.158436339956698, Valid Losses: 1.0431835991560299\n",
            "Train RMSE: 1.0703587298060755, Valid Losses: 1.003836080413531\n",
            "Train RMSE: 1.0580781539759643, Valid Losses: 0.95708086569633\n",
            "Train RMSE: 1.0673692382232072, Valid Losses: 0.8761827983302257\n",
            "Train RMSE: 1.0324406109698159, Valid Losses: 0.9705622418295381\n",
            "Train RMSE: 1.124775745510115, Valid Losses: 0.902250062282819\n",
            "Train RMSE: 1.0239808406862942, Valid Losses: 0.8716060644102436\n",
            "Train RMSE: 1.0363173043068066, Valid Losses: 0.9096540789978493\n",
            "Train RMSE: 0.9945552710715463, Valid Losses: 0.9259326136570442\n",
            "Train RMSE: 0.9825309627024825, Valid Losses: 0.8854909067365271\n",
            "Train RMSE: 0.968580906491703, Valid Losses: 0.8960912824178121\n",
            "Train RMSE: 0.9518256509969375, Valid Losses: 1.0815178783328145\n",
            "Train RMSE: 1.0360680205908896, Valid Losses: 0.9514859339762439\n",
            "Train RMSE: 0.9941734767753958, Valid Losses: 0.8841799400824422\n",
            "Train RMSE: 0.9562710112749205, Valid Losses: 1.10168880420841\n",
            "Train RMSE: 0.9674720911565298, Valid Losses: 0.8851903329382849\n",
            "Train RMSE: 1.0302150478458971, Valid Losses: 0.9563880248638206\n",
            "Train RMSE: 1.0124710732006603, Valid Losses: 1.0377400899478184\n",
            "Train RMSE: 0.9639669109340648, Valid Losses: 0.9169109245560509\n",
            "Train RMSE: 0.958802132565806, Valid Losses: 0.8394890540464777\n",
            "Train RMSE: 0.9502977898789396, Valid Losses: 0.7998155965070666\n",
            "Train RMSE: 0.926745892525107, Valid Losses: 0.8153535571834882\n",
            "Train RMSE: 0.9578365848777949, Valid Losses: 0.8361047093040357\n",
            "Train RMSE: 0.9559282449585056, Valid Losses: 0.8684660153812069\n",
            "Train RMSE: 0.9825737149366734, Valid Losses: 0.8500575821377148\n",
            "Train RMSE: 0.937178494473826, Valid Losses: 1.0098305874999782\n",
            "Train RMSE: 0.9209083210570577, Valid Losses: 0.8249241966367677\n",
            "Train RMSE: 0.9814534744092546, Valid Losses: 0.8049277367007519\n",
            "Train RMSE: 0.8888841281443716, Valid Losses: 0.8315863651677721\n",
            "Train RMSE: 0.8815133425203163, Valid Losses: 0.8245144370157158\n",
            "Train RMSE: 0.9111581338022912, Valid Losses: 0.7378683201873009\n",
            "Train RMSE: 0.9384627698833209, Valid Losses: 0.7331313589727576\n",
            "Train RMSE: 0.9212886010158082, Valid Losses: 0.839862377349343\n",
            "Train RMSE: 0.957316435795594, Valid Losses: 0.8324335270844714\n",
            "Train RMSE: 0.899473772658087, Valid Losses: 0.7780809646262932\n",
            "Train RMSE: 0.9164819252638314, Valid Losses: 0.7975641573371332\n",
            "Train RMSE: 0.9900072529118079, Valid Losses: 0.836453295345315\n",
            "Train RMSE: 0.8987820221915958, Valid Losses: 0.8110998239610799\n",
            "Train RMSE: 0.861667916032611, Valid Losses: 0.7969361257141736\n",
            "Train RMSE: 0.8688030671347311, Valid Losses: 0.9059108664937756\n",
            "Train RMSE: 0.8587603780460888, Valid Losses: 0.8587218869075735\n",
            "Train RMSE: 0.8632301635573729, Valid Losses: 0.9489993368999398\n",
            "Train RMSE: 0.8859143603969127, Valid Losses: 0.7547522465951103\n",
            "Train RMSE: 0.8362331335592752, Valid Losses: 0.8436074339376899\n",
            "Train RMSE: 0.8364937359598804, Valid Losses: 0.6867511166717555\n",
            "Train RMSE: 0.8517298591413357, Valid Losses: 0.7558975924535768\n",
            "Train RMSE: 0.7900978684334388, Valid Losses: 0.6985138700776747\n",
            "Train RMSE: 0.8829872452898223, Valid Losses: 0.7654065248100108\n",
            "Train RMSE: 0.8586170648392601, Valid Losses: 0.7759708227337297\n",
            "Train RMSE: 0.8397400311074169, Valid Losses: 0.7664760307028855\n",
            "Train RMSE: 0.8233080718000556, Valid Losses: 0.7391804293708732\n",
            "Train RMSE: 0.8052483510924544, Valid Losses: 0.7046557957421814\n",
            "Train RMSE: 0.8436674113654082, Valid Losses: 0.712470403150294\n",
            "Train RMSE: 0.8367783013215278, Valid Losses: 0.7523720747593291\n",
            "Train RMSE: 0.7966119566945161, Valid Losses: 0.7234952651223241\n",
            "Train RMSE: 0.8216605718991327, Valid Losses: 0.7491488555212862\n",
            "Train RMSE: 0.8198445870078003, Valid Losses: 0.7415350944720333\n",
            "Train RMSE: 0.8001279012189452, Valid Losses: 0.7327550589591523\n",
            "Train RMSE: 0.762215831127921, Valid Losses: 0.7622376164452545\n",
            "Train RMSE: 0.8151099493363845, Valid Losses: 0.7048141038097241\n",
            "Train RMSE: 0.773250513080851, Valid Losses: 0.7265248531846547\n",
            "Train RMSE: 0.775475568441389, Valid Losses: 0.6837755333687786\n",
            "Train RMSE: 0.8268490003192062, Valid Losses: 0.7279411459612349\n",
            "Train RMSE: 0.7873425499700385, Valid Losses: 0.6988762244920037\n",
            "Train RMSE: 0.7898499483216808, Valid Losses: 0.6842957506834357\n",
            "Train RMSE: 0.7818122355752509, Valid Losses: 0.670018249269758\n",
            "Train RMSE: 0.7638357016098117, Valid Losses: 0.7013447340766241\n",
            "Train RMSE: 0.7664675825255012, Valid Losses: 0.7201081868209647\n",
            "Train RMSE: 0.7878982740248781, Valid Losses: 0.7183701052022471\n",
            "Train RMSE: 0.7535447435729431, Valid Losses: 0.6931512141396072\n",
            "Train RMSE: 0.7886710152259973, Valid Losses: 0.7068252075750839\n",
            "Train RMSE: 0.7953295220830937, Valid Losses: 0.7210882452277383\n",
            "Train RMSE: 0.7405312504275432, Valid Losses: 0.731710176091454\n",
            "Train RMSE: 0.7768804099877812, Valid Losses: 0.6873861862058803\n",
            "Train RMSE: 0.8270621281636181, Valid Losses: 0.7012891293078167\n",
            "Train RMSE: 0.7909244286684245, Valid Losses: 0.6724804294227997\n",
            "Train RMSE: 0.7918363111643656, Valid Losses: 0.7452939242461333\n",
            "Train RMSE: 0.7768504597877041, Valid Losses: 0.7028822521893192\n",
            "Train RMSE: 0.7859458268111539, Valid Losses: 0.7768549708100901\n",
            "Train RMSE: 0.7582234061197736, Valid Losses: 0.688530114962243\n",
            "Train RMSE: 0.7201653872201838, Valid Losses: 0.6183625313790718\n",
            "Train RMSE: 0.7183977379739852, Valid Losses: 0.6964637764622558\n",
            "Train RMSE: 0.7625413548203346, Valid Losses: 0.6580211083703824\n",
            "Train RMSE: 0.7993027465350413, Valid Losses: 0.6961530519301627\n",
            "Train RMSE: 0.7350227291612336, Valid Losses: 0.6509180257586613\n",
            "Train RMSE: 0.7564791837699375, Valid Losses: 0.6472910475753865\n",
            "Train RMSE: 0.7547892603195704, Valid Losses: 0.6820399823512526\n",
            "Train RMSE: 0.7265600013000575, Valid Losses: 0.7480355518410962\n",
            "Train RMSE: 0.7681973042551161, Valid Losses: 0.6820803506924329\n",
            "Train RMSE: 0.7343850097179041, Valid Losses: 0.7057563808042528\n",
            "Train RMSE: 0.7428292466556584, Valid Losses: 0.6871633673154359\n",
            "Train RMSE: 0.6922095215120417, Valid Losses: 0.6642546924289882\n",
            "Train RMSE: 0.7534837460928979, Valid Losses: 0.6651245682585537\n",
            "Train RMSE: 0.7274458867180608, Valid Losses: 0.6714241209236269\n",
            "Train RMSE: 0.694079263024035, Valid Losses: 0.7063563659153393\n",
            "Train RMSE: 0.7319043854079194, Valid Losses: 0.6746744034853963\n",
            "Train RMSE: 0.6916932804147756, Valid Losses: 0.6507032361376037\n",
            "Train RMSE: 0.7704087353439723, Valid Losses: 0.7237253490843681\n",
            "Train RMSE: 0.7430481950552277, Valid Losses: 0.6354079456698255\n",
            "Train RMSE: 0.7508778857493108, Valid Losses: 0.6555198819105824\n",
            "Train RMSE: 0.740056088035371, Valid Losses: 0.6415968739714676\n",
            "Train RMSE: 0.7370878533424242, Valid Losses: 0.6637585237452033\n",
            "Train RMSE: 0.7169736778789881, Valid Losses: 0.659899840662683\n",
            "Train RMSE: 0.7391429100452452, Valid Losses: 0.6349242611460774\n",
            "Train RMSE: 0.6840248893977765, Valid Losses: 0.6198660971427985\n",
            "Train RMSE: 0.7178275094553668, Valid Losses: 0.6413743297075485\n",
            "Train RMSE: 0.7134381528343086, Valid Losses: 0.6079378439224338\n",
            "Train RMSE: 0.7240359959451368, Valid Losses: 0.704282029477622\n",
            "Train RMSE: 0.72777589114441, Valid Losses: 0.659266764234138\n",
            "Train RMSE: 0.7053730829469503, Valid Losses: 0.6360289145268048\n",
            "Train RMSE: 0.7035165927784074, Valid Losses: 0.6620019685044698\n",
            "Train RMSE: 0.6598440801698892, Valid Losses: 0.650803073792256\n",
            "Train RMSE: 0.652121074195697, Valid Losses: 0.6782734032051017\n",
            "Train RMSE: 0.7040836345288678, Valid Losses: 0.6399571610515209\n",
            "Train RMSE: 0.7074623907233348, Valid Losses: 0.6421226606274117\n",
            "Train RMSE: 0.7032354468533468, Valid Losses: 0.6298543052180607\n",
            "Train RMSE: 0.6826516871215719, Valid Losses: 0.6420007277920143\n",
            "Train RMSE: 0.7058669340083991, Valid Losses: 0.6334391111884679\n",
            "Train RMSE: 0.6558047749774251, Valid Losses: 0.6199992908827051\n",
            "Train RMSE: 0.6767367713575966, Valid Losses: 0.6381539366099425\n",
            "Train RMSE: 0.7181421955871483, Valid Losses: 0.627119680292076\n",
            "Train RMSE: 0.7201756759365511, Valid Losses: 0.6719636347463254\n",
            "Train RMSE: 0.7346862036751104, Valid Losses: 0.7025285014770281\n",
            "Train RMSE: 0.7355392878799675, Valid Losses: 0.6108081029308126\n",
            "Train RMSE: 0.6692359259785202, Valid Losses: 0.6872171402725981\n",
            "Train RMSE: 0.6991508628814621, Valid Losses: 0.7001757514986109\n",
            "Train RMSE: 0.7616851700563456, Valid Losses: 0.6000945967907015\n",
            "Train RMSE: 0.7243600691576226, Valid Losses: 0.6701453807290347\n",
            "Train RMSE: 0.6726591108107833, Valid Losses: 0.6511768282020024\n",
            "Train RMSE: 0.6711342308452026, Valid Losses: 0.6310376204600645\n",
            "Train RMSE: 0.6847956933604059, Valid Losses: 0.7075279186623058\n",
            "Train RMSE: 0.6480822281136907, Valid Losses: 0.6463269159775495\n",
            "Train RMSE: 0.6988162313860896, Valid Losses: 0.6590337268246884\n",
            "Train RMSE: 0.6426164976615953, Valid Losses: 0.670695567671647\n",
            "Train RMSE: 0.6501254925663096, Valid Losses: 0.6858473806177141\n",
            "Train RMSE: 0.6887900872819036, Valid Losses: 0.7061071648353086\n",
            "Train RMSE: 0.7197870894738989, Valid Losses: 0.6206628082761866\n",
            "Train RMSE: 0.6878133148990446, Valid Losses: 0.6319833819115857\n",
            "Train RMSE: 0.654270649291185, Valid Losses: 0.6394669503968724\n",
            "Train RMSE: 0.7270741276376055, Valid Losses: 0.5880116808419706\n",
            "Train RMSE: 0.6864374581564793, Valid Losses: 0.6039924922319627\n",
            "Train RMSE: 0.6731802474979356, Valid Losses: 0.6006582873887844\n",
            "Train RMSE: 0.6573495381050998, Valid Losses: 0.6211306967235155\n",
            "Train RMSE: 0.681054155313159, Valid Losses: 0.6401376948714007\n",
            "Train RMSE: 0.7328862040682131, Valid Losses: 0.6158355641863325\n",
            "Train RMSE: 0.6699406568559347, Valid Losses: 0.6234483590108783\n",
            "Train RMSE: 0.6975215301580112, Valid Losses: 0.6243795064571518\n",
            "Train RMSE: 0.6437619283207909, Valid Losses: 0.6470052592994369\n",
            "Train RMSE: 0.6818579160844418, Valid Losses: 0.5657313987391586\n",
            "Train RMSE: 0.6682132463895724, Valid Losses: 0.6252843703741195\n",
            "Train RMSE: 0.6566518940084407, Valid Losses: 0.6586287160017926\n",
            "Train RMSE: 0.6613782366102989, Valid Losses: 0.6484077907567729\n",
            "Train RMSE: 0.6440774321467297, Valid Losses: 0.6279555745444203\n",
            "Train RMSE: 0.6665705185363101, Valid Losses: 0.6337425131551101\n",
            "Train RMSE: 0.6990289722724528, Valid Losses: 0.6302048436960965\n",
            "Train RMSE: 0.6554400061763614, Valid Losses: 0.6533517281567424\n",
            "Train RMSE: 0.7385900500513237, Valid Losses: 0.6181488035805484\n",
            "Train RMSE: 0.6842865005322329, Valid Losses: 0.6291254454238608\n",
            "Train RMSE: 0.6724126577214489, Valid Losses: 0.5767631129527033\n",
            "Train RMSE: 0.6533781258767936, Valid Losses: 0.6053194744698747\n",
            "Train RMSE: 0.7143740497312785, Valid Losses: 0.6392854831326968\n",
            "Train RMSE: 0.6276234800293918, Valid Losses: 0.5907335532670784\n",
            "Train RMSE: 0.622461879337006, Valid Losses: 0.5851229871268832\n",
            "Train RMSE: 0.6637814900502248, Valid Losses: 0.62487656736974\n",
            "Train RMSE: 0.6644765749177719, Valid Losses: 0.6200283420713572\n",
            "Train RMSE: 0.6272772189787775, Valid Losses: 0.6102784981417197\n",
            "Train RMSE: 0.6746637406267355, Valid Losses: 0.5875863647641927\n",
            "Train RMSE: 0.6766936489077124, Valid Losses: 0.6263304034115031\n",
            "Train RMSE: 0.6761392292252135, Valid Losses: 0.6427372975134109\n",
            "Train RMSE: 0.6370031931606034, Valid Losses: 0.6212900958445507\n",
            "Train RMSE: 0.670396294692812, Valid Losses: 0.5970679432395725\n",
            "Train RMSE: 0.6502535832184497, Valid Losses: 0.5979307181374076\n",
            "Train RMSE: 0.6951925522293771, Valid Losses: 0.6685201824075754\n",
            "Train RMSE: 0.6514457502663065, Valid Losses: 0.6369217767591635\n",
            "Train RMSE: 0.6520310408789145, Valid Losses: 0.619756956851845\n",
            "Train RMSE: 0.6571484416288123, Valid Losses: 0.6747194339379554\n",
            "Train RMSE: 0.6578076454668482, Valid Losses: 0.6169562110792121\n",
            "Train RMSE: 0.6845496021230871, Valid Losses: 0.6218557842527415\n",
            "Train RMSE: 0.6454297791566621, Valid Losses: 0.7465737743995433\n",
            "Train RMSE: 0.6407808660766251, Valid Losses: 0.668074639614568\n",
            "Train RMSE: 0.6392491397879803, Valid Losses: 0.6517873133470236\n",
            "Train RMSE: 0.6692955424124887, Valid Losses: 0.6578861858805617\n",
            "Train RMSE: 0.6728386409720227, Valid Losses: 0.6577604465163363\n",
            "Train RMSE: 0.7225724932034929, Valid Losses: 0.7073850832380428\n",
            "Train RMSE: 0.6609573258200931, Valid Losses: 0.7097697199974182\n",
            "Train RMSE: 0.6636338622910332, Valid Losses: 0.6315399874576417\n",
            "Train RMSE: 0.6270157427638783, Valid Losses: 0.685835058030334\n",
            "Train RMSE: 0.6824499859159417, Valid Losses: 0.6537620088116214\n",
            "Train RMSE: 0.6570470533647417, Valid Losses: 0.660164422218883\n",
            "Train RMSE: 0.6730225855319554, Valid Losses: 0.626085289448797\n",
            "Train RMSE: 0.6351277353764301, Valid Losses: 0.6663995747924499\n",
            "Train RMSE: 0.6417994033267848, Valid Losses: 0.6941356755477919\n",
            "Train RMSE: 0.667864902234359, Valid Losses: 0.6652915625169231\n",
            "Train RMSE: 0.63838095094128, Valid Losses: 0.6492286514133971\n",
            "Train RMSE: 0.6731565654236895, Valid Losses: 0.6349180223167304\n",
            "Train RMSE: 0.6191785951314203, Valid Losses: 0.6780234886275991\n",
            "Train RMSE: 0.6522734347862303, Valid Losses: 0.6931561851229603\n",
            "Train RMSE: 0.6608917618661748, Valid Losses: 0.6724565134075249\n",
            "Train RMSE: 0.6280681444855025, Valid Losses: 0.6153110263894326\n",
            "Train RMSE: 0.6266401436508161, Valid Losses: 0.6381192204441699\n",
            "Train RMSE: 0.6385245860642769, Valid Losses: 0.6724157293531232\n",
            "Train RMSE: 0.6741369730708359, Valid Losses: 0.6760056791151221\n",
            "Train RMSE: 0.6360009764013947, Valid Losses: 0.6426470034365197\n",
            "Train RMSE: 0.6687511426824525, Valid Losses: 0.6678209150294346\n",
            "Train RMSE: 0.6404402626693626, Valid Losses: 0.7282234572486301\n",
            "Train RMSE: 0.6601416050523178, Valid Losses: 0.6017170670971502\n",
            "Train RMSE: 0.6417871373884518, Valid Losses: 0.6750538412151795\n",
            "Train RMSE: 0.6265516832345737, Valid Losses: 0.5953325890084255\n",
            "Train RMSE: 0.6075931204779695, Valid Losses: 0.6183317646616745\n",
            "Train RMSE: 0.6198809208493699, Valid Losses: 0.6487050390031567\n",
            "Train RMSE: 0.6340671666036466, Valid Losses: 0.6291781976136605\n",
            "Train RMSE: 0.6559062447948376, Valid Losses: 0.6239440238343195\n",
            "Train RMSE: 0.6150755004408994, Valid Losses: 0.7433547483277283\n",
            "Train RMSE: 0.5848484943863093, Valid Losses: 0.6326084354701037\n",
            "Train RMSE: 0.6441460378826537, Valid Losses: 0.6165337554007273\n",
            "Train RMSE: 0.629243164465062, Valid Losses: 0.7329603394673715\n",
            "Train RMSE: 0.6901184887388917, Valid Losses: 0.6166724931346178\n",
            "Train RMSE: 0.618284568285811, Valid Losses: 0.658057700431016\n",
            "Train RMSE: 0.6268767207492992, Valid Losses: 0.6130881766563727\n",
            "Train RMSE: 0.683914221435974, Valid Losses: 0.6379445647050676\n",
            "Train RMSE: 0.6337814861275194, Valid Losses: 0.6251916866611925\n",
            "Train RMSE: 0.6333828784621977, Valid Losses: 0.6049020111763699\n",
            "Train RMSE: 0.649788010864762, Valid Losses: 0.6567402577314523\n",
            "Train RMSE: 0.645463676542634, Valid Losses: 0.6019086591239137\n",
            "Train RMSE: 0.6475069180943044, Valid Losses: 0.6188335992640777\n",
            "Train RMSE: 0.6344213811103274, Valid Losses: 0.6136396359066891\n",
            "Train RMSE: 0.6221028405390917, Valid Losses: 0.5967997761627679\n",
            "Train RMSE: 0.6829259075754934, Valid Losses: 0.6972260081194618\n",
            "Train RMSE: 0.6106023861796901, Valid Losses: 0.6048218840830979\n",
            "Train RMSE: 0.645206830523588, Valid Losses: 0.6252406539374202\n",
            "Train RMSE: 0.6039231244160842, Valid Losses: 0.6461972528171535\n",
            "Train RMSE: 0.6598423929931367, Valid Losses: 0.6222658535612158\n",
            "Train RMSE: 0.6586023193429189, Valid Losses: 0.6473482659173955\n",
            "Train RMSE: 0.6200932852898715, Valid Losses: 0.6198084779772668\n",
            "Train RMSE: 0.6522542299263105, Valid Losses: 0.672222915846159\n",
            "Train RMSE: 0.5922358702574408, Valid Losses: 0.6142719977065053\n",
            "Train RMSE: 0.6070932965968777, Valid Losses: 0.657206133986238\n",
            "Train RMSE: 0.6171047022056914, Valid Losses: 0.6672649790622267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZOgRUVjKTo8",
        "outputId": "0d3e0f04-d9a1-47bf-8bad-b218cca13f80"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One CYCLE LR**"
      ],
      "metadata": {
        "id": "GYqnhBcjj28v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GATV2(hidden_channels=64, dataset= train_dataset, regression= True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-03, weight_decay = 1e-03)\n",
        "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
        "                                                   max_lr = 1e-03, \n",
        "                                                  epochs = 250, \n",
        "                                                  steps_per_epoch = 29)"
      ],
      "metadata": {
        "id": "KoT8EpKKj0Lz"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GATV2(hidden_channels=64, dataset= train_dataset, regression= True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-03, weight_decay = 1e-03)\n",
        "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
        "                                                   max_lr = 1e-03, \n",
        "                                                  epochs = 250, \n",
        "                                                  steps_per_epoch = 29)\n",
        "criterion = torch.nn.MSELoss()\n",
        "model.to(device)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        # out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "        train_losses.append(loss.item())\n",
        "        lr_scheduler.step()\n",
        "    \n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            data.to(device)\n",
        "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "            loss = criterion(out, data.y)\n",
        "            val_losses.append(loss.item())\n",
        "    \n",
        "    return np.mean(np.sqrt(train_losses)), np.mean(np.sqrt(val_losses))\n",
        "    \n",
        "\n",
        "\n",
        "for epoch in range(1, 250):\n",
        "    train_loss , valid_loss = train()\n",
        "    print(f'Epoch: {epoch} - Train RMSE: {train_loss:.3f}, Valid RMSE: {valid_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5xbTARbkbQH",
        "outputId": "33805507-1da7-45fd-eb40-40b46b27fb8f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 - Train RMSE: 3.537, Valid RMSE: 3.532\n",
            "Epoch: 2 - Train RMSE: 3.481, Valid RMSE: 3.412\n",
            "Epoch: 3 - Train RMSE: 3.369, Valid RMSE: 3.360\n",
            "Epoch: 4 - Train RMSE: 3.367, Valid RMSE: 3.348\n",
            "Epoch: 5 - Train RMSE: 3.295, Valid RMSE: 3.378\n",
            "Epoch: 6 - Train RMSE: 3.199, Valid RMSE: 3.334\n",
            "Epoch: 7 - Train RMSE: 3.117, Valid RMSE: 3.426\n",
            "Epoch: 8 - Train RMSE: 3.043, Valid RMSE: 3.460\n",
            "Epoch: 9 - Train RMSE: 2.960, Valid RMSE: 3.281\n",
            "Epoch: 10 - Train RMSE: 2.872, Valid RMSE: 3.311\n",
            "Epoch: 11 - Train RMSE: 2.757, Valid RMSE: 3.195\n",
            "Epoch: 12 - Train RMSE: 2.645, Valid RMSE: 3.146\n",
            "Epoch: 13 - Train RMSE: 2.538, Valid RMSE: 3.111\n",
            "Epoch: 14 - Train RMSE: 2.486, Valid RMSE: 3.036\n",
            "Epoch: 15 - Train RMSE: 2.336, Valid RMSE: 2.927\n",
            "Epoch: 16 - Train RMSE: 2.175, Valid RMSE: 2.811\n",
            "Epoch: 17 - Train RMSE: 2.119, Valid RMSE: 2.728\n",
            "Epoch: 18 - Train RMSE: 2.005, Valid RMSE: 2.525\n",
            "Epoch: 19 - Train RMSE: 1.870, Valid RMSE: 2.451\n",
            "Epoch: 20 - Train RMSE: 1.775, Valid RMSE: 2.377\n",
            "Epoch: 21 - Train RMSE: 1.671, Valid RMSE: 2.433\n",
            "Epoch: 22 - Train RMSE: 1.542, Valid RMSE: 2.188\n",
            "Epoch: 23 - Train RMSE: 1.507, Valid RMSE: 2.018\n",
            "Epoch: 24 - Train RMSE: 1.401, Valid RMSE: 1.763\n",
            "Epoch: 25 - Train RMSE: 1.333, Valid RMSE: 1.702\n",
            "Epoch: 26 - Train RMSE: 1.296, Valid RMSE: 1.426\n",
            "Epoch: 27 - Train RMSE: 1.377, Valid RMSE: 1.447\n",
            "Epoch: 28 - Train RMSE: 1.281, Valid RMSE: 1.809\n",
            "Epoch: 29 - Train RMSE: 1.246, Valid RMSE: 1.239\n",
            "Epoch: 30 - Train RMSE: 1.248, Valid RMSE: 1.229\n",
            "Epoch: 31 - Train RMSE: 1.253, Valid RMSE: 1.377\n",
            "Epoch: 32 - Train RMSE: 1.179, Valid RMSE: 1.287\n",
            "Epoch: 33 - Train RMSE: 1.159, Valid RMSE: 1.192\n",
            "Epoch: 34 - Train RMSE: 1.140, Valid RMSE: 1.143\n",
            "Epoch: 35 - Train RMSE: 1.173, Valid RMSE: 1.201\n",
            "Epoch: 36 - Train RMSE: 1.134, Valid RMSE: 1.154\n",
            "Epoch: 37 - Train RMSE: 1.086, Valid RMSE: 1.251\n",
            "Epoch: 38 - Train RMSE: 1.132, Valid RMSE: 1.283\n",
            "Epoch: 39 - Train RMSE: 1.067, Valid RMSE: 0.896\n",
            "Epoch: 40 - Train RMSE: 1.037, Valid RMSE: 1.402\n",
            "Epoch: 41 - Train RMSE: 1.061, Valid RMSE: 1.427\n",
            "Epoch: 42 - Train RMSE: 1.125, Valid RMSE: 1.077\n",
            "Epoch: 43 - Train RMSE: 1.094, Valid RMSE: 1.139\n",
            "Epoch: 44 - Train RMSE: 1.104, Valid RMSE: 0.993\n",
            "Epoch: 45 - Train RMSE: 1.010, Valid RMSE: 1.032\n",
            "Epoch: 46 - Train RMSE: 1.090, Valid RMSE: 1.236\n",
            "Epoch: 47 - Train RMSE: 1.108, Valid RMSE: 1.429\n",
            "Epoch: 48 - Train RMSE: 1.058, Valid RMSE: 1.862\n",
            "Epoch: 49 - Train RMSE: 1.028, Valid RMSE: 0.988\n",
            "Epoch: 50 - Train RMSE: 0.993, Valid RMSE: 1.313\n",
            "Epoch: 51 - Train RMSE: 0.962, Valid RMSE: 0.984\n",
            "Epoch: 52 - Train RMSE: 0.994, Valid RMSE: 0.930\n",
            "Epoch: 53 - Train RMSE: 0.957, Valid RMSE: 1.107\n",
            "Epoch: 54 - Train RMSE: 0.988, Valid RMSE: 1.021\n",
            "Epoch: 55 - Train RMSE: 0.984, Valid RMSE: 1.688\n",
            "Epoch: 56 - Train RMSE: 1.004, Valid RMSE: 0.974\n",
            "Epoch: 57 - Train RMSE: 0.917, Valid RMSE: 0.983\n",
            "Epoch: 58 - Train RMSE: 0.995, Valid RMSE: 1.086\n",
            "Epoch: 59 - Train RMSE: 0.944, Valid RMSE: 0.977\n",
            "Epoch: 60 - Train RMSE: 0.939, Valid RMSE: 1.216\n",
            "Epoch: 61 - Train RMSE: 0.940, Valid RMSE: 1.405\n",
            "Epoch: 62 - Train RMSE: 0.959, Valid RMSE: 1.007\n",
            "Epoch: 63 - Train RMSE: 0.969, Valid RMSE: 0.823\n",
            "Epoch: 64 - Train RMSE: 0.896, Valid RMSE: 0.877\n",
            "Epoch: 65 - Train RMSE: 0.917, Valid RMSE: 1.168\n",
            "Epoch: 66 - Train RMSE: 0.940, Valid RMSE: 0.906\n",
            "Epoch: 67 - Train RMSE: 0.927, Valid RMSE: 1.784\n",
            "Epoch: 68 - Train RMSE: 0.935, Valid RMSE: 0.876\n",
            "Epoch: 69 - Train RMSE: 0.853, Valid RMSE: 0.878\n",
            "Epoch: 70 - Train RMSE: 0.903, Valid RMSE: 1.858\n",
            "Epoch: 71 - Train RMSE: 0.904, Valid RMSE: 1.150\n",
            "Epoch: 72 - Train RMSE: 0.919, Valid RMSE: 0.905\n",
            "Epoch: 73 - Train RMSE: 0.941, Valid RMSE: 0.948\n",
            "Epoch: 74 - Train RMSE: 0.915, Valid RMSE: 0.815\n",
            "Epoch: 75 - Train RMSE: 0.879, Valid RMSE: 0.903\n",
            "Epoch: 76 - Train RMSE: 0.908, Valid RMSE: 0.957\n",
            "Epoch: 77 - Train RMSE: 0.862, Valid RMSE: 1.057\n",
            "Epoch: 78 - Train RMSE: 0.848, Valid RMSE: 0.898\n",
            "Epoch: 79 - Train RMSE: 0.878, Valid RMSE: 0.852\n",
            "Epoch: 80 - Train RMSE: 0.874, Valid RMSE: 1.088\n",
            "Epoch: 81 - Train RMSE: 0.874, Valid RMSE: 0.925\n",
            "Epoch: 82 - Train RMSE: 0.881, Valid RMSE: 0.909\n",
            "Epoch: 83 - Train RMSE: 0.859, Valid RMSE: 1.821\n",
            "Epoch: 84 - Train RMSE: 0.874, Valid RMSE: 0.869\n",
            "Epoch: 85 - Train RMSE: 0.903, Valid RMSE: 1.190\n",
            "Epoch: 86 - Train RMSE: 0.889, Valid RMSE: 1.136\n",
            "Epoch: 87 - Train RMSE: 0.886, Valid RMSE: 2.183\n",
            "Epoch: 88 - Train RMSE: 0.852, Valid RMSE: 0.879\n",
            "Epoch: 89 - Train RMSE: 0.894, Valid RMSE: 1.168\n",
            "Epoch: 90 - Train RMSE: 0.857, Valid RMSE: 0.812\n",
            "Epoch: 91 - Train RMSE: 0.782, Valid RMSE: 0.969\n",
            "Epoch: 92 - Train RMSE: 0.795, Valid RMSE: 1.080\n",
            "Epoch: 93 - Train RMSE: 0.859, Valid RMSE: 0.849\n",
            "Epoch: 94 - Train RMSE: 0.871, Valid RMSE: 1.165\n",
            "Epoch: 95 - Train RMSE: 0.840, Valid RMSE: 1.143\n",
            "Epoch: 96 - Train RMSE: 0.831, Valid RMSE: 0.803\n",
            "Epoch: 97 - Train RMSE: 0.817, Valid RMSE: 0.866\n",
            "Epoch: 98 - Train RMSE: 0.819, Valid RMSE: 0.863\n",
            "Epoch: 99 - Train RMSE: 0.862, Valid RMSE: 0.935\n",
            "Epoch: 100 - Train RMSE: 0.819, Valid RMSE: 2.123\n",
            "Epoch: 101 - Train RMSE: 0.795, Valid RMSE: 0.772\n",
            "Epoch: 102 - Train RMSE: 0.771, Valid RMSE: 2.564\n",
            "Epoch: 103 - Train RMSE: 0.837, Valid RMSE: 0.933\n",
            "Epoch: 104 - Train RMSE: 0.849, Valid RMSE: 0.830\n",
            "Epoch: 105 - Train RMSE: 0.768, Valid RMSE: 0.738\n",
            "Epoch: 106 - Train RMSE: 0.802, Valid RMSE: 1.050\n",
            "Epoch: 107 - Train RMSE: 0.861, Valid RMSE: 0.809\n",
            "Epoch: 108 - Train RMSE: 0.888, Valid RMSE: 1.133\n",
            "Epoch: 109 - Train RMSE: 0.833, Valid RMSE: 0.809\n",
            "Epoch: 110 - Train RMSE: 0.848, Valid RMSE: 0.862\n",
            "Epoch: 111 - Train RMSE: 0.843, Valid RMSE: 0.962\n",
            "Epoch: 112 - Train RMSE: 0.832, Valid RMSE: 1.004\n",
            "Epoch: 113 - Train RMSE: 0.779, Valid RMSE: 0.881\n",
            "Epoch: 114 - Train RMSE: 0.790, Valid RMSE: 0.956\n",
            "Epoch: 115 - Train RMSE: 0.755, Valid RMSE: 0.761\n",
            "Epoch: 116 - Train RMSE: 0.771, Valid RMSE: 0.860\n",
            "Epoch: 117 - Train RMSE: 0.796, Valid RMSE: 0.812\n",
            "Epoch: 118 - Train RMSE: 0.813, Valid RMSE: 1.169\n",
            "Epoch: 119 - Train RMSE: 0.827, Valid RMSE: 0.749\n",
            "Epoch: 120 - Train RMSE: 0.785, Valid RMSE: 1.436\n",
            "Epoch: 121 - Train RMSE: 0.785, Valid RMSE: 0.774\n",
            "Epoch: 122 - Train RMSE: 0.745, Valid RMSE: 0.952\n",
            "Epoch: 123 - Train RMSE: 0.721, Valid RMSE: 0.825\n",
            "Epoch: 124 - Train RMSE: 0.736, Valid RMSE: 0.760\n",
            "Epoch: 125 - Train RMSE: 0.758, Valid RMSE: 1.325\n",
            "Epoch: 126 - Train RMSE: 0.775, Valid RMSE: 1.552\n",
            "Epoch: 127 - Train RMSE: 0.719, Valid RMSE: 1.051\n",
            "Epoch: 128 - Train RMSE: 0.743, Valid RMSE: 0.796\n",
            "Epoch: 129 - Train RMSE: 0.723, Valid RMSE: 2.252\n",
            "Epoch: 130 - Train RMSE: 0.793, Valid RMSE: 0.737\n",
            "Epoch: 131 - Train RMSE: 0.772, Valid RMSE: 0.996\n",
            "Epoch: 132 - Train RMSE: 0.828, Valid RMSE: 1.092\n",
            "Epoch: 133 - Train RMSE: 0.844, Valid RMSE: 1.781\n",
            "Epoch: 134 - Train RMSE: 0.873, Valid RMSE: 0.905\n",
            "Epoch: 135 - Train RMSE: 0.920, Valid RMSE: 1.203\n",
            "Epoch: 136 - Train RMSE: 0.848, Valid RMSE: 0.968\n",
            "Epoch: 137 - Train RMSE: 0.858, Valid RMSE: 0.745\n",
            "Epoch: 138 - Train RMSE: 0.829, Valid RMSE: 0.970\n",
            "Epoch: 139 - Train RMSE: 0.752, Valid RMSE: 0.984\n",
            "Epoch: 140 - Train RMSE: 0.742, Valid RMSE: 0.734\n",
            "Epoch: 141 - Train RMSE: 0.767, Valid RMSE: 0.861\n",
            "Epoch: 142 - Train RMSE: 0.726, Valid RMSE: 1.265\n",
            "Epoch: 143 - Train RMSE: 0.768, Valid RMSE: 0.789\n",
            "Epoch: 144 - Train RMSE: 0.719, Valid RMSE: 0.707\n",
            "Epoch: 145 - Train RMSE: 0.705, Valid RMSE: 0.769\n",
            "Epoch: 146 - Train RMSE: 0.753, Valid RMSE: 1.373\n",
            "Epoch: 147 - Train RMSE: 0.827, Valid RMSE: 0.868\n",
            "Epoch: 148 - Train RMSE: 0.749, Valid RMSE: 0.916\n",
            "Epoch: 149 - Train RMSE: 0.718, Valid RMSE: 1.164\n",
            "Epoch: 150 - Train RMSE: 0.771, Valid RMSE: 0.725\n",
            "Epoch: 151 - Train RMSE: 0.731, Valid RMSE: 1.823\n",
            "Epoch: 152 - Train RMSE: 0.719, Valid RMSE: 0.693\n",
            "Epoch: 153 - Train RMSE: 0.710, Valid RMSE: 0.765\n",
            "Epoch: 154 - Train RMSE: 0.710, Valid RMSE: 0.817\n",
            "Epoch: 155 - Train RMSE: 0.747, Valid RMSE: 0.778\n",
            "Epoch: 156 - Train RMSE: 0.689, Valid RMSE: 0.784\n",
            "Epoch: 157 - Train RMSE: 0.712, Valid RMSE: 1.167\n",
            "Epoch: 158 - Train RMSE: 0.701, Valid RMSE: 0.686\n",
            "Epoch: 159 - Train RMSE: 0.705, Valid RMSE: 0.737\n",
            "Epoch: 160 - Train RMSE: 0.681, Valid RMSE: 0.756\n",
            "Epoch: 161 - Train RMSE: 0.690, Valid RMSE: 0.769\n",
            "Epoch: 162 - Train RMSE: 0.668, Valid RMSE: 0.633\n",
            "Epoch: 163 - Train RMSE: 0.681, Valid RMSE: 0.804\n",
            "Epoch: 164 - Train RMSE: 0.711, Valid RMSE: 0.724\n",
            "Epoch: 165 - Train RMSE: 0.724, Valid RMSE: 0.812\n",
            "Epoch: 166 - Train RMSE: 0.710, Valid RMSE: 0.731\n",
            "Epoch: 167 - Train RMSE: 0.769, Valid RMSE: 0.961\n",
            "Epoch: 168 - Train RMSE: 0.722, Valid RMSE: 0.837\n",
            "Epoch: 169 - Train RMSE: 0.703, Valid RMSE: 0.661\n",
            "Epoch: 170 - Train RMSE: 0.677, Valid RMSE: 0.787\n",
            "Epoch: 171 - Train RMSE: 0.722, Valid RMSE: 0.691\n",
            "Epoch: 172 - Train RMSE: 0.675, Valid RMSE: 0.684\n",
            "Epoch: 173 - Train RMSE: 0.629, Valid RMSE: 0.729\n",
            "Epoch: 174 - Train RMSE: 0.670, Valid RMSE: 0.701\n",
            "Epoch: 175 - Train RMSE: 0.680, Valid RMSE: 0.748\n",
            "Epoch: 176 - Train RMSE: 0.659, Valid RMSE: 0.855\n",
            "Epoch: 177 - Train RMSE: 0.681, Valid RMSE: 0.882\n",
            "Epoch: 178 - Train RMSE: 0.656, Valid RMSE: 0.803\n",
            "Epoch: 179 - Train RMSE: 0.673, Valid RMSE: 0.692\n",
            "Epoch: 180 - Train RMSE: 0.634, Valid RMSE: 0.715\n",
            "Epoch: 181 - Train RMSE: 0.695, Valid RMSE: 0.754\n",
            "Epoch: 182 - Train RMSE: 0.668, Valid RMSE: 0.731\n",
            "Epoch: 183 - Train RMSE: 0.695, Valid RMSE: 0.875\n",
            "Epoch: 184 - Train RMSE: 0.668, Valid RMSE: 0.714\n",
            "Epoch: 185 - Train RMSE: 0.677, Valid RMSE: 0.760\n",
            "Epoch: 186 - Train RMSE: 0.678, Valid RMSE: 0.697\n",
            "Epoch: 187 - Train RMSE: 0.642, Valid RMSE: 0.816\n",
            "Epoch: 188 - Train RMSE: 0.652, Valid RMSE: 0.695\n",
            "Epoch: 189 - Train RMSE: 0.652, Valid RMSE: 0.751\n",
            "Epoch: 190 - Train RMSE: 0.655, Valid RMSE: 0.800\n",
            "Epoch: 191 - Train RMSE: 0.639, Valid RMSE: 0.745\n",
            "Epoch: 192 - Train RMSE: 0.670, Valid RMSE: 0.720\n",
            "Epoch: 193 - Train RMSE: 0.651, Valid RMSE: 0.786\n",
            "Epoch: 194 - Train RMSE: 0.695, Valid RMSE: 0.696\n",
            "Epoch: 195 - Train RMSE: 0.653, Valid RMSE: 0.900\n",
            "Epoch: 196 - Train RMSE: 0.642, Valid RMSE: 0.735\n",
            "Epoch: 197 - Train RMSE: 0.627, Valid RMSE: 0.679\n",
            "Epoch: 198 - Train RMSE: 0.674, Valid RMSE: 0.770\n",
            "Epoch: 199 - Train RMSE: 0.648, Valid RMSE: 0.705\n",
            "Epoch: 200 - Train RMSE: 0.664, Valid RMSE: 0.703\n",
            "Epoch: 201 - Train RMSE: 0.621, Valid RMSE: 0.738\n",
            "Epoch: 202 - Train RMSE: 0.648, Valid RMSE: 0.727\n",
            "Epoch: 203 - Train RMSE: 0.641, Valid RMSE: 0.724\n",
            "Epoch: 204 - Train RMSE: 0.604, Valid RMSE: 0.701\n",
            "Epoch: 205 - Train RMSE: 0.638, Valid RMSE: 0.644\n",
            "Epoch: 206 - Train RMSE: 0.606, Valid RMSE: 0.646\n",
            "Epoch: 207 - Train RMSE: 0.617, Valid RMSE: 0.675\n",
            "Epoch: 208 - Train RMSE: 0.638, Valid RMSE: 0.688\n",
            "Epoch: 209 - Train RMSE: 0.615, Valid RMSE: 0.748\n",
            "Epoch: 210 - Train RMSE: 0.630, Valid RMSE: 0.708\n",
            "Epoch: 211 - Train RMSE: 0.619, Valid RMSE: 0.718\n",
            "Epoch: 212 - Train RMSE: 0.659, Valid RMSE: 0.745\n",
            "Epoch: 213 - Train RMSE: 0.625, Valid RMSE: 0.652\n",
            "Epoch: 214 - Train RMSE: 0.641, Valid RMSE: 0.729\n",
            "Epoch: 215 - Train RMSE: 0.603, Valid RMSE: 0.766\n",
            "Epoch: 216 - Train RMSE: 0.633, Valid RMSE: 0.733\n",
            "Epoch: 217 - Train RMSE: 0.618, Valid RMSE: 0.736\n",
            "Epoch: 218 - Train RMSE: 0.598, Valid RMSE: 0.651\n",
            "Epoch: 219 - Train RMSE: 0.589, Valid RMSE: 0.685\n",
            "Epoch: 220 - Train RMSE: 0.590, Valid RMSE: 0.695\n",
            "Epoch: 221 - Train RMSE: 0.593, Valid RMSE: 0.700\n",
            "Epoch: 222 - Train RMSE: 0.620, Valid RMSE: 0.681\n",
            "Epoch: 223 - Train RMSE: 0.588, Valid RMSE: 0.690\n",
            "Epoch: 224 - Train RMSE: 0.550, Valid RMSE: 0.684\n",
            "Epoch: 225 - Train RMSE: 0.597, Valid RMSE: 0.687\n",
            "Epoch: 226 - Train RMSE: 0.575, Valid RMSE: 0.719\n",
            "Epoch: 227 - Train RMSE: 0.619, Valid RMSE: 0.666\n",
            "Epoch: 228 - Train RMSE: 0.580, Valid RMSE: 0.654\n",
            "Epoch: 229 - Train RMSE: 0.594, Valid RMSE: 0.693\n",
            "Epoch: 230 - Train RMSE: 0.622, Valid RMSE: 0.671\n",
            "Epoch: 231 - Train RMSE: 0.586, Valid RMSE: 0.723\n",
            "Epoch: 232 - Train RMSE: 0.572, Valid RMSE: 0.683\n",
            "Epoch: 233 - Train RMSE: 0.577, Valid RMSE: 0.680\n",
            "Epoch: 234 - Train RMSE: 0.596, Valid RMSE: 0.738\n",
            "Epoch: 235 - Train RMSE: 0.628, Valid RMSE: 0.662\n",
            "Epoch: 236 - Train RMSE: 0.595, Valid RMSE: 0.701\n",
            "Epoch: 237 - Train RMSE: 0.583, Valid RMSE: 0.636\n",
            "Epoch: 238 - Train RMSE: 0.644, Valid RMSE: 0.658\n",
            "Epoch: 239 - Train RMSE: 0.575, Valid RMSE: 0.701\n",
            "Epoch: 240 - Train RMSE: 0.601, Valid RMSE: 0.709\n",
            "Epoch: 241 - Train RMSE: 0.569, Valid RMSE: 0.666\n",
            "Epoch: 242 - Train RMSE: 0.609, Valid RMSE: 0.650\n",
            "Epoch: 243 - Train RMSE: 0.607, Valid RMSE: 0.754\n",
            "Epoch: 244 - Train RMSE: 0.584, Valid RMSE: 0.692\n",
            "Epoch: 245 - Train RMSE: 0.616, Valid RMSE: 0.728\n",
            "Epoch: 246 - Train RMSE: 0.559, Valid RMSE: 0.668\n",
            "Epoch: 247 - Train RMSE: 0.568, Valid RMSE: 0.736\n",
            "Epoch: 248 - Train RMSE: 0.572, Valid RMSE: 0.719\n",
            "Epoch: 249 - Train RMSE: 0.594, Valid RMSE: 0.671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**New Version of Network**"
      ],
      "metadata": {
        "id": "q28_5Q8nsJWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GATv2ConvEV3(MessagePassing):\n",
        "\n",
        "  def __init__(self, n_in, n_out, e_in, e_out):\n",
        "    super().__init__(aggr = 'add')\n",
        "    self.n_transform = ResLayer(n_in, n_out)\n",
        "    self.e_transform = ResLayer(e_in, e_out)\n",
        "\n",
        "    # Node\n",
        "    self.msg = ResLayer(n_out*2 + e_out, n_out)\n",
        "    self.n_attn = attn_heads(n_out*2 + e_out, 1)\n",
        "\n",
        "    self.self_attn = attn_heads(n_out*2, 1)\n",
        "    self.msg_attn = attn_heads(n_out*2, 1)\n",
        "    self.msg_norm = nn.BatchNorm1d(n_out)\n",
        "    self.n_update = attn_heads(2*n_out, n_out)\n",
        "\n",
        "    # Edge\n",
        "    self.e_msg = ResLayer(n_out*2 + e_out, e_out)\n",
        "    self.e_msg_attn = attn_heads(2*e_out, 1)\n",
        "    self.e_self_attn = attn_heads(2*e_out, 1)\n",
        "\n",
        "    self.e_update = ResLayer(2*e_out, e_out)\n",
        " \n",
        "  def forward(self, x, edge_index, edge_attr):\n",
        "\n",
        "    x = self.n_transform(x)\n",
        "    edge_attr = self.e_transform(edge_attr)\n",
        "\n",
        "    node_features = self.propagate(edge_index, x = x, edge_attr = edge_attr)\n",
        "    edge_features = self.edge_updater(edge_index, x = x, edge_attr = edge_attr)\n",
        "\n",
        "    return node_features, edge_features\n",
        "  \n",
        "  def edge_update(self, edge_index, x_i, x_j, edge_attr):\n",
        "    # 2. Edge Message Calculation\n",
        "\n",
        "    input = torch.cat([x_i, x_j, edge_attr], dim = 1)\n",
        "    edge_message = self.e_msg(input)\n",
        "\n",
        "    input = torch.cat([edge_attr, edge_message], dim = 1)\n",
        "    e_attn = self.e_msg_attn(input)\n",
        "    s_attn = self.e_self_attn(input)\n",
        "\n",
        "    return self.e_update(torch.cat([e_attn*edge_message, s_attn*edge_attr], dim = 1)) + edge_attr\n",
        "  \n",
        "  def message(self, x_i, x_j, edge_index, edge_attr):\n",
        "\n",
        "    input = torch.cat([x_i, x_j, edge_attr], dim = 1)\n",
        "    # 1. Node Message Calculation\n",
        "    msg = self.msg(input)\n",
        "    n_attn = self.n_attn(input)\n",
        "\n",
        "    n_attn = softmax(n_attn, edge_index[0])\n",
        "    return n_attn * msg\n",
        "    \n",
        "   \n",
        "  def update(self, aggregated_output, x):\n",
        "\n",
        "    aggregated_output = self.msg_norm(aggregated_output)\n",
        "    \n",
        "    input = torch.cat([aggregated_output, x], dim = 1)\n",
        "    m_attn = self.msg_attn(input)\n",
        "    s_attn = self.self_attn(input)\n",
        "\n",
        "    return self.n_update(torch.cat([m_attn*aggregated_output, s_attn*x], dim = 1)) + x"
      ],
      "metadata": {
        "id": "0pWpblTHsIgc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.num_node_features, train_dataset.num_edge_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcEuERdoCaT8",
        "outputId": "b8d7bbb8-3787-4910-d81b-e540fec5e86b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_layer = GATv2ConvEV2(train_dataset.num_node_features, 64, train_dataset.num_edge_features, 32)"
      ],
      "metadata": {
        "id": "QM16gCFmCetq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_layer(batch.x, batch.edge_index, batch.edge_attr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzsydbr5C6b5",
        "outputId": "0a7ca3b6-def2-45a0-feac-da368f873284"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-3.7840e-04, -3.2419e-03, -2.3486e-03,  ...,  1.8757e-01,\n",
              "          -2.6538e-03, -1.0874e-04],\n",
              "         [ 1.3851e+00, -4.6392e-03,  1.6738e+00,  ..., -3.0314e-03,\n",
              "          -3.4369e-03,  4.6408e-01],\n",
              "         [-3.7840e-04, -3.2419e-03, -2.3486e-03,  ...,  1.8757e-01,\n",
              "          -2.6538e-03, -1.0874e-04],\n",
              "         ...,\n",
              "         [-2.5238e-03, -1.6228e-03, -3.1280e-03,  ..., -2.5991e-04,\n",
              "          -3.1683e-03,  2.0472e-03],\n",
              "         [-2.3685e-03, -2.5353e-03, -2.4958e-03,  ..., -2.1541e-04,\n",
              "          -2.6862e-03, -3.2880e-04],\n",
              "         [ 6.0563e-04, -3.1276e-03,  3.2228e-01,  ..., -1.5571e-04,\n",
              "           4.3979e-01,  3.1364e-01]], grad_fn=<AddBackward0>),\n",
              " tensor([[ 1.1414,  0.6479,  1.1044,  ...,  1.5050,  1.4114,  1.2883],\n",
              "         [ 0.0942,  0.6519,  0.2624,  ...,  1.0635,  2.0526,  1.2396],\n",
              "         [ 0.0942,  0.6519,  0.2624,  ...,  1.0635,  2.0526,  1.2396],\n",
              "         ...,\n",
              "         [-0.0118,  0.9659,  0.8857,  ...,  2.3765,  2.2921,  0.6812],\n",
              "         [ 0.8222,  1.3104,  1.0727,  ...,  1.7262,  1.6948,  0.1363],\n",
              "         [ 0.1820,  1.7875,  1.0824,  ...,  1.0870,  1.6713,  1.1541]],\n",
              "        grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GATV2_2(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, dataset = train_dataset, regression = False):\n",
        "        super(GATV2_2, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GATv2ConvEV3(dataset.num_node_features, hidden_channels, dataset.num_edge_features, hidden_channels)\n",
        "        self.conv2 = GATv2ConvEV3(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        self.conv3 = GATv2ConvEV3(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        self.conv4 = GATv2ConvEV3(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        self.regression = regression\n",
        "        if regression:\n",
        "          self.lin = nn.Sequential(nn.Linear(hidden_channels, 1))\n",
        "        else:\n",
        "          self.lin = ResLayer(hidden_channels, dataset.num_classes)  \n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x, edge_attr = self.conv1(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv2(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv3(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv4(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.2), F.dropout(edge_attr, p = 0.2)\n",
        "       \n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        if self.regression:\n",
        "          x = self.lin(x)\n",
        "        else:\n",
        "          x = F.sigmoid(self.lin(x))\n",
        "\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "# print(model)"
      ],
      "metadata": {
        "id": "cDDbybyzGy_E"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "KSfogxeVH1fw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GATV2_2(hidden_channels=64, dataset= train_dataset, regression= True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-03, weight_decay = 1e-03)\n",
        "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
        "                                                   max_lr = 1e-03, \n",
        "                                                  epochs = 250, \n",
        "                                                  steps_per_epoch = 29)\n",
        "criterion = torch.nn.MSELoss()\n",
        "model.to(device)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        # out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "        train_losses.append(loss.item())\n",
        "        lr_scheduler.step()\n",
        "    \n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            data.to(device)\n",
        "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "            loss = criterion(out, data.y)\n",
        "            val_losses.append(loss.item())\n",
        "    \n",
        "    return np.mean(np.sqrt(train_losses)), np.mean(np.sqrt(val_losses))\n",
        "    \n",
        "\n",
        "\n",
        "for epoch in range(1, 250):\n",
        "    train_loss , valid_loss = train()\n",
        "    print(f'Epoch: {epoch} - Train RMSE: {train_loss:.3f}, Valid RMSE: {valid_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSRdPu1hIyNu",
        "outputId": "6c702054-6c20-48a4-a1a6-d90153c7d956"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 - Train RMSE: 2.868, Valid RMSE: 2.888\n",
            "Epoch: 2 - Train RMSE: 2.826, Valid RMSE: 2.810\n",
            "Epoch: 3 - Train RMSE: 2.748, Valid RMSE: 2.729\n",
            "Epoch: 4 - Train RMSE: 2.653, Valid RMSE: 2.604\n",
            "Epoch: 5 - Train RMSE: 2.504, Valid RMSE: 2.552\n",
            "Epoch: 6 - Train RMSE: 2.398, Valid RMSE: 2.500\n",
            "Epoch: 7 - Train RMSE: 2.351, Valid RMSE: 2.446\n",
            "Epoch: 8 - Train RMSE: 2.209, Valid RMSE: 2.400\n",
            "Epoch: 9 - Train RMSE: 2.128, Valid RMSE: 2.304\n",
            "Epoch: 10 - Train RMSE: 2.048, Valid RMSE: 2.285\n",
            "Epoch: 11 - Train RMSE: 1.861, Valid RMSE: 2.151\n",
            "Epoch: 12 - Train RMSE: 1.839, Valid RMSE: 2.234\n",
            "Epoch: 13 - Train RMSE: 1.668, Valid RMSE: 1.802\n",
            "Epoch: 14 - Train RMSE: 1.657, Valid RMSE: 1.587\n",
            "Epoch: 15 - Train RMSE: 1.584, Valid RMSE: 1.657\n",
            "Epoch: 16 - Train RMSE: 1.547, Valid RMSE: 1.708\n",
            "Epoch: 17 - Train RMSE: 1.443, Valid RMSE: 1.434\n",
            "Epoch: 18 - Train RMSE: 1.373, Valid RMSE: 1.242\n",
            "Epoch: 19 - Train RMSE: 1.419, Valid RMSE: 1.445\n",
            "Epoch: 20 - Train RMSE: 1.325, Valid RMSE: 1.111\n",
            "Epoch: 21 - Train RMSE: 1.308, Valid RMSE: 1.123\n",
            "Epoch: 22 - Train RMSE: 1.223, Valid RMSE: 1.058\n",
            "Epoch: 23 - Train RMSE: 1.251, Valid RMSE: 1.148\n",
            "Epoch: 24 - Train RMSE: 1.256, Valid RMSE: 1.107\n",
            "Epoch: 25 - Train RMSE: 1.154, Valid RMSE: 1.151\n",
            "Epoch: 26 - Train RMSE: 1.212, Valid RMSE: 1.066\n",
            "Epoch: 27 - Train RMSE: 1.138, Valid RMSE: 1.089\n",
            "Epoch: 28 - Train RMSE: 1.103, Valid RMSE: 1.118\n",
            "Epoch: 29 - Train RMSE: 1.140, Valid RMSE: 1.235\n",
            "Epoch: 30 - Train RMSE: 1.116, Valid RMSE: 1.016\n",
            "Epoch: 31 - Train RMSE: 1.047, Valid RMSE: 1.124\n",
            "Epoch: 32 - Train RMSE: 1.070, Valid RMSE: 0.958\n",
            "Epoch: 33 - Train RMSE: 1.049, Valid RMSE: 1.048\n",
            "Epoch: 34 - Train RMSE: 1.106, Valid RMSE: 1.036\n",
            "Epoch: 35 - Train RMSE: 1.057, Valid RMSE: 0.945\n",
            "Epoch: 36 - Train RMSE: 1.083, Valid RMSE: 0.876\n",
            "Epoch: 37 - Train RMSE: 1.039, Valid RMSE: 0.873\n",
            "Epoch: 38 - Train RMSE: 1.079, Valid RMSE: 0.878\n",
            "Epoch: 39 - Train RMSE: 1.033, Valid RMSE: 0.968\n",
            "Epoch: 40 - Train RMSE: 1.036, Valid RMSE: 0.963\n",
            "Epoch: 41 - Train RMSE: 0.988, Valid RMSE: 1.067\n",
            "Epoch: 42 - Train RMSE: 1.118, Valid RMSE: 1.043\n",
            "Epoch: 43 - Train RMSE: 1.102, Valid RMSE: 1.030\n",
            "Epoch: 44 - Train RMSE: 1.063, Valid RMSE: 1.006\n",
            "Epoch: 45 - Train RMSE: 1.058, Valid RMSE: 1.067\n",
            "Epoch: 46 - Train RMSE: 1.032, Valid RMSE: 1.064\n",
            "Epoch: 47 - Train RMSE: 1.003, Valid RMSE: 0.965\n",
            "Epoch: 48 - Train RMSE: 0.989, Valid RMSE: 0.896\n",
            "Epoch: 49 - Train RMSE: 1.010, Valid RMSE: 0.857\n",
            "Epoch: 50 - Train RMSE: 0.968, Valid RMSE: 0.875\n",
            "Epoch: 51 - Train RMSE: 0.983, Valid RMSE: 0.953\n",
            "Epoch: 52 - Train RMSE: 0.964, Valid RMSE: 0.883\n",
            "Epoch: 53 - Train RMSE: 0.949, Valid RMSE: 0.975\n",
            "Epoch: 54 - Train RMSE: 0.954, Valid RMSE: 0.909\n",
            "Epoch: 55 - Train RMSE: 0.902, Valid RMSE: 0.838\n",
            "Epoch: 56 - Train RMSE: 0.939, Valid RMSE: 0.968\n",
            "Epoch: 57 - Train RMSE: 0.919, Valid RMSE: 0.875\n",
            "Epoch: 58 - Train RMSE: 0.924, Valid RMSE: 0.997\n",
            "Epoch: 59 - Train RMSE: 0.981, Valid RMSE: 0.929\n",
            "Epoch: 60 - Train RMSE: 0.999, Valid RMSE: 0.898\n",
            "Epoch: 61 - Train RMSE: 0.884, Valid RMSE: 0.848\n",
            "Epoch: 62 - Train RMSE: 0.900, Valid RMSE: 0.819\n",
            "Epoch: 63 - Train RMSE: 0.949, Valid RMSE: 0.868\n",
            "Epoch: 64 - Train RMSE: 0.926, Valid RMSE: 0.896\n",
            "Epoch: 65 - Train RMSE: 0.916, Valid RMSE: 0.932\n",
            "Epoch: 66 - Train RMSE: 0.918, Valid RMSE: 0.962\n",
            "Epoch: 67 - Train RMSE: 0.900, Valid RMSE: 0.918\n",
            "Epoch: 68 - Train RMSE: 0.931, Valid RMSE: 0.863\n",
            "Epoch: 69 - Train RMSE: 0.924, Valid RMSE: 0.848\n",
            "Epoch: 70 - Train RMSE: 0.938, Valid RMSE: 0.886\n",
            "Epoch: 71 - Train RMSE: 0.867, Valid RMSE: 0.773\n",
            "Epoch: 72 - Train RMSE: 0.898, Valid RMSE: 0.967\n",
            "Epoch: 73 - Train RMSE: 0.862, Valid RMSE: 0.857\n",
            "Epoch: 74 - Train RMSE: 0.858, Valid RMSE: 0.873\n",
            "Epoch: 75 - Train RMSE: 0.896, Valid RMSE: 0.807\n",
            "Epoch: 76 - Train RMSE: 0.917, Valid RMSE: 0.825\n",
            "Epoch: 77 - Train RMSE: 0.940, Valid RMSE: 0.848\n",
            "Epoch: 78 - Train RMSE: 0.921, Valid RMSE: 0.793\n",
            "Epoch: 79 - Train RMSE: 0.840, Valid RMSE: 0.815\n",
            "Epoch: 80 - Train RMSE: 0.815, Valid RMSE: 0.811\n",
            "Epoch: 81 - Train RMSE: 0.852, Valid RMSE: 0.854\n",
            "Epoch: 82 - Train RMSE: 0.840, Valid RMSE: 0.880\n",
            "Epoch: 83 - Train RMSE: 0.861, Valid RMSE: 0.825\n",
            "Epoch: 84 - Train RMSE: 0.834, Valid RMSE: 0.897\n",
            "Epoch: 85 - Train RMSE: 0.827, Valid RMSE: 0.804\n",
            "Epoch: 86 - Train RMSE: 0.818, Valid RMSE: 0.809\n",
            "Epoch: 87 - Train RMSE: 0.823, Valid RMSE: 0.811\n",
            "Epoch: 88 - Train RMSE: 0.808, Valid RMSE: 0.833\n",
            "Epoch: 89 - Train RMSE: 0.823, Valid RMSE: 0.838\n",
            "Epoch: 90 - Train RMSE: 0.838, Valid RMSE: 0.767\n",
            "Epoch: 91 - Train RMSE: 0.810, Valid RMSE: 0.845\n",
            "Epoch: 92 - Train RMSE: 0.795, Valid RMSE: 0.835\n",
            "Epoch: 93 - Train RMSE: 0.859, Valid RMSE: 0.789\n",
            "Epoch: 94 - Train RMSE: 0.818, Valid RMSE: 0.855\n",
            "Epoch: 95 - Train RMSE: 0.781, Valid RMSE: 0.905\n",
            "Epoch: 96 - Train RMSE: 0.833, Valid RMSE: 0.771\n",
            "Epoch: 97 - Train RMSE: 0.781, Valid RMSE: 0.738\n",
            "Epoch: 98 - Train RMSE: 0.809, Valid RMSE: 0.796\n",
            "Epoch: 99 - Train RMSE: 0.803, Valid RMSE: 0.749\n",
            "Epoch: 100 - Train RMSE: 0.792, Valid RMSE: 0.755\n",
            "Epoch: 101 - Train RMSE: 0.806, Valid RMSE: 0.811\n",
            "Epoch: 102 - Train RMSE: 0.784, Valid RMSE: 0.743\n",
            "Epoch: 103 - Train RMSE: 0.797, Valid RMSE: 0.806\n",
            "Epoch: 104 - Train RMSE: 0.771, Valid RMSE: 0.792\n",
            "Epoch: 105 - Train RMSE: 0.828, Valid RMSE: 0.798\n",
            "Epoch: 106 - Train RMSE: 0.820, Valid RMSE: 0.826\n",
            "Epoch: 107 - Train RMSE: 0.811, Valid RMSE: 0.860\n",
            "Epoch: 108 - Train RMSE: 0.783, Valid RMSE: 0.807\n",
            "Epoch: 109 - Train RMSE: 0.752, Valid RMSE: 0.796\n",
            "Epoch: 110 - Train RMSE: 0.781, Valid RMSE: 0.755\n",
            "Epoch: 111 - Train RMSE: 0.822, Valid RMSE: 0.726\n",
            "Epoch: 112 - Train RMSE: 0.748, Valid RMSE: 0.718\n",
            "Epoch: 113 - Train RMSE: 0.810, Valid RMSE: 0.838\n",
            "Epoch: 114 - Train RMSE: 0.779, Valid RMSE: 0.802\n",
            "Epoch: 115 - Train RMSE: 0.782, Valid RMSE: 0.771\n",
            "Epoch: 116 - Train RMSE: 0.792, Valid RMSE: 0.818\n",
            "Epoch: 117 - Train RMSE: 0.784, Valid RMSE: 0.765\n",
            "Epoch: 118 - Train RMSE: 0.734, Valid RMSE: 0.743\n",
            "Epoch: 119 - Train RMSE: 0.694, Valid RMSE: 0.738\n",
            "Epoch: 120 - Train RMSE: 0.764, Valid RMSE: 0.822\n",
            "Epoch: 121 - Train RMSE: 0.765, Valid RMSE: 0.808\n",
            "Epoch: 122 - Train RMSE: 0.752, Valid RMSE: 0.857\n",
            "Epoch: 123 - Train RMSE: 0.768, Valid RMSE: 0.813\n",
            "Epoch: 124 - Train RMSE: 0.780, Valid RMSE: 0.748\n",
            "Epoch: 125 - Train RMSE: 0.759, Valid RMSE: 0.752\n",
            "Epoch: 126 - Train RMSE: 0.743, Valid RMSE: 0.780\n",
            "Epoch: 127 - Train RMSE: 0.779, Valid RMSE: 0.810\n",
            "Epoch: 128 - Train RMSE: 0.739, Valid RMSE: 0.705\n",
            "Epoch: 129 - Train RMSE: 0.727, Valid RMSE: 0.745\n",
            "Epoch: 130 - Train RMSE: 0.716, Valid RMSE: 0.771\n",
            "Epoch: 131 - Train RMSE: 0.719, Valid RMSE: 0.799\n",
            "Epoch: 132 - Train RMSE: 0.742, Valid RMSE: 0.800\n",
            "Epoch: 133 - Train RMSE: 0.740, Valid RMSE: 0.866\n",
            "Epoch: 134 - Train RMSE: 0.730, Valid RMSE: 0.815\n",
            "Epoch: 135 - Train RMSE: 0.728, Valid RMSE: 0.794\n",
            "Epoch: 136 - Train RMSE: 0.735, Valid RMSE: 0.795\n",
            "Epoch: 137 - Train RMSE: 0.719, Valid RMSE: 0.756\n",
            "Epoch: 138 - Train RMSE: 0.687, Valid RMSE: 0.763\n",
            "Epoch: 139 - Train RMSE: 0.702, Valid RMSE: 0.749\n",
            "Epoch: 140 - Train RMSE: 0.708, Valid RMSE: 0.731\n",
            "Epoch: 141 - Train RMSE: 0.690, Valid RMSE: 0.761\n",
            "Epoch: 142 - Train RMSE: 0.812, Valid RMSE: 0.811\n",
            "Epoch: 143 - Train RMSE: 0.736, Valid RMSE: 0.744\n",
            "Epoch: 144 - Train RMSE: 0.730, Valid RMSE: 0.793\n",
            "Epoch: 145 - Train RMSE: 0.747, Valid RMSE: 0.744\n",
            "Epoch: 146 - Train RMSE: 0.738, Valid RMSE: 0.813\n",
            "Epoch: 147 - Train RMSE: 0.770, Valid RMSE: 0.759\n",
            "Epoch: 148 - Train RMSE: 0.753, Valid RMSE: 0.742\n",
            "Epoch: 149 - Train RMSE: 0.693, Valid RMSE: 0.772\n",
            "Epoch: 150 - Train RMSE: 0.714, Valid RMSE: 0.845\n",
            "Epoch: 151 - Train RMSE: 0.723, Valid RMSE: 0.777\n",
            "Epoch: 152 - Train RMSE: 0.703, Valid RMSE: 0.845\n",
            "Epoch: 153 - Train RMSE: 0.704, Valid RMSE: 0.769\n",
            "Epoch: 154 - Train RMSE: 0.713, Valid RMSE: 0.742\n",
            "Epoch: 155 - Train RMSE: 0.735, Valid RMSE: 0.728\n",
            "Epoch: 156 - Train RMSE: 0.680, Valid RMSE: 0.774\n",
            "Epoch: 157 - Train RMSE: 0.697, Valid RMSE: 0.753\n",
            "Epoch: 158 - Train RMSE: 0.677, Valid RMSE: 0.759\n",
            "Epoch: 159 - Train RMSE: 0.682, Valid RMSE: 0.772\n",
            "Epoch: 160 - Train RMSE: 0.715, Valid RMSE: 0.739\n",
            "Epoch: 161 - Train RMSE: 0.684, Valid RMSE: 0.763\n",
            "Epoch: 162 - Train RMSE: 0.679, Valid RMSE: 0.784\n",
            "Epoch: 163 - Train RMSE: 0.711, Valid RMSE: 0.777\n",
            "Epoch: 164 - Train RMSE: 0.714, Valid RMSE: 0.796\n",
            "Epoch: 165 - Train RMSE: 0.760, Valid RMSE: 0.754\n",
            "Epoch: 166 - Train RMSE: 0.733, Valid RMSE: 0.797\n",
            "Epoch: 167 - Train RMSE: 0.726, Valid RMSE: 0.755\n",
            "Epoch: 168 - Train RMSE: 0.692, Valid RMSE: 0.739\n",
            "Epoch: 169 - Train RMSE: 0.717, Valid RMSE: 0.826\n",
            "Epoch: 170 - Train RMSE: 0.661, Valid RMSE: 0.754\n",
            "Epoch: 171 - Train RMSE: 0.674, Valid RMSE: 0.729\n",
            "Epoch: 172 - Train RMSE: 0.700, Valid RMSE: 0.812\n",
            "Epoch: 173 - Train RMSE: 0.665, Valid RMSE: 0.776\n",
            "Epoch: 174 - Train RMSE: 0.704, Valid RMSE: 0.837\n",
            "Epoch: 175 - Train RMSE: 0.697, Valid RMSE: 0.821\n",
            "Epoch: 176 - Train RMSE: 0.682, Valid RMSE: 0.749\n",
            "Epoch: 177 - Train RMSE: 0.670, Valid RMSE: 0.710\n",
            "Epoch: 178 - Train RMSE: 0.673, Valid RMSE: 0.735\n",
            "Epoch: 179 - Train RMSE: 0.639, Valid RMSE: 0.805\n",
            "Epoch: 180 - Train RMSE: 0.680, Valid RMSE: 0.735\n",
            "Epoch: 181 - Train RMSE: 0.638, Valid RMSE: 0.689\n",
            "Epoch: 182 - Train RMSE: 0.650, Valid RMSE: 0.738\n",
            "Epoch: 183 - Train RMSE: 0.677, Valid RMSE: 0.758\n",
            "Epoch: 184 - Train RMSE: 0.642, Valid RMSE: 0.724\n",
            "Epoch: 185 - Train RMSE: 0.654, Valid RMSE: 0.800\n",
            "Epoch: 186 - Train RMSE: 0.660, Valid RMSE: 0.728\n",
            "Epoch: 187 - Train RMSE: 0.638, Valid RMSE: 0.746\n",
            "Epoch: 188 - Train RMSE: 0.630, Valid RMSE: 0.706\n",
            "Epoch: 189 - Train RMSE: 0.702, Valid RMSE: 0.769\n",
            "Epoch: 190 - Train RMSE: 0.636, Valid RMSE: 0.712\n",
            "Epoch: 191 - Train RMSE: 0.674, Valid RMSE: 0.740\n",
            "Epoch: 192 - Train RMSE: 0.689, Valid RMSE: 0.734\n",
            "Epoch: 193 - Train RMSE: 0.649, Valid RMSE: 0.773\n",
            "Epoch: 194 - Train RMSE: 0.673, Valid RMSE: 0.699\n",
            "Epoch: 195 - Train RMSE: 0.639, Valid RMSE: 0.711\n",
            "Epoch: 196 - Train RMSE: 0.637, Valid RMSE: 0.767\n",
            "Epoch: 197 - Train RMSE: 0.617, Valid RMSE: 0.722\n",
            "Epoch: 198 - Train RMSE: 0.605, Valid RMSE: 0.665\n",
            "Epoch: 199 - Train RMSE: 0.623, Valid RMSE: 0.700\n",
            "Epoch: 200 - Train RMSE: 0.614, Valid RMSE: 0.733\n",
            "Epoch: 201 - Train RMSE: 0.602, Valid RMSE: 0.718\n",
            "Epoch: 202 - Train RMSE: 0.640, Valid RMSE: 0.717\n",
            "Epoch: 203 - Train RMSE: 0.619, Valid RMSE: 0.690\n",
            "Epoch: 204 - Train RMSE: 0.606, Valid RMSE: 0.745\n",
            "Epoch: 205 - Train RMSE: 0.634, Valid RMSE: 0.749\n",
            "Epoch: 206 - Train RMSE: 0.606, Valid RMSE: 0.745\n",
            "Epoch: 207 - Train RMSE: 0.593, Valid RMSE: 0.699\n",
            "Epoch: 208 - Train RMSE: 0.624, Valid RMSE: 0.689\n",
            "Epoch: 209 - Train RMSE: 0.634, Valid RMSE: 0.719\n",
            "Epoch: 210 - Train RMSE: 0.618, Valid RMSE: 0.722\n",
            "Epoch: 211 - Train RMSE: 0.618, Valid RMSE: 0.736\n",
            "Epoch: 212 - Train RMSE: 0.599, Valid RMSE: 0.754\n",
            "Epoch: 213 - Train RMSE: 0.667, Valid RMSE: 0.731\n",
            "Epoch: 214 - Train RMSE: 0.613, Valid RMSE: 0.700\n",
            "Epoch: 215 - Train RMSE: 0.591, Valid RMSE: 0.731\n",
            "Epoch: 216 - Train RMSE: 0.625, Valid RMSE: 0.741\n",
            "Epoch: 217 - Train RMSE: 0.596, Valid RMSE: 0.754\n",
            "Epoch: 218 - Train RMSE: 0.579, Valid RMSE: 0.750\n",
            "Epoch: 219 - Train RMSE: 0.621, Valid RMSE: 0.731\n",
            "Epoch: 220 - Train RMSE: 0.586, Valid RMSE: 0.732\n",
            "Epoch: 221 - Train RMSE: 0.614, Valid RMSE: 0.804\n",
            "Epoch: 222 - Train RMSE: 0.600, Valid RMSE: 0.714\n",
            "Epoch: 223 - Train RMSE: 0.615, Valid RMSE: 0.800\n",
            "Epoch: 224 - Train RMSE: 0.625, Valid RMSE: 0.800\n",
            "Epoch: 225 - Train RMSE: 0.592, Valid RMSE: 0.729\n",
            "Epoch: 226 - Train RMSE: 0.642, Valid RMSE: 0.732\n",
            "Epoch: 227 - Train RMSE: 0.595, Valid RMSE: 0.701\n",
            "Epoch: 228 - Train RMSE: 0.630, Valid RMSE: 0.776\n",
            "Epoch: 229 - Train RMSE: 0.650, Valid RMSE: 0.735\n",
            "Epoch: 230 - Train RMSE: 0.602, Valid RMSE: 0.715\n",
            "Epoch: 231 - Train RMSE: 0.579, Valid RMSE: 0.712\n",
            "Epoch: 232 - Train RMSE: 0.589, Valid RMSE: 0.763\n",
            "Epoch: 233 - Train RMSE: 0.570, Valid RMSE: 0.756\n",
            "Epoch: 234 - Train RMSE: 0.553, Valid RMSE: 0.729\n",
            "Epoch: 235 - Train RMSE: 0.629, Valid RMSE: 0.749\n",
            "Epoch: 236 - Train RMSE: 0.595, Valid RMSE: 0.775\n",
            "Epoch: 237 - Train RMSE: 0.580, Valid RMSE: 0.731\n",
            "Epoch: 238 - Train RMSE: 0.578, Valid RMSE: 0.740\n",
            "Epoch: 239 - Train RMSE: 0.629, Valid RMSE: 0.761\n",
            "Epoch: 240 - Train RMSE: 0.614, Valid RMSE: 0.790\n",
            "Epoch: 241 - Train RMSE: 0.617, Valid RMSE: 0.786\n",
            "Epoch: 242 - Train RMSE: 0.566, Valid RMSE: 0.741\n",
            "Epoch: 243 - Train RMSE: 0.635, Valid RMSE: 0.703\n",
            "Epoch: 244 - Train RMSE: 0.592, Valid RMSE: 0.752\n",
            "Epoch: 245 - Train RMSE: 0.576, Valid RMSE: 0.712\n",
            "Epoch: 246 - Train RMSE: 0.632, Valid RMSE: 0.697\n",
            "Epoch: 247 - Train RMSE: 0.632, Valid RMSE: 0.755\n",
            "Epoch: 248 - Train RMSE: 0.577, Valid RMSE: 0.731\n",
            "Epoch: 249 - Train RMSE: 0.632, Valid RMSE: 0.854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Attempt**"
      ],
      "metadata": {
        "id": "5oPgyQw4Qbnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GATv2ConvEV4(MessagePassing):\n",
        "\n",
        "  def __init__(self, n_in, n_out, e_in, e_out):\n",
        "    super().__init__(aggr = 'add')\n",
        "    self.n_transform = ResLayer(n_in, n_out)\n",
        "    self.e_transform = ResLayer(e_in, e_out)\n",
        "\n",
        "    # Node\n",
        "    self.n_attn = attn_heads(2*n_out + e_out, 1)\n",
        "    self.e_attn = attn_heads(2*n_out + e_out, 1)\n",
        "    self.s_attn = attn_heads(2*n_out + e_out, 1)\n",
        "    self.n_message = ResLayer(e_out + 2*n_out , n_out)\n",
        "\n",
        "    # For Edge\n",
        "    self.n1_attn = attn_heads(e_out + 2*n_out, 1)\n",
        "    self.n2_attn = attn_heads(e_out + 2*n_out, 1)\n",
        "    self.s_e_attn = attn_heads(e_out + 2*n_out, 1)\n",
        "    self.e_update = ResLayer(e_out + 2*n_out, e_out)\n",
        "\n",
        "    # For Node Update\n",
        "    self.u_m_attn = attn_heads(2*n_out, 1)\n",
        "    self.u_s_attn = attn_heads(2*n_out, 1)\n",
        "    self.n_update = ResLayer(2*n_out, n_out)\n",
        "\n",
        "    self.n_in = n_in\n",
        "    self.n_out = n_out\n",
        "    self.e_in = e_in\n",
        "    self.e_out = e_out\n",
        "\n",
        "  def forward(self, x, edge_index, edge_attr):\n",
        "    # st()\n",
        "    x = self.n_transform(x)\n",
        "    edge_attr = self.e_transform(edge_attr)\n",
        "\n",
        "    node_features = self.propagate(edge_index, x = x, edge_attr = edge_attr)\n",
        "    edge_features = self.edge_updater(edge_index, x = x, edge_attr = edge_attr)\n",
        "\n",
        "    return node_features, edge_features\n",
        "  \n",
        "  def edge_update(self, edge_index, x_i, x_j, edge_attr):\n",
        "    # 2. Edge Message Calculation\n",
        "\n",
        "    input = torch.cat([x_i, x_j, edge_attr], dim = 1)\n",
        "    n1_attn = self.n1_attn(input)\n",
        "    n2_attn = self.n2_attn(input)\n",
        "    self_attn = self.s_e_attn(input)\n",
        "\n",
        "    edge_message = torch.cat([n1_attn*x_i, n2_attn*x_j, self_attn*edge_attr], dim = 1)\n",
        "    return self.e_update(edge_message)\n",
        "  \n",
        "  def message(self, x_i, x_j, edge_index, edge_attr):\n",
        "\n",
        "    input = torch.cat([x_i, x_j, edge_attr], dim = 1)\n",
        "    # 1. Node Message Calculation\n",
        "    \n",
        "    neighbour_attn = self.n_attn(input)\n",
        "    edge_attn = self.e_attn(input)\n",
        "    self_attn = self.s_attn(input)\n",
        "    \n",
        "    # 1.1 Attention Softmax\n",
        "    neighbour_attn = softmax(neighbour_attn, edge_index[0])\n",
        "    edge_attn = softmax(edge_attn, edge_index[0])\n",
        "    self_attn = softmax(self_attn, edge_index[0])\n",
        "\n",
        "    # 1.2 Gather the message\n",
        "    node_message = torch.cat([self_attn*x_i, neighbour_attn*x_j, edge_attn*edge_attr], dim = 1)\n",
        "\n",
        "    op = self.n_message(node_message)\n",
        "    return op\n",
        "  \n",
        "  def update(self, aggregated_output, x):\n",
        "    '''\n",
        "    input = aggregated_output || x\n",
        "    message_attn = msg_attn(input)\n",
        "    self_attn = self_attn(input)\n",
        "\n",
        "    h` = update(message_attn*aggregated_output || self_attn*x)\n",
        "    return h`\n",
        "    '''\n",
        "\n",
        "    input = torch.cat([aggregated_output, x], dim = 1)\n",
        "    m_attn = self.u_m_attn(input)\n",
        "    s_attn = self.u_s_attn(input)\n",
        "\n",
        "    return self.n_update(torch.cat([m_attn*aggregated_output, s_attn*x], dim = 1))\n",
        "  "
      ],
      "metadata": {
        "id": "eqWaoWfxQbG4"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATV4(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, dataset = train_dataset, regression = False):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GATv2ConvEV4(dataset.num_node_features, hidden_channels, dataset.num_edge_features, hidden_channels)\n",
        "        self.conv2 = GATv2ConvEV4(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        self.conv3 = GATv2ConvEV4(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        self.conv4 = GATv2ConvEV4(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        self.regression = regression\n",
        "        if regression:\n",
        "          self.lin = nn.Sequential(nn.Linear(hidden_channels, 1))\n",
        "        else:\n",
        "          self.lin = ResLayer(hidden_channels, dataset.num_classes)  \n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x, edge_attr = self.conv1(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv2(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv3(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv4(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.2), F.dropout(edge_attr, p = 0.2)\n",
        "       \n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        if self.regression:\n",
        "          x = self.lin(x)\n",
        "        else:\n",
        "          x = F.sigmoid(self.lin(x))\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "a03jwtoCQxj_"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = GATV4(hidden_channels=64, dataset= train_dataset, regression= True)"
      ],
      "metadata": {
        "id": "SBrvpxvXYrE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GATV4(hidden_channels=64, dataset= train_dataset, regression= True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-03, weight_decay = 5e-03)\n",
        "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
        "                                                  max_lr = 1e-03, \n",
        "                                                  epochs = 250, \n",
        "                                                  steps_per_epoch = 29)\n",
        "criterion = torch.nn.MSELoss()\n",
        "model.to(device)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        # out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "        train_losses.append(loss.item())\n",
        "        lr_scheduler.step()\n",
        "    \n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            data.to(device)\n",
        "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "            loss = criterion(out, data.y)\n",
        "            val_losses.append(loss.item())\n",
        "    \n",
        "    return np.mean(np.sqrt(train_losses)), np.mean(np.sqrt(val_losses))\n",
        "    \n",
        "\n",
        "\n",
        "for epoch in range(1, 250):\n",
        "    train_loss , valid_loss = train()\n",
        "    print(f'Epoch: {epoch} - Train RMSE: {train_loss:.3f}, Valid RMSE: {valid_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N1JsGYBQ9st",
        "outputId": "22420dbe-ffb0-42ab-df9a-2aab72c9c39e"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 - Train RMSE: 3.570, Valid RMSE: 3.625\n",
            "Epoch: 2 - Train RMSE: 3.518, Valid RMSE: 3.524\n",
            "Epoch: 3 - Train RMSE: 3.527, Valid RMSE: 3.461\n",
            "Epoch: 4 - Train RMSE: 3.432, Valid RMSE: 3.430\n",
            "Epoch: 5 - Train RMSE: 3.370, Valid RMSE: 3.321\n",
            "Epoch: 6 - Train RMSE: 3.330, Valid RMSE: 3.257\n",
            "Epoch: 7 - Train RMSE: 3.179, Valid RMSE: 3.189\n",
            "Epoch: 8 - Train RMSE: 3.165, Valid RMSE: 3.075\n",
            "Epoch: 9 - Train RMSE: 3.089, Valid RMSE: 3.005\n",
            "Epoch: 10 - Train RMSE: 2.922, Valid RMSE: 2.873\n",
            "Epoch: 11 - Train RMSE: 2.790, Valid RMSE: 2.806\n",
            "Epoch: 12 - Train RMSE: 2.586, Valid RMSE: 2.556\n",
            "Epoch: 13 - Train RMSE: 2.505, Valid RMSE: 2.419\n",
            "Epoch: 14 - Train RMSE: 2.354, Valid RMSE: 2.217\n",
            "Epoch: 15 - Train RMSE: 2.163, Valid RMSE: 2.150\n",
            "Epoch: 16 - Train RMSE: 2.087, Valid RMSE: 2.017\n",
            "Epoch: 17 - Train RMSE: 1.995, Valid RMSE: 2.071\n",
            "Epoch: 18 - Train RMSE: 1.940, Valid RMSE: 1.894\n",
            "Epoch: 19 - Train RMSE: 1.925, Valid RMSE: 1.928\n",
            "Epoch: 20 - Train RMSE: 1.866, Valid RMSE: 1.839\n",
            "Epoch: 21 - Train RMSE: 1.800, Valid RMSE: 1.766\n",
            "Epoch: 22 - Train RMSE: 1.726, Valid RMSE: 1.764\n",
            "Epoch: 23 - Train RMSE: 1.750, Valid RMSE: 2.094\n",
            "Epoch: 24 - Train RMSE: 1.670, Valid RMSE: 2.064\n",
            "Epoch: 25 - Train RMSE: 1.625, Valid RMSE: 1.534\n",
            "Epoch: 26 - Train RMSE: 1.594, Valid RMSE: 1.705\n",
            "Epoch: 27 - Train RMSE: 1.566, Valid RMSE: 1.496\n",
            "Epoch: 28 - Train RMSE: 1.478, Valid RMSE: 1.287\n",
            "Epoch: 29 - Train RMSE: 1.445, Valid RMSE: 1.315\n",
            "Epoch: 30 - Train RMSE: 1.477, Valid RMSE: 1.433\n",
            "Epoch: 31 - Train RMSE: 1.455, Valid RMSE: 1.384\n",
            "Epoch: 32 - Train RMSE: 1.476, Valid RMSE: 1.458\n",
            "Epoch: 33 - Train RMSE: 1.372, Valid RMSE: 1.255\n",
            "Epoch: 34 - Train RMSE: 1.374, Valid RMSE: 1.275\n",
            "Epoch: 35 - Train RMSE: 1.356, Valid RMSE: 1.235\n",
            "Epoch: 36 - Train RMSE: 1.375, Valid RMSE: 1.193\n",
            "Epoch: 37 - Train RMSE: 1.319, Valid RMSE: 1.144\n",
            "Epoch: 38 - Train RMSE: 1.327, Valid RMSE: 1.220\n",
            "Epoch: 39 - Train RMSE: 1.286, Valid RMSE: 1.079\n",
            "Epoch: 40 - Train RMSE: 1.292, Valid RMSE: 1.090\n",
            "Epoch: 41 - Train RMSE: 1.246, Valid RMSE: 1.024\n",
            "Epoch: 42 - Train RMSE: 1.240, Valid RMSE: 1.087\n",
            "Epoch: 43 - Train RMSE: 1.195, Valid RMSE: 1.131\n",
            "Epoch: 44 - Train RMSE: 1.230, Valid RMSE: 1.098\n",
            "Epoch: 45 - Train RMSE: 1.243, Valid RMSE: 1.161\n",
            "Epoch: 46 - Train RMSE: 1.176, Valid RMSE: 1.084\n",
            "Epoch: 47 - Train RMSE: 1.183, Valid RMSE: 1.009\n",
            "Epoch: 48 - Train RMSE: 1.150, Valid RMSE: 1.080\n",
            "Epoch: 49 - Train RMSE: 1.210, Valid RMSE: 1.109\n",
            "Epoch: 50 - Train RMSE: 1.166, Valid RMSE: 1.040\n",
            "Epoch: 51 - Train RMSE: 1.153, Valid RMSE: 0.905\n",
            "Epoch: 52 - Train RMSE: 1.210, Valid RMSE: 1.118\n",
            "Epoch: 53 - Train RMSE: 1.178, Valid RMSE: 1.325\n",
            "Epoch: 54 - Train RMSE: 1.154, Valid RMSE: 1.016\n",
            "Epoch: 55 - Train RMSE: 1.197, Valid RMSE: 1.063\n",
            "Epoch: 56 - Train RMSE: 1.185, Valid RMSE: 1.093\n",
            "Epoch: 57 - Train RMSE: 1.115, Valid RMSE: 1.090\n",
            "Epoch: 58 - Train RMSE: 1.152, Valid RMSE: 1.028\n",
            "Epoch: 59 - Train RMSE: 1.045, Valid RMSE: 0.899\n",
            "Epoch: 60 - Train RMSE: 1.065, Valid RMSE: 0.965\n",
            "Epoch: 61 - Train RMSE: 1.047, Valid RMSE: 0.990\n",
            "Epoch: 62 - Train RMSE: 1.105, Valid RMSE: 1.116\n",
            "Epoch: 63 - Train RMSE: 1.039, Valid RMSE: 0.955\n",
            "Epoch: 64 - Train RMSE: 0.991, Valid RMSE: 1.001\n",
            "Epoch: 65 - Train RMSE: 1.076, Valid RMSE: 1.155\n",
            "Epoch: 66 - Train RMSE: 1.039, Valid RMSE: 0.873\n",
            "Epoch: 67 - Train RMSE: 1.076, Valid RMSE: 0.915\n",
            "Epoch: 68 - Train RMSE: 1.000, Valid RMSE: 0.900\n",
            "Epoch: 69 - Train RMSE: 0.969, Valid RMSE: 0.947\n",
            "Epoch: 70 - Train RMSE: 1.000, Valid RMSE: 0.898\n",
            "Epoch: 71 - Train RMSE: 0.986, Valid RMSE: 1.041\n",
            "Epoch: 72 - Train RMSE: 0.986, Valid RMSE: 0.944\n",
            "Epoch: 73 - Train RMSE: 1.053, Valid RMSE: 1.017\n",
            "Epoch: 74 - Train RMSE: 1.005, Valid RMSE: 1.034\n",
            "Epoch: 75 - Train RMSE: 0.967, Valid RMSE: 0.822\n",
            "Epoch: 76 - Train RMSE: 0.973, Valid RMSE: 0.904\n",
            "Epoch: 77 - Train RMSE: 0.979, Valid RMSE: 0.873\n",
            "Epoch: 78 - Train RMSE: 0.910, Valid RMSE: 0.989\n",
            "Epoch: 79 - Train RMSE: 0.966, Valid RMSE: 0.911\n",
            "Epoch: 80 - Train RMSE: 0.957, Valid RMSE: 0.878\n",
            "Epoch: 81 - Train RMSE: 0.934, Valid RMSE: 0.961\n",
            "Epoch: 82 - Train RMSE: 0.919, Valid RMSE: 0.860\n",
            "Epoch: 83 - Train RMSE: 0.956, Valid RMSE: 0.867\n",
            "Epoch: 84 - Train RMSE: 0.945, Valid RMSE: 0.823\n",
            "Epoch: 85 - Train RMSE: 0.904, Valid RMSE: 0.828\n",
            "Epoch: 86 - Train RMSE: 0.895, Valid RMSE: 0.807\n",
            "Epoch: 87 - Train RMSE: 0.943, Valid RMSE: 0.842\n",
            "Epoch: 88 - Train RMSE: 0.981, Valid RMSE: 0.839\n",
            "Epoch: 89 - Train RMSE: 0.920, Valid RMSE: 0.796\n",
            "Epoch: 90 - Train RMSE: 0.953, Valid RMSE: 0.842\n",
            "Epoch: 91 - Train RMSE: 0.915, Valid RMSE: 1.053\n",
            "Epoch: 92 - Train RMSE: 0.858, Valid RMSE: 0.886\n",
            "Epoch: 93 - Train RMSE: 0.945, Valid RMSE: 0.909\n",
            "Epoch: 94 - Train RMSE: 0.918, Valid RMSE: 0.815\n",
            "Epoch: 95 - Train RMSE: 0.936, Valid RMSE: 0.810\n",
            "Epoch: 96 - Train RMSE: 0.914, Valid RMSE: 0.859\n",
            "Epoch: 97 - Train RMSE: 0.915, Valid RMSE: 0.902\n",
            "Epoch: 98 - Train RMSE: 0.896, Valid RMSE: 0.833\n",
            "Epoch: 99 - Train RMSE: 0.874, Valid RMSE: 0.779\n",
            "Epoch: 100 - Train RMSE: 0.853, Valid RMSE: 0.828\n",
            "Epoch: 101 - Train RMSE: 0.840, Valid RMSE: 0.820\n",
            "Epoch: 102 - Train RMSE: 0.870, Valid RMSE: 0.840\n",
            "Epoch: 103 - Train RMSE: 0.892, Valid RMSE: 0.818\n",
            "Epoch: 104 - Train RMSE: 0.910, Valid RMSE: 0.796\n",
            "Epoch: 105 - Train RMSE: 0.884, Valid RMSE: 0.795\n",
            "Epoch: 106 - Train RMSE: 0.859, Valid RMSE: 0.817\n",
            "Epoch: 107 - Train RMSE: 0.832, Valid RMSE: 0.885\n",
            "Epoch: 108 - Train RMSE: 0.958, Valid RMSE: 0.779\n",
            "Epoch: 109 - Train RMSE: 0.872, Valid RMSE: 0.870\n",
            "Epoch: 110 - Train RMSE: 0.822, Valid RMSE: 0.784\n",
            "Epoch: 111 - Train RMSE: 0.773, Valid RMSE: 0.908\n",
            "Epoch: 112 - Train RMSE: 0.854, Valid RMSE: 0.868\n",
            "Epoch: 113 - Train RMSE: 0.833, Valid RMSE: 0.871\n",
            "Epoch: 114 - Train RMSE: 0.790, Valid RMSE: 0.827\n",
            "Epoch: 115 - Train RMSE: 0.804, Valid RMSE: 0.813\n",
            "Epoch: 116 - Train RMSE: 0.791, Valid RMSE: 0.831\n",
            "Epoch: 117 - Train RMSE: 0.857, Valid RMSE: 0.738\n",
            "Epoch: 118 - Train RMSE: 0.856, Valid RMSE: 0.850\n",
            "Epoch: 119 - Train RMSE: 0.847, Valid RMSE: 0.779\n",
            "Epoch: 120 - Train RMSE: 0.839, Valid RMSE: 0.914\n",
            "Epoch: 121 - Train RMSE: 0.870, Valid RMSE: 0.800\n",
            "Epoch: 122 - Train RMSE: 0.765, Valid RMSE: 0.717\n",
            "Epoch: 123 - Train RMSE: 0.819, Valid RMSE: 0.761\n",
            "Epoch: 124 - Train RMSE: 0.793, Valid RMSE: 0.814\n",
            "Epoch: 125 - Train RMSE: 0.812, Valid RMSE: 0.759\n",
            "Epoch: 126 - Train RMSE: 0.801, Valid RMSE: 0.752\n",
            "Epoch: 127 - Train RMSE: 0.819, Valid RMSE: 0.825\n",
            "Epoch: 128 - Train RMSE: 0.848, Valid RMSE: 0.806\n",
            "Epoch: 129 - Train RMSE: 0.808, Valid RMSE: 0.771\n",
            "Epoch: 130 - Train RMSE: 0.792, Valid RMSE: 0.788\n",
            "Epoch: 131 - Train RMSE: 0.722, Valid RMSE: 0.814\n",
            "Epoch: 132 - Train RMSE: 0.778, Valid RMSE: 0.821\n",
            "Epoch: 133 - Train RMSE: 0.777, Valid RMSE: 0.858\n",
            "Epoch: 134 - Train RMSE: 0.773, Valid RMSE: 0.801\n",
            "Epoch: 135 - Train RMSE: 0.766, Valid RMSE: 0.752\n",
            "Epoch: 136 - Train RMSE: 0.733, Valid RMSE: 0.737\n",
            "Epoch: 137 - Train RMSE: 0.766, Valid RMSE: 0.714\n",
            "Epoch: 138 - Train RMSE: 0.773, Valid RMSE: 0.754\n",
            "Epoch: 139 - Train RMSE: 0.811, Valid RMSE: 0.781\n",
            "Epoch: 140 - Train RMSE: 0.787, Valid RMSE: 0.791\n",
            "Epoch: 141 - Train RMSE: 0.791, Valid RMSE: 0.686\n",
            "Epoch: 142 - Train RMSE: 0.848, Valid RMSE: 0.864\n",
            "Epoch: 143 - Train RMSE: 0.949, Valid RMSE: 0.798\n",
            "Epoch: 144 - Train RMSE: 0.846, Valid RMSE: 0.782\n",
            "Epoch: 145 - Train RMSE: 0.838, Valid RMSE: 0.788\n",
            "Epoch: 146 - Train RMSE: 0.880, Valid RMSE: 0.817\n",
            "Epoch: 147 - Train RMSE: 0.851, Valid RMSE: 0.755\n",
            "Epoch: 148 - Train RMSE: 0.753, Valid RMSE: 0.788\n",
            "Epoch: 149 - Train RMSE: 0.790, Valid RMSE: 0.817\n",
            "Epoch: 150 - Train RMSE: 0.854, Valid RMSE: 0.833\n",
            "Epoch: 151 - Train RMSE: 0.823, Valid RMSE: 0.825\n",
            "Epoch: 152 - Train RMSE: 0.813, Valid RMSE: 0.749\n",
            "Epoch: 153 - Train RMSE: 0.778, Valid RMSE: 0.857\n",
            "Epoch: 154 - Train RMSE: 0.760, Valid RMSE: 0.828\n",
            "Epoch: 155 - Train RMSE: 0.808, Valid RMSE: 0.820\n",
            "Epoch: 156 - Train RMSE: 0.810, Valid RMSE: 0.817\n",
            "Epoch: 157 - Train RMSE: 0.808, Valid RMSE: 0.832\n",
            "Epoch: 158 - Train RMSE: 0.811, Valid RMSE: 0.815\n",
            "Epoch: 159 - Train RMSE: 0.783, Valid RMSE: 0.767\n",
            "Epoch: 160 - Train RMSE: 0.769, Valid RMSE: 0.744\n",
            "Epoch: 161 - Train RMSE: 0.730, Valid RMSE: 0.688\n",
            "Epoch: 162 - Train RMSE: 0.705, Valid RMSE: 0.736\n",
            "Epoch: 163 - Train RMSE: 0.711, Valid RMSE: 0.747\n",
            "Epoch: 164 - Train RMSE: 0.785, Valid RMSE: 0.731\n",
            "Epoch: 165 - Train RMSE: 0.727, Valid RMSE: 0.724\n",
            "Epoch: 166 - Train RMSE: 0.702, Valid RMSE: 0.738\n",
            "Epoch: 167 - Train RMSE: 0.720, Valid RMSE: 0.715\n",
            "Epoch: 168 - Train RMSE: 0.711, Valid RMSE: 0.774\n",
            "Epoch: 169 - Train RMSE: 0.703, Valid RMSE: 0.705\n",
            "Epoch: 170 - Train RMSE: 0.702, Valid RMSE: 0.830\n",
            "Epoch: 171 - Train RMSE: 0.747, Valid RMSE: 0.718\n",
            "Epoch: 172 - Train RMSE: 0.743, Valid RMSE: 0.741\n",
            "Epoch: 173 - Train RMSE: 0.702, Valid RMSE: 0.755\n",
            "Epoch: 174 - Train RMSE: 0.745, Valid RMSE: 0.720\n",
            "Epoch: 175 - Train RMSE: 0.691, Valid RMSE: 0.688\n",
            "Epoch: 176 - Train RMSE: 0.765, Valid RMSE: 0.849\n",
            "Epoch: 177 - Train RMSE: 0.754, Valid RMSE: 0.753\n",
            "Epoch: 178 - Train RMSE: 0.765, Valid RMSE: 0.748\n",
            "Epoch: 179 - Train RMSE: 0.777, Valid RMSE: 0.750\n",
            "Epoch: 180 - Train RMSE: 0.739, Valid RMSE: 0.755\n",
            "Epoch: 181 - Train RMSE: 0.723, Valid RMSE: 0.663\n",
            "Epoch: 182 - Train RMSE: 0.694, Valid RMSE: 0.815\n",
            "Epoch: 183 - Train RMSE: 0.717, Valid RMSE: 0.748\n",
            "Epoch: 184 - Train RMSE: 0.726, Valid RMSE: 0.708\n",
            "Epoch: 185 - Train RMSE: 0.706, Valid RMSE: 0.726\n",
            "Epoch: 186 - Train RMSE: 0.738, Valid RMSE: 0.756\n",
            "Epoch: 187 - Train RMSE: 0.703, Valid RMSE: 0.769\n",
            "Epoch: 188 - Train RMSE: 0.709, Valid RMSE: 0.740\n",
            "Epoch: 189 - Train RMSE: 0.679, Valid RMSE: 0.795\n",
            "Epoch: 190 - Train RMSE: 0.730, Valid RMSE: 0.647\n",
            "Epoch: 191 - Train RMSE: 0.696, Valid RMSE: 0.730\n",
            "Epoch: 192 - Train RMSE: 0.702, Valid RMSE: 0.699\n",
            "Epoch: 193 - Train RMSE: 0.685, Valid RMSE: 0.703\n",
            "Epoch: 194 - Train RMSE: 0.702, Valid RMSE: 0.725\n",
            "Epoch: 195 - Train RMSE: 0.672, Valid RMSE: 0.785\n",
            "Epoch: 196 - Train RMSE: 0.699, Valid RMSE: 0.724\n",
            "Epoch: 197 - Train RMSE: 0.639, Valid RMSE: 0.696\n",
            "Epoch: 198 - Train RMSE: 0.666, Valid RMSE: 0.836\n",
            "Epoch: 199 - Train RMSE: 0.671, Valid RMSE: 0.705\n",
            "Epoch: 200 - Train RMSE: 0.658, Valid RMSE: 0.798\n",
            "Epoch: 201 - Train RMSE: 0.691, Valid RMSE: 0.823\n",
            "Epoch: 202 - Train RMSE: 0.693, Valid RMSE: 0.764\n",
            "Epoch: 203 - Train RMSE: 0.714, Valid RMSE: 0.910\n",
            "Epoch: 204 - Train RMSE: 0.667, Valid RMSE: 0.761\n",
            "Epoch: 205 - Train RMSE: 0.671, Valid RMSE: 0.761\n",
            "Epoch: 206 - Train RMSE: 0.650, Valid RMSE: 0.796\n",
            "Epoch: 207 - Train RMSE: 0.671, Valid RMSE: 0.736\n",
            "Epoch: 208 - Train RMSE: 0.675, Valid RMSE: 0.742\n",
            "Epoch: 209 - Train RMSE: 0.672, Valid RMSE: 0.738\n",
            "Epoch: 210 - Train RMSE: 0.649, Valid RMSE: 0.759\n",
            "Epoch: 211 - Train RMSE: 0.639, Valid RMSE: 0.676\n",
            "Epoch: 212 - Train RMSE: 0.668, Valid RMSE: 0.778\n",
            "Epoch: 213 - Train RMSE: 0.617, Valid RMSE: 0.778\n",
            "Epoch: 214 - Train RMSE: 0.643, Valid RMSE: 0.742\n",
            "Epoch: 215 - Train RMSE: 0.645, Valid RMSE: 0.753\n",
            "Epoch: 216 - Train RMSE: 0.652, Valid RMSE: 0.750\n",
            "Epoch: 217 - Train RMSE: 0.670, Valid RMSE: 0.679\n",
            "Epoch: 218 - Train RMSE: 0.615, Valid RMSE: 0.703\n",
            "Epoch: 219 - Train RMSE: 0.638, Valid RMSE: 0.718\n",
            "Epoch: 220 - Train RMSE: 0.641, Valid RMSE: 0.719\n",
            "Epoch: 221 - Train RMSE: 0.634, Valid RMSE: 0.700\n",
            "Epoch: 222 - Train RMSE: 0.643, Valid RMSE: 0.678\n",
            "Epoch: 223 - Train RMSE: 0.646, Valid RMSE: 0.787\n",
            "Epoch: 224 - Train RMSE: 0.644, Valid RMSE: 0.790\n",
            "Epoch: 225 - Train RMSE: 0.631, Valid RMSE: 0.728\n",
            "Epoch: 226 - Train RMSE: 0.669, Valid RMSE: 0.714\n",
            "Epoch: 227 - Train RMSE: 0.610, Valid RMSE: 0.659\n",
            "Epoch: 228 - Train RMSE: 0.624, Valid RMSE: 0.821\n",
            "Epoch: 229 - Train RMSE: 0.618, Valid RMSE: 0.686\n",
            "Epoch: 230 - Train RMSE: 0.623, Valid RMSE: 0.738\n",
            "Epoch: 231 - Train RMSE: 0.665, Valid RMSE: 0.731\n",
            "Epoch: 232 - Train RMSE: 0.604, Valid RMSE: 0.710\n",
            "Epoch: 233 - Train RMSE: 0.622, Valid RMSE: 0.753\n",
            "Epoch: 234 - Train RMSE: 0.647, Valid RMSE: 0.741\n",
            "Epoch: 235 - Train RMSE: 0.627, Valid RMSE: 0.782\n",
            "Epoch: 236 - Train RMSE: 0.661, Valid RMSE: 0.692\n",
            "Epoch: 237 - Train RMSE: 0.624, Valid RMSE: 0.732\n",
            "Epoch: 238 - Train RMSE: 0.629, Valid RMSE: 0.669\n",
            "Epoch: 239 - Train RMSE: 0.665, Valid RMSE: 0.668\n",
            "Epoch: 240 - Train RMSE: 0.620, Valid RMSE: 0.755\n",
            "Epoch: 241 - Train RMSE: 0.645, Valid RMSE: 0.700\n",
            "Epoch: 242 - Train RMSE: 0.626, Valid RMSE: 0.678\n",
            "Epoch: 243 - Train RMSE: 0.635, Valid RMSE: 0.690\n",
            "Epoch: 244 - Train RMSE: 0.614, Valid RMSE: 0.733\n",
            "Epoch: 245 - Train RMSE: 0.660, Valid RMSE: 0.644\n",
            "Epoch: 246 - Train RMSE: 0.607, Valid RMSE: 0.731\n",
            "Epoch: 247 - Train RMSE: 0.674, Valid RMSE: 0.651\n",
            "Epoch: 248 - Train RMSE: 0.582, Valid RMSE: 0.689\n",
            "Epoch: 249 - Train RMSE: 0.654, Valid RMSE: 0.779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Update Attention Mechanism - Use new activation functions, try leaky relu and other things.\n",
        "\n",
        "### 2. Try MutliHeadAttention. "
      ],
      "metadata": {
        "id": "-coMryO-kLaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_layer(in_channels, out_channels):\n",
        "  return nn.Sequential(nn.Linear(in_channels, out_channels), \n",
        "                      nn.BatchNorm1d(out_channels),\n",
        "                      nn.ReLU())\n",
        "  \n",
        "def attn_heads(in_channels, out_channels):\n",
        "  return nn.Sequential(nn.Linear(in_channels, out_channels), \n",
        "                      # nn.BatchNorm1d(out_channels),\n",
        "                      nn.Tanh())\n",
        "\n",
        "class ResLayer(nn.Module):\n",
        "  def __init__(self, in_c, out_c):\n",
        "    super().__init__()\n",
        "    self.l1 = linear_layer(in_c, out_c)\n",
        "    self.l2 = linear_layer(out_c, out_c)\n",
        "    self.l3 = linear_layer(out_c, out_c)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.l1(x)\n",
        "    return self.l3(self.l2(x)) + x"
      ],
      "metadata": {
        "id": "ul1vYBgpktGK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATv2ConvEV4(MessagePassing):\n",
        "\n",
        "  def __init__(self, n_in, n_out, e_in, e_out):\n",
        "    super().__init__(aggr = 'add')\n",
        "    self.n_transform = ResLayer(n_in, n_out)\n",
        "    self.e_transform = ResLayer(e_in, e_out)\n",
        "\n",
        "    # Node\n",
        "    self.n_attn = attn_heads(2*n_out + e_out, 1)\n",
        "    self.e_attn = attn_heads(2*n_out + e_out, 1)\n",
        "    self.s_attn = attn_heads(2*n_out + e_out, 1)\n",
        "    self.n_message = ResLayer(e_out + 2*n_out , n_out)\n",
        "\n",
        "    # For Edge\n",
        "    self.n1_attn = attn_heads(e_out + 2*n_out, 1)\n",
        "    self.n2_attn = attn_heads(e_out + 2*n_out, 1)\n",
        "    self.s_e_attn = attn_heads(e_out + 2*n_out, 1)\n",
        "    self.e_update = ResLayer(e_out + 2*n_out, e_out)\n",
        "\n",
        "    # For Node Update\n",
        "    self.u_m_attn = attn_heads(2*n_out, 1)\n",
        "    self.u_s_attn = attn_heads(2*n_out, 1)\n",
        "    self.n_update = ResLayer(2*n_out, n_out)\n",
        "\n",
        "    self.n_in = n_in\n",
        "    self.n_out = n_out\n",
        "    self.e_in = e_in\n",
        "    self.e_out = e_out\n",
        "\n",
        "  def forward(self, x, edge_index, edge_attr):\n",
        "    # st()\n",
        "    x = self.n_transform(x)\n",
        "    edge_attr = self.e_transform(edge_attr)\n",
        "\n",
        "    node_features = self.propagate(edge_index, x = x, edge_attr = edge_attr)\n",
        "    edge_features = self.edge_updater(edge_index, x = x, edge_attr = edge_attr)\n",
        "\n",
        "    return node_features, edge_features\n",
        "  \n",
        "  def edge_update(self, edge_index, x_i, x_j, edge_attr):\n",
        "    # 2. Edge Message Calculation\n",
        "\n",
        "    input = torch.cat([x_i, x_j, edge_attr], dim = 1)\n",
        "    n1_attn = self.n1_attn(input)\n",
        "    n2_attn = self.n2_attn(input)\n",
        "    self_attn = self.s_e_attn(input)\n",
        "\n",
        "    edge_message = torch.cat([n1_attn*x_i, n2_attn*x_j, self_attn*edge_attr], dim = 1)\n",
        "    return self.e_update(edge_message)\n",
        "  \n",
        "  def message(self, x_i, x_j, edge_index, edge_attr):\n",
        "\n",
        "    input = torch.cat([x_i, x_j, edge_attr], dim = 1)\n",
        "    # 1. Node Message Calculation\n",
        "    \n",
        "    neighbour_attn = self.n_attn(input)\n",
        "    edge_attn = self.e_attn(input)\n",
        "    self_attn = self.s_attn(input)\n",
        "    \n",
        "    # 1.1 Attention Softmax\n",
        "    neighbour_attn = softmax(neighbour_attn, edge_index[0])\n",
        "    edge_attn = softmax(edge_attn, edge_index[0])\n",
        "    self_attn = softmax(self_attn, edge_index[0])\n",
        "\n",
        "    # 1.2 Gather the message\n",
        "    node_message = torch.cat([self_attn*x_i, neighbour_attn*x_j, edge_attn*edge_attr], dim = 1)\n",
        "\n",
        "    op = self.n_message(node_message)\n",
        "    return op\n",
        "  \n",
        "  def update(self, aggregated_output, x):\n",
        "    '''\n",
        "    input = aggregated_output || x\n",
        "    message_attn = msg_attn(input)\n",
        "    self_attn = self_attn(input)\n",
        "\n",
        "    h` = update(message_attn*aggregated_output || self_attn*x)\n",
        "    return h`\n",
        "    '''\n",
        "\n",
        "    input = torch.cat([aggregated_output, x], dim = 1)\n",
        "    m_attn = self.u_m_attn(input)\n",
        "    s_attn = self.u_s_attn(input)\n",
        "\n",
        "    return self.n_update(torch.cat([m_attn*aggregated_output, s_attn*x], dim = 1))\n",
        "  "
      ],
      "metadata": {
        "id": "tmvu880Vk5Sz"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATV4(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, dataset = train_dataset, regression = False):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GATv2ConvEV4(dataset.num_node_features, hidden_channels, dataset.num_edge_features, hidden_channels)\n",
        "        self.conv2 = GATv2ConvEV4(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        self.conv3 = GATv2ConvEV4(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        self.conv4 = GATv2ConvEV4(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        self.regression = regression\n",
        "        if regression:\n",
        "          self.lin = nn.Sequential(nn.Linear(hidden_channels, 1))\n",
        "        else:\n",
        "          self.lin = ResLayer(hidden_channels, dataset.num_classes)  \n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x, edge_attr = self.conv1(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv2(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv3(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv4(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.2), F.dropout(edge_attr, p = 0.2)\n",
        "       \n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        if self.regression:\n",
        "          x = self.lin(x)\n",
        "        else:\n",
        "          x = F.sigmoid(self.lin(x))\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "2SEOog3Np1cW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GATV4(hidden_channels=64, dataset= train_dataset, regression= True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-03, weight_decay = 5e-03)\n",
        "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
        "                                                  max_lr = 1e-03, \n",
        "                                                  epochs = 250, \n",
        "                                                  steps_per_epoch = 29)\n",
        "criterion = torch.nn.MSELoss()\n",
        "model.to(device)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        # out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "        train_losses.append(loss.item())\n",
        "        lr_scheduler.step()\n",
        "    \n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            data.to(device)\n",
        "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "            loss = criterion(out, data.y)\n",
        "            val_losses.append(loss.item())\n",
        "    \n",
        "    return np.mean(np.sqrt(train_losses)), np.mean(np.sqrt(val_losses))\n",
        "    \n",
        "\n",
        "\n",
        "for epoch in range(1, 250):\n",
        "    train_loss , valid_loss = train()\n",
        "    print(f'Epoch: {epoch} - Train RMSE: {train_loss:.3f}, Valid RMSE: {valid_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDQAceIdk7zU",
        "outputId": "661ba763-e704-43fe-b174-06c70abf9707"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 - Train RMSE: 3.569, Valid RMSE: 3.624\n",
            "Epoch: 2 - Train RMSE: 3.512, Valid RMSE: 3.479\n",
            "Epoch: 3 - Train RMSE: 3.522, Valid RMSE: 3.459\n",
            "Epoch: 4 - Train RMSE: 3.410, Valid RMSE: 3.418\n",
            "Epoch: 5 - Train RMSE: 3.345, Valid RMSE: 3.301\n",
            "Epoch: 6 - Train RMSE: 3.319, Valid RMSE: 3.193\n",
            "Epoch: 7 - Train RMSE: 3.164, Valid RMSE: 3.187\n",
            "Epoch: 8 - Train RMSE: 3.135, Valid RMSE: 3.041\n",
            "Epoch: 9 - Train RMSE: 3.060, Valid RMSE: 2.946\n",
            "Epoch: 10 - Train RMSE: 2.895, Valid RMSE: 2.828\n",
            "Epoch: 11 - Train RMSE: 2.735, Valid RMSE: 2.780\n",
            "Epoch: 12 - Train RMSE: 2.512, Valid RMSE: 2.477\n",
            "Epoch: 13 - Train RMSE: 2.405, Valid RMSE: 2.398\n",
            "Epoch: 14 - Train RMSE: 2.239, Valid RMSE: 2.144\n",
            "Epoch: 15 - Train RMSE: 2.074, Valid RMSE: 2.031\n",
            "Epoch: 16 - Train RMSE: 2.039, Valid RMSE: 1.964\n",
            "Epoch: 17 - Train RMSE: 1.928, Valid RMSE: 1.932\n",
            "Epoch: 18 - Train RMSE: 1.875, Valid RMSE: 1.831\n",
            "Epoch: 19 - Train RMSE: 1.854, Valid RMSE: 1.864\n",
            "Epoch: 20 - Train RMSE: 1.788, Valid RMSE: 1.903\n",
            "Epoch: 21 - Train RMSE: 1.722, Valid RMSE: 1.722\n",
            "Epoch: 22 - Train RMSE: 1.693, Valid RMSE: 1.697\n",
            "Epoch: 23 - Train RMSE: 1.748, Valid RMSE: 1.587\n",
            "Epoch: 24 - Train RMSE: 1.644, Valid RMSE: 1.595\n",
            "Epoch: 25 - Train RMSE: 1.586, Valid RMSE: 1.467\n",
            "Epoch: 26 - Train RMSE: 1.569, Valid RMSE: 1.500\n",
            "Epoch: 27 - Train RMSE: 1.520, Valid RMSE: 1.299\n",
            "Epoch: 28 - Train RMSE: 1.491, Valid RMSE: 1.397\n",
            "Epoch: 29 - Train RMSE: 1.449, Valid RMSE: 1.339\n",
            "Epoch: 30 - Train RMSE: 1.466, Valid RMSE: 1.365\n",
            "Epoch: 31 - Train RMSE: 1.476, Valid RMSE: 1.253\n",
            "Epoch: 32 - Train RMSE: 1.458, Valid RMSE: 1.252\n",
            "Epoch: 33 - Train RMSE: 1.379, Valid RMSE: 1.343\n",
            "Epoch: 34 - Train RMSE: 1.336, Valid RMSE: 1.217\n",
            "Epoch: 35 - Train RMSE: 1.334, Valid RMSE: 1.267\n",
            "Epoch: 36 - Train RMSE: 1.403, Valid RMSE: 1.235\n",
            "Epoch: 37 - Train RMSE: 1.279, Valid RMSE: 1.159\n",
            "Epoch: 38 - Train RMSE: 1.357, Valid RMSE: 1.228\n",
            "Epoch: 39 - Train RMSE: 1.308, Valid RMSE: 1.273\n",
            "Epoch: 40 - Train RMSE: 1.269, Valid RMSE: 1.219\n",
            "Epoch: 41 - Train RMSE: 1.298, Valid RMSE: 1.204\n",
            "Epoch: 42 - Train RMSE: 1.324, Valid RMSE: 1.168\n",
            "Epoch: 43 - Train RMSE: 1.305, Valid RMSE: 1.161\n",
            "Epoch: 44 - Train RMSE: 1.300, Valid RMSE: 1.294\n",
            "Epoch: 45 - Train RMSE: 1.311, Valid RMSE: 1.242\n",
            "Epoch: 46 - Train RMSE: 1.319, Valid RMSE: 1.073\n",
            "Epoch: 47 - Train RMSE: 1.288, Valid RMSE: 1.144\n",
            "Epoch: 48 - Train RMSE: 1.246, Valid RMSE: 1.066\n",
            "Epoch: 49 - Train RMSE: 1.261, Valid RMSE: 1.053\n",
            "Epoch: 50 - Train RMSE: 1.278, Valid RMSE: 1.142\n",
            "Epoch: 51 - Train RMSE: 1.296, Valid RMSE: 1.110\n",
            "Epoch: 52 - Train RMSE: 1.280, Valid RMSE: 1.038\n",
            "Epoch: 53 - Train RMSE: 1.213, Valid RMSE: 1.188\n",
            "Epoch: 54 - Train RMSE: 1.183, Valid RMSE: 1.057\n",
            "Epoch: 55 - Train RMSE: 1.215, Valid RMSE: 1.175\n",
            "Epoch: 56 - Train RMSE: 1.266, Valid RMSE: 1.091\n",
            "Epoch: 57 - Train RMSE: 1.229, Valid RMSE: 1.158\n",
            "Epoch: 58 - Train RMSE: 1.162, Valid RMSE: 1.005\n",
            "Epoch: 59 - Train RMSE: 1.148, Valid RMSE: 0.989\n",
            "Epoch: 60 - Train RMSE: 1.149, Valid RMSE: 0.984\n",
            "Epoch: 61 - Train RMSE: 1.108, Valid RMSE: 1.105\n",
            "Epoch: 62 - Train RMSE: 1.166, Valid RMSE: 1.250\n",
            "Epoch: 63 - Train RMSE: 1.106, Valid RMSE: 1.045\n",
            "Epoch: 64 - Train RMSE: 1.054, Valid RMSE: 0.989\n",
            "Epoch: 65 - Train RMSE: 1.086, Valid RMSE: 0.946\n",
            "Epoch: 66 - Train RMSE: 1.075, Valid RMSE: 1.107\n",
            "Epoch: 67 - Train RMSE: 1.116, Valid RMSE: 1.012\n",
            "Epoch: 68 - Train RMSE: 1.045, Valid RMSE: 0.993\n",
            "Epoch: 69 - Train RMSE: 1.042, Valid RMSE: 0.935\n",
            "Epoch: 70 - Train RMSE: 1.040, Valid RMSE: 1.007\n",
            "Epoch: 71 - Train RMSE: 1.068, Valid RMSE: 0.956\n",
            "Epoch: 72 - Train RMSE: 1.021, Valid RMSE: 1.008\n",
            "Epoch: 73 - Train RMSE: 1.054, Valid RMSE: 1.060\n",
            "Epoch: 74 - Train RMSE: 0.995, Valid RMSE: 1.064\n",
            "Epoch: 75 - Train RMSE: 0.983, Valid RMSE: 0.907\n",
            "Epoch: 76 - Train RMSE: 0.986, Valid RMSE: 0.880\n",
            "Epoch: 77 - Train RMSE: 1.060, Valid RMSE: 0.908\n",
            "Epoch: 78 - Train RMSE: 0.944, Valid RMSE: 1.006\n",
            "Epoch: 79 - Train RMSE: 1.006, Valid RMSE: 0.987\n",
            "Epoch: 80 - Train RMSE: 0.982, Valid RMSE: 0.995\n",
            "Epoch: 81 - Train RMSE: 0.997, Valid RMSE: 1.032\n",
            "Epoch: 82 - Train RMSE: 0.973, Valid RMSE: 1.014\n",
            "Epoch: 83 - Train RMSE: 0.968, Valid RMSE: 1.000\n",
            "Epoch: 84 - Train RMSE: 1.000, Valid RMSE: 0.982\n",
            "Epoch: 85 - Train RMSE: 0.979, Valid RMSE: 0.901\n",
            "Epoch: 86 - Train RMSE: 0.944, Valid RMSE: 0.892\n",
            "Epoch: 87 - Train RMSE: 0.955, Valid RMSE: 0.833\n",
            "Epoch: 88 - Train RMSE: 0.955, Valid RMSE: 0.783\n",
            "Epoch: 89 - Train RMSE: 0.924, Valid RMSE: 0.785\n",
            "Epoch: 90 - Train RMSE: 0.968, Valid RMSE: 0.939\n",
            "Epoch: 91 - Train RMSE: 0.959, Valid RMSE: 0.904\n",
            "Epoch: 92 - Train RMSE: 0.922, Valid RMSE: 0.858\n",
            "Epoch: 93 - Train RMSE: 0.975, Valid RMSE: 0.972\n",
            "Epoch: 94 - Train RMSE: 0.970, Valid RMSE: 0.848\n",
            "Epoch: 95 - Train RMSE: 0.983, Valid RMSE: 0.839\n",
            "Epoch: 96 - Train RMSE: 0.998, Valid RMSE: 1.013\n",
            "Epoch: 97 - Train RMSE: 0.942, Valid RMSE: 0.824\n",
            "Epoch: 98 - Train RMSE: 0.928, Valid RMSE: 0.874\n",
            "Epoch: 99 - Train RMSE: 0.891, Valid RMSE: 0.807\n",
            "Epoch: 100 - Train RMSE: 0.937, Valid RMSE: 0.944\n",
            "Epoch: 101 - Train RMSE: 0.894, Valid RMSE: 0.833\n",
            "Epoch: 102 - Train RMSE: 0.905, Valid RMSE: 0.909\n",
            "Epoch: 103 - Train RMSE: 0.900, Valid RMSE: 0.864\n",
            "Epoch: 104 - Train RMSE: 0.928, Valid RMSE: 0.892\n",
            "Epoch: 105 - Train RMSE: 0.946, Valid RMSE: 0.781\n",
            "Epoch: 106 - Train RMSE: 0.892, Valid RMSE: 0.813\n",
            "Epoch: 107 - Train RMSE: 0.856, Valid RMSE: 0.779\n",
            "Epoch: 108 - Train RMSE: 0.973, Valid RMSE: 0.850\n",
            "Epoch: 109 - Train RMSE: 0.900, Valid RMSE: 0.809\n",
            "Epoch: 110 - Train RMSE: 0.854, Valid RMSE: 0.778\n",
            "Epoch: 111 - Train RMSE: 0.831, Valid RMSE: 0.923\n",
            "Epoch: 112 - Train RMSE: 0.917, Valid RMSE: 0.828\n",
            "Epoch: 113 - Train RMSE: 0.856, Valid RMSE: 0.863\n",
            "Epoch: 114 - Train RMSE: 0.805, Valid RMSE: 0.809\n",
            "Epoch: 115 - Train RMSE: 0.864, Valid RMSE: 0.790\n",
            "Epoch: 116 - Train RMSE: 0.811, Valid RMSE: 0.802\n",
            "Epoch: 117 - Train RMSE: 0.877, Valid RMSE: 0.815\n",
            "Epoch: 118 - Train RMSE: 0.873, Valid RMSE: 0.718\n",
            "Epoch: 119 - Train RMSE: 0.919, Valid RMSE: 0.797\n",
            "Epoch: 120 - Train RMSE: 0.856, Valid RMSE: 0.942\n",
            "Epoch: 121 - Train RMSE: 0.911, Valid RMSE: 0.842\n",
            "Epoch: 122 - Train RMSE: 0.818, Valid RMSE: 0.742\n",
            "Epoch: 123 - Train RMSE: 0.817, Valid RMSE: 0.742\n",
            "Epoch: 124 - Train RMSE: 0.786, Valid RMSE: 0.726\n",
            "Epoch: 125 - Train RMSE: 0.803, Valid RMSE: 0.770\n",
            "Epoch: 126 - Train RMSE: 0.845, Valid RMSE: 0.915\n",
            "Epoch: 127 - Train RMSE: 0.887, Valid RMSE: 0.822\n",
            "Epoch: 128 - Train RMSE: 0.902, Valid RMSE: 0.814\n",
            "Epoch: 129 - Train RMSE: 0.838, Valid RMSE: 0.793\n",
            "Epoch: 130 - Train RMSE: 0.832, Valid RMSE: 0.804\n",
            "Epoch: 131 - Train RMSE: 0.770, Valid RMSE: 0.806\n",
            "Epoch: 132 - Train RMSE: 0.782, Valid RMSE: 0.828\n",
            "Epoch: 133 - Train RMSE: 0.789, Valid RMSE: 0.878\n",
            "Epoch: 134 - Train RMSE: 0.822, Valid RMSE: 0.791\n",
            "Epoch: 135 - Train RMSE: 0.838, Valid RMSE: 0.786\n",
            "Epoch: 136 - Train RMSE: 0.770, Valid RMSE: 0.781\n",
            "Epoch: 137 - Train RMSE: 0.796, Valid RMSE: 0.750\n",
            "Epoch: 138 - Train RMSE: 0.767, Valid RMSE: 0.811\n",
            "Epoch: 139 - Train RMSE: 0.806, Valid RMSE: 0.845\n",
            "Epoch: 140 - Train RMSE: 0.796, Valid RMSE: 0.731\n",
            "Epoch: 141 - Train RMSE: 0.806, Valid RMSE: 0.704\n",
            "Epoch: 142 - Train RMSE: 0.942, Valid RMSE: 0.790\n",
            "Epoch: 143 - Train RMSE: 0.899, Valid RMSE: 0.829\n",
            "Epoch: 144 - Train RMSE: 0.854, Valid RMSE: 0.805\n",
            "Epoch: 145 - Train RMSE: 0.825, Valid RMSE: 0.701\n",
            "Epoch: 146 - Train RMSE: 0.900, Valid RMSE: 0.730\n",
            "Epoch: 147 - Train RMSE: 0.830, Valid RMSE: 0.741\n",
            "Epoch: 148 - Train RMSE: 0.775, Valid RMSE: 0.698\n",
            "Epoch: 149 - Train RMSE: 0.783, Valid RMSE: 0.781\n",
            "Epoch: 150 - Train RMSE: 0.802, Valid RMSE: 0.770\n",
            "Epoch: 151 - Train RMSE: 0.766, Valid RMSE: 0.743\n",
            "Epoch: 152 - Train RMSE: 0.782, Valid RMSE: 0.751\n",
            "Epoch: 153 - Train RMSE: 0.785, Valid RMSE: 0.754\n",
            "Epoch: 154 - Train RMSE: 0.712, Valid RMSE: 0.765\n",
            "Epoch: 155 - Train RMSE: 0.797, Valid RMSE: 0.771\n",
            "Epoch: 156 - Train RMSE: 0.871, Valid RMSE: 0.810\n",
            "Epoch: 157 - Train RMSE: 1.002, Valid RMSE: 0.838\n",
            "Epoch: 158 - Train RMSE: 0.865, Valid RMSE: 0.791\n",
            "Epoch: 159 - Train RMSE: 0.809, Valid RMSE: 0.761\n",
            "Epoch: 160 - Train RMSE: 0.790, Valid RMSE: 0.673\n",
            "Epoch: 161 - Train RMSE: 0.757, Valid RMSE: 0.675\n",
            "Epoch: 162 - Train RMSE: 0.753, Valid RMSE: 0.658\n",
            "Epoch: 163 - Train RMSE: 0.715, Valid RMSE: 0.785\n",
            "Epoch: 164 - Train RMSE: 0.791, Valid RMSE: 0.753\n",
            "Epoch: 165 - Train RMSE: 0.758, Valid RMSE: 0.759\n",
            "Epoch: 166 - Train RMSE: 0.734, Valid RMSE: 0.714\n",
            "Epoch: 167 - Train RMSE: 0.731, Valid RMSE: 0.739\n",
            "Epoch: 168 - Train RMSE: 0.693, Valid RMSE: 0.744\n",
            "Epoch: 169 - Train RMSE: 0.717, Valid RMSE: 0.712\n",
            "Epoch: 170 - Train RMSE: 0.700, Valid RMSE: 0.824\n",
            "Epoch: 171 - Train RMSE: 0.758, Valid RMSE: 0.731\n",
            "Epoch: 172 - Train RMSE: 0.749, Valid RMSE: 0.821\n",
            "Epoch: 173 - Train RMSE: 0.748, Valid RMSE: 0.773\n",
            "Epoch: 174 - Train RMSE: 0.751, Valid RMSE: 0.678\n",
            "Epoch: 175 - Train RMSE: 0.714, Valid RMSE: 0.682\n",
            "Epoch: 176 - Train RMSE: 0.762, Valid RMSE: 0.795\n",
            "Epoch: 177 - Train RMSE: 0.720, Valid RMSE: 0.676\n",
            "Epoch: 178 - Train RMSE: 0.720, Valid RMSE: 0.723\n",
            "Epoch: 179 - Train RMSE: 0.782, Valid RMSE: 0.679\n",
            "Epoch: 180 - Train RMSE: 0.762, Valid RMSE: 0.735\n",
            "Epoch: 181 - Train RMSE: 0.734, Valid RMSE: 0.673\n",
            "Epoch: 182 - Train RMSE: 0.682, Valid RMSE: 0.694\n",
            "Epoch: 183 - Train RMSE: 0.727, Valid RMSE: 0.620\n",
            "Epoch: 184 - Train RMSE: 0.713, Valid RMSE: 0.656\n",
            "Epoch: 185 - Train RMSE: 0.705, Valid RMSE: 0.669\n",
            "Epoch: 186 - Train RMSE: 0.728, Valid RMSE: 0.698\n",
            "Epoch: 187 - Train RMSE: 0.716, Valid RMSE: 0.682\n",
            "Epoch: 188 - Train RMSE: 0.707, Valid RMSE: 0.659\n",
            "Epoch: 189 - Train RMSE: 0.690, Valid RMSE: 0.709\n",
            "Epoch: 190 - Train RMSE: 0.755, Valid RMSE: 0.697\n",
            "Epoch: 191 - Train RMSE: 0.697, Valid RMSE: 0.693\n",
            "Epoch: 192 - Train RMSE: 0.690, Valid RMSE: 0.667\n",
            "Epoch: 193 - Train RMSE: 0.696, Valid RMSE: 0.729\n",
            "Epoch: 194 - Train RMSE: 0.706, Valid RMSE: 0.713\n",
            "Epoch: 195 - Train RMSE: 0.694, Valid RMSE: 0.715\n",
            "Epoch: 196 - Train RMSE: 0.693, Valid RMSE: 0.660\n",
            "Epoch: 197 - Train RMSE: 0.666, Valid RMSE: 0.665\n",
            "Epoch: 198 - Train RMSE: 0.684, Valid RMSE: 0.813\n",
            "Epoch: 199 - Train RMSE: 0.697, Valid RMSE: 0.705\n",
            "Epoch: 200 - Train RMSE: 0.679, Valid RMSE: 0.667\n",
            "Epoch: 201 - Train RMSE: 0.718, Valid RMSE: 0.632\n",
            "Epoch: 202 - Train RMSE: 0.689, Valid RMSE: 0.635\n",
            "Epoch: 203 - Train RMSE: 0.739, Valid RMSE: 0.727\n",
            "Epoch: 204 - Train RMSE: 0.686, Valid RMSE: 0.666\n",
            "Epoch: 205 - Train RMSE: 0.659, Valid RMSE: 0.655\n",
            "Epoch: 206 - Train RMSE: 0.646, Valid RMSE: 0.692\n",
            "Epoch: 207 - Train RMSE: 0.673, Valid RMSE: 0.630\n",
            "Epoch: 208 - Train RMSE: 0.707, Valid RMSE: 0.626\n",
            "Epoch: 209 - Train RMSE: 0.688, Valid RMSE: 0.647\n",
            "Epoch: 210 - Train RMSE: 0.653, Valid RMSE: 0.668\n",
            "Epoch: 211 - Train RMSE: 0.647, Valid RMSE: 0.664\n",
            "Epoch: 212 - Train RMSE: 0.696, Valid RMSE: 0.722\n",
            "Epoch: 213 - Train RMSE: 0.632, Valid RMSE: 0.712\n",
            "Epoch: 214 - Train RMSE: 0.672, Valid RMSE: 0.723\n",
            "Epoch: 215 - Train RMSE: 0.638, Valid RMSE: 0.747\n",
            "Epoch: 216 - Train RMSE: 0.654, Valid RMSE: 0.682\n",
            "Epoch: 217 - Train RMSE: 0.675, Valid RMSE: 0.647\n",
            "Epoch: 218 - Train RMSE: 0.627, Valid RMSE: 0.691\n",
            "Epoch: 219 - Train RMSE: 0.649, Valid RMSE: 0.631\n",
            "Epoch: 220 - Train RMSE: 0.656, Valid RMSE: 0.685\n",
            "Epoch: 221 - Train RMSE: 0.638, Valid RMSE: 0.664\n",
            "Epoch: 222 - Train RMSE: 0.653, Valid RMSE: 0.688\n",
            "Epoch: 223 - Train RMSE: 0.651, Valid RMSE: 0.679\n",
            "Epoch: 224 - Train RMSE: 0.641, Valid RMSE: 0.672\n",
            "Epoch: 225 - Train RMSE: 0.626, Valid RMSE: 0.687\n",
            "Epoch: 226 - Train RMSE: 0.672, Valid RMSE: 0.633\n",
            "Epoch: 227 - Train RMSE: 0.591, Valid RMSE: 0.642\n",
            "Epoch: 228 - Train RMSE: 0.634, Valid RMSE: 0.704\n",
            "Epoch: 229 - Train RMSE: 0.621, Valid RMSE: 0.751\n",
            "Epoch: 230 - Train RMSE: 0.640, Valid RMSE: 0.666\n",
            "Epoch: 231 - Train RMSE: 0.659, Valid RMSE: 0.711\n",
            "Epoch: 232 - Train RMSE: 0.607, Valid RMSE: 0.666\n",
            "Epoch: 233 - Train RMSE: 0.603, Valid RMSE: 0.653\n",
            "Epoch: 234 - Train RMSE: 0.646, Valid RMSE: 0.709\n",
            "Epoch: 235 - Train RMSE: 0.638, Valid RMSE: 0.640\n",
            "Epoch: 236 - Train RMSE: 0.650, Valid RMSE: 0.655\n",
            "Epoch: 237 - Train RMSE: 0.627, Valid RMSE: 0.643\n",
            "Epoch: 238 - Train RMSE: 0.628, Valid RMSE: 0.644\n",
            "Epoch: 239 - Train RMSE: 0.685, Valid RMSE: 0.681\n",
            "Epoch: 240 - Train RMSE: 0.623, Valid RMSE: 0.682\n",
            "Epoch: 241 - Train RMSE: 0.632, Valid RMSE: 0.708\n",
            "Epoch: 242 - Train RMSE: 0.626, Valid RMSE: 0.672\n",
            "Epoch: 243 - Train RMSE: 0.646, Valid RMSE: 0.692\n",
            "Epoch: 244 - Train RMSE: 0.614, Valid RMSE: 0.688\n",
            "Epoch: 245 - Train RMSE: 0.658, Valid RMSE: 0.686\n",
            "Epoch: 246 - Train RMSE: 0.592, Valid RMSE: 0.655\n",
            "Epoch: 247 - Train RMSE: 0.678, Valid RMSE: 0.700\n",
            "Epoch: 248 - Train RMSE: 0.592, Valid RMSE: 0.719\n",
            "Epoch: 249 - Train RMSE: 0.664, Valid RMSE: 0.732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_scatter import scatter_sum\n",
        "class GATV5(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, dataset = train_dataset, regression = False):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GATv2ConvEV4(dataset.num_node_features, hidden_channels, dataset.num_edge_features, hidden_channels)\n",
        "        self.conv2 = GATv2ConvEV4(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        self.conv3 = GATv2ConvEV4(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        self.conv4 = GATv2ConvEV4(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "\n",
        "        self.feature_bn = nn.BatchNorm1d(hidden_channels)\n",
        "\n",
        "        self.regression = regression\n",
        "        if regression:\n",
        "          self.lin = nn.Sequential(nn.Linear(hidden_channels, 1))\n",
        "        else:\n",
        "          self.lin = ResLayer(hidden_channels, dataset.num_classes)  \n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x, edge_attr = self.conv1(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv2(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv3(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        x, edge_attr = self.conv4(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = 0.2), F.dropout(edge_attr, p = 0.2)\n",
        "       \n",
        "        # 2. Readout layer\n",
        "\n",
        "        x = global_add_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "        edge = global_add_pool(edge_attr, batch[edge_index[0]])\n",
        "        try:\n",
        "          assert x.shape == edge.shape\n",
        "        except:\n",
        "\n",
        "          x = x[:edge.shape[0], :]\n",
        "        features = self.feature_bn(x + edge)\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        features = F.dropout(features, p=0.2, training=self.training)\n",
        "        if self.regression:\n",
        "          features = self.lin(features)\n",
        "        else:\n",
        "          features = F.sigmoid(self.lin(features))\n",
        "        \n",
        "        return features"
      ],
      "metadata": {
        "id": "PeqXqSXAaQik"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAH6-zu0bAlN",
        "outputId": "bc082973-d813-4734-a18b-f3f8c1d87914"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MolDataBatch(x=[397, 45], edge_index=[2, 810], edge_attr=[810, 10], cluster_index=[397], fra_edge_index=[2, 736], fra_edge_attr=[736, 10], y=[32, 1], batch=[397], ptr=[33])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = None"
      ],
      "metadata": {
        "id": "gil3vX-6bBOj"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GATV5(hidden_channels=64, dataset= train_dataset, regression= True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-03, weight_decay = 5e-03)\n",
        "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
        "                                                  max_lr = 1e-03, \n",
        "                                                  epochs = 250, \n",
        "                                                  steps_per_epoch = 29)\n",
        "criterion = torch.nn.MSELoss()\n",
        "model.to(device)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        # out = model(data.x, data.edge_index, data.batch)\n",
        "        out = out.squeeze()\n",
        "        if out.shape[0] != data.y.shape[0]:\n",
        "            out = out[:data.y.shape[0]]\n",
        "\n",
        "        loss = criterion(out, data.y)  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "        train_losses.append(loss.item())\n",
        "        lr_scheduler.step()\n",
        "    \n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            data.to(device)\n",
        "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "            out = out.squeeze()\n",
        "            if out.shape[0] != data.y.shape[0]:\n",
        "                out = out[:data.y.shape[0]]\n",
        "\n",
        "            loss = criterion(out, data.y)\n",
        "            val_losses.append(loss.item())\n",
        "    \n",
        "    return np.mean(np.sqrt(train_losses)), np.mean(np.sqrt(val_losses))\n",
        "    \n",
        "\n",
        "\n",
        "for epoch in range(1, 250):\n",
        "    train_loss , valid_loss = train()\n",
        "    print(f'Epoch: {epoch} - Train RMSE: {train_loss:.3f}, Valid RMSE: {valid_loss:.3f}')"
      ],
      "metadata": {
        "id": "tXW9TP79bCjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_scatter import scatter_sum\n",
        "class GATV6(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, dataset = train_dataset, regression = False, num_layer = 5):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GATv2ConvEV4(dataset.num_node_features, hidden_channels, dataset.num_edge_features, hidden_channels)\n",
        "        # self.conv2 = GATv2ConvEV4(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        # self.conv3 = GATv2ConvEV4(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "        # self.conv4 = GATv2ConvEV4(hidden_channels, hidden_channels, hidden_channels, hidden_channels)\n",
        "\n",
        "        self.layers = nn.ModuleList([GATv2ConvEV4(hidden_channels, hidden_channels, hidden_channels, hidden_channels) for i in range(num_layer - 1)])\n",
        "\n",
        "\n",
        "        self.cross_att = GATConv(hidden_channels, hidden_channels, heads=4,\n",
        "                                     dropout = 0.3, add_self_loops=False,\n",
        "                                     negative_slope=0.01, concat=False)\n",
        "\n",
        "        self.mol_bn = nn.BatchNorm1d(hidden_channels)\n",
        "        self.fra_bn = nn.BatchNorm1d(hidden_channels)\n",
        "\n",
        "        self.regression = regression\n",
        "        if regression:\n",
        "          self.lin = nn.Sequential(nn.Linear(hidden_channels, 1))\n",
        "        else:\n",
        "          self.lin = ResLayer(2*hidden_channels, dataset.num_classes)  \n",
        "    \n",
        "    def layer_f(self, x, edge_index, edge_attr, p = 0.4):\n",
        "        x, edge_attr = self.conv1(x, edge_index, edge_attr = edge_attr)\n",
        "        x, edge_attr = F.dropout(x, p = p), F.dropout(edge_attr, p = p)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x, edge_attr = layer(x, edge_index, edge_attr = edge_attr)\n",
        "            x, edge_attr = F.dropout(x, p = p), F.dropout(edge_attr, p = p)\n",
        "          \n",
        "        return x, edge_attr\n",
        "\n",
        "    def forward(self, data):\n",
        "        # 1. Obtain node embeddings \n",
        "        # x, edge_attr = self.conv1(x, edge_index, edge_attr = edge_attr)\n",
        "        # x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        # x, edge_attr = self.conv2(x, edge_index, edge_attr = edge_attr)\n",
        "        # x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        # x, edge_attr = self.conv3(x, edge_index, edge_attr = edge_attr)\n",
        "        # x, edge_attr = F.dropout(x, p = 0.4), F.dropout(edge_attr, p = 0.4)\n",
        "        # x, edge_attr = self.conv4(x, edge_index, edge_attr = edge_attr)\n",
        "        # x, edge_attr = F.dropout(x, p = 0.2), F.dropout(edge_attr, p = 0.2)\n",
        "\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "\n",
        "        x, edge_attr = self.layer_f(x, edge_index, edge_attr = edge_attr, p= 0.3)\n",
        "\n",
        "        mol_vec = global_add_pool(x, batch)\n",
        "\n",
        "        # fra_x = data.x\n",
        "        # fra_edge_index = data.fra_edge_index\n",
        "        # fra_edge_attr = data.fra_edge_attr\n",
        "        # cluster = data.cluster_index\n",
        "\n",
        "        # fra_x, fra_edge_attr = self.layer_f(fra_x, fra_edge_index, fra_edge_attr, p = 0.3)\n",
        "        # fra_x = global_add_pool(fra_x, cluster)\n",
        "        \n",
        "        # # get fragment batch\n",
        "        # cluster, perm = consecutive_cluster(cluster)\n",
        "        # fra_batch = pool_batch(perm, data.batch)\n",
        "\n",
        "        # fra_x = self.fra_bn(global_add_pool(fra_x, fra_batch))\n",
        "        # features = torch.cat([mol_vec, fra_x], 1)\n",
        "        # features = F.dropout(features, p=0.3, training=self.training)\n",
        "        if self.regression:\n",
        "          features = self.lin(mol_vec)\n",
        "        else:\n",
        "          features = F.sigmoid(self.lin(mol_vec))\n",
        "        \n",
        "        return features"
      ],
      "metadata": {
        "id": "jitL6c-ugwnV"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GATV6(hidden_channels=64, dataset= train_dataset, regression= True)\n",
        "model = None"
      ],
      "metadata": {
        "id": "ylQmzKntkB0D"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GATV6(hidden_channels=64, dataset= train_dataset, regression= True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-03, weight_decay = 5e-03)\n",
        "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
        "                                                  max_lr = 1e-03, \n",
        "                                                  epochs = 250, \n",
        "                                                  steps_per_epoch = 29)\n",
        "criterion = torch.nn.MSELoss()\n",
        "model.to(device)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        data.to(device)\n",
        "        out = model(data)\n",
        "        # out = model(data.x, data.edge_index, data.batch)\n",
        "        out = out.squeeze()\n",
        "\n",
        "        loss = criterion(out, data.y)  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "        train_losses.append(loss.item())\n",
        "        lr_scheduler.step()\n",
        "    \n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            data.to(device)\n",
        "            out = model(data)\n",
        "            out = out.squeeze()\n",
        "            if out.shape[0] != data.y.shape[0]:\n",
        "                out = out[:data.y.shape[0]]\n",
        "\n",
        "            loss = criterion(out, data.y)\n",
        "            val_losses.append(loss.item())\n",
        "    \n",
        "    return np.mean(np.sqrt(train_losses)), np.mean(np.sqrt(val_losses))\n",
        "    \n",
        "\n",
        "\n",
        "for epoch in range(1, 250):\n",
        "    train_loss , valid_loss = train()\n",
        "    print(f'Epoch: {epoch} - Train RMSE: {train_loss:.3f}, Valid RMSE: {valid_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cmbRsi6ekoUS",
        "outputId": "2b80ea13-a634-414e-c98a-604d473a119f"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 - Train RMSE: 4.759, Valid RMSE: 3.538\n",
            "Epoch: 2 - Train RMSE: 4.198, Valid RMSE: 5.442\n",
            "Epoch: 3 - Train RMSE: 3.694, Valid RMSE: 6.503\n",
            "Epoch: 4 - Train RMSE: 3.394, Valid RMSE: 3.812\n",
            "Epoch: 5 - Train RMSE: 3.331, Valid RMSE: 3.766\n",
            "Epoch: 6 - Train RMSE: 3.012, Valid RMSE: 4.255\n",
            "Epoch: 7 - Train RMSE: 2.956, Valid RMSE: 5.538\n",
            "Epoch: 8 - Train RMSE: 2.930, Valid RMSE: 3.965\n",
            "Epoch: 9 - Train RMSE: 2.910, Valid RMSE: 4.105\n",
            "Epoch: 10 - Train RMSE: 2.977, Valid RMSE: 4.961\n",
            "Epoch: 11 - Train RMSE: 2.948, Valid RMSE: 3.003\n",
            "Epoch: 12 - Train RMSE: 2.898, Valid RMSE: 3.430\n",
            "Epoch: 13 - Train RMSE: 2.850, Valid RMSE: 5.759\n",
            "Epoch: 14 - Train RMSE: 2.780, Valid RMSE: 5.222\n",
            "Epoch: 15 - Train RMSE: 2.897, Valid RMSE: 3.442\n",
            "Epoch: 16 - Train RMSE: 2.733, Valid RMSE: 4.650\n",
            "Epoch: 17 - Train RMSE: 2.749, Valid RMSE: 3.209\n",
            "Epoch: 18 - Train RMSE: 2.740, Valid RMSE: 3.720\n",
            "Epoch: 19 - Train RMSE: 2.729, Valid RMSE: 3.061\n",
            "Epoch: 20 - Train RMSE: 2.704, Valid RMSE: 3.328\n",
            "Epoch: 21 - Train RMSE: 2.639, Valid RMSE: 3.698\n",
            "Epoch: 22 - Train RMSE: 2.622, Valid RMSE: 4.066\n",
            "Epoch: 23 - Train RMSE: 2.662, Valid RMSE: 2.866\n",
            "Epoch: 24 - Train RMSE: 2.596, Valid RMSE: 3.770\n",
            "Epoch: 25 - Train RMSE: 2.529, Valid RMSE: 3.491\n",
            "Epoch: 26 - Train RMSE: 2.574, Valid RMSE: 3.820\n",
            "Epoch: 27 - Train RMSE: 2.558, Valid RMSE: 4.436\n",
            "Epoch: 28 - Train RMSE: 2.548, Valid RMSE: 3.259\n",
            "Epoch: 29 - Train RMSE: 2.501, Valid RMSE: 3.296\n",
            "Epoch: 30 - Train RMSE: 2.493, Valid RMSE: 2.992\n",
            "Epoch: 31 - Train RMSE: 2.469, Valid RMSE: 3.856\n",
            "Epoch: 32 - Train RMSE: 2.458, Valid RMSE: 3.295\n",
            "Epoch: 33 - Train RMSE: 2.450, Valid RMSE: 3.310\n",
            "Epoch: 34 - Train RMSE: 2.487, Valid RMSE: 2.910\n",
            "Epoch: 35 - Train RMSE: 2.461, Valid RMSE: 2.915\n",
            "Epoch: 36 - Train RMSE: 2.448, Valid RMSE: 2.729\n",
            "Epoch: 37 - Train RMSE: 2.490, Valid RMSE: 2.569\n",
            "Epoch: 38 - Train RMSE: 2.504, Valid RMSE: 2.749\n",
            "Epoch: 39 - Train RMSE: 2.427, Valid RMSE: 2.693\n",
            "Epoch: 40 - Train RMSE: 2.403, Valid RMSE: 2.838\n",
            "Epoch: 41 - Train RMSE: 2.384, Valid RMSE: 2.783\n",
            "Epoch: 42 - Train RMSE: 2.397, Valid RMSE: 3.322\n",
            "Epoch: 43 - Train RMSE: 2.377, Valid RMSE: 2.809\n",
            "Epoch: 44 - Train RMSE: 2.392, Valid RMSE: 2.707\n",
            "Epoch: 45 - Train RMSE: 2.402, Valid RMSE: 2.672\n",
            "Epoch: 46 - Train RMSE: 2.352, Valid RMSE: 2.571\n",
            "Epoch: 47 - Train RMSE: 2.414, Valid RMSE: 2.534\n",
            "Epoch: 48 - Train RMSE: 2.386, Valid RMSE: 2.539\n",
            "Epoch: 49 - Train RMSE: 2.364, Valid RMSE: 2.552\n",
            "Epoch: 50 - Train RMSE: 2.346, Valid RMSE: 2.682\n",
            "Epoch: 51 - Train RMSE: 2.366, Valid RMSE: 2.431\n",
            "Epoch: 52 - Train RMSE: 2.370, Valid RMSE: 2.885\n",
            "Epoch: 53 - Train RMSE: 2.326, Valid RMSE: 2.906\n",
            "Epoch: 54 - Train RMSE: 2.387, Valid RMSE: 2.795\n",
            "Epoch: 55 - Train RMSE: 2.323, Valid RMSE: 2.677\n",
            "Epoch: 56 - Train RMSE: 2.303, Valid RMSE: 2.514\n",
            "Epoch: 57 - Train RMSE: 2.310, Valid RMSE: 2.617\n",
            "Epoch: 58 - Train RMSE: 2.333, Valid RMSE: 2.486\n",
            "Epoch: 59 - Train RMSE: 2.349, Valid RMSE: 2.516\n",
            "Epoch: 60 - Train RMSE: 2.359, Valid RMSE: 2.814\n",
            "Epoch: 61 - Train RMSE: 2.379, Valid RMSE: 2.428\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_17458/3153235180.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {epoch} - Train RMSE: {train_loss:.3f}, Valid RMSE: {valid_loss:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_17458/3153235180.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute the loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Derive gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update parameters based on gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Clear gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/virtenv/lib64/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/virtenv/lib64/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/virtenv/lib64/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/virtenv/lib64/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    250\u001b[0m                  \u001b[0mfused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fused'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                  \u001b[0mgrad_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                  found_inf=found_inf)\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/virtenv/lib64/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    314\u001b[0m          \u001b[0mdifferentiable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdifferentiable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m          \u001b[0mgrad_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m          found_inf=found_inf)\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/virtenv/lib64/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}