{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8od8r24z3nQ3NpaWPhx/U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranay8297/deep-learning-projects/blob/master/ChemBERTA_Arrakis_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZW0-7mhQiUF"
      },
      "outputs": [],
      "source": [
        "from ipdb import set_trace as st\n",
        "import sklearn\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "from torchmetrics import MeanAbsolutePercentageError\n",
        "from ipdb import set_trace as st\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from typing import List\n",
        "from deepchem.molnet import load_bbbp, load_clearance, load_clintox, load_delaney, load_hiv, load_qm7, load_tox21\n",
        "from rdkit import Chem\n",
        "from transformers import RobertaTokenizerFast\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import PreTrainedModel, RobertaModel\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from pytorch_metric_learning import losses\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.data import random_split\n",
        "from transformers import RobertaConfig, RobertaTokenizerFast, RobertaForMaskedLM\n",
        "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
        "from transformers.file_utils import ModelOutput\n",
        "from dataclasses import dataclass\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from nlp import load_dataset\n",
        "\n",
        "from transformers.data.data_collator import InputDataClass\n",
        "from transformers.tokenization_utils_base import BatchEncoding\n",
        "from transformers.trainer_callback import EarlyStoppingCallback\n",
        "\n",
        "from  torch.utils.data import random_split\n",
        "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
        "from bertviz import head_view, model_view\n",
        "TOKENIZERS_PARALLELISM = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smiles_tokenizer = RobertaTokenizerFast.from_pretrained(\"seyonec/SMILES_tokenized_PubChem_shard00_160k\", max_len = 512)\n",
        "\n",
        "class MyCustomException(Exception):\n",
        "    pass\n",
        "\n",
        "def multitask_data_collator(features: List[InputDataClass]) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Very simple data collator that simply collates batches of dict-like objects and performs special handling for potential keys named label\n",
        "    \"\"\"\n",
        "    if not isinstance(features[0], (dict, BatchEncoding)):\n",
        "        features = [vars(f) for f in features]\n",
        "\n",
        "    first = features[0]\n",
        "    batch = {}\n",
        "\n",
        "    if \"label\" in first and first[\"label\"] is not None:\n",
        "        batch[\"labels\"] = torch.stack([f[\"label\"] for f in features])\n",
        "\n",
        "    # Handling of all other possible keys.\n",
        "    # Again, we will use the first element to figure out which key/values are not None for this model.\n",
        "    for k, v in first.items():\n",
        "        if k != \"label\" and v is not None and not isinstance(v, str):\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                batch[k] = torch.stack([f[k] for f in features])\n",
        "            else:\n",
        "                batch[k] = torch.tensor([f[k] for f in features])\n",
        "\n",
        "    return batch\n",
        "\n",
        "def preprocess(line, tokenizer, block_size, text_name= 'SMILES', label_names = []):\n",
        "\n",
        "    def _clean_property(x):\n",
        "        return float(x)\n",
        "    # st()\n",
        "    smiles = line[text_name]\n",
        "    labels = [_clean_property(line[label_name]) for label_name in label_names]\n",
        "\n",
        "    batch_encoding = tokenizer(\n",
        "        smiles,\n",
        "        add_special_tokens = True,\n",
        "        truncation = True,\n",
        "        padding = \"max_length\",\n",
        "        max_length=block_size,\n",
        "    )\n",
        "    batch_encoding[\"label\"] = labels\n",
        "    batch_encoding = {k: torch.tensor(v) for k, v in batch_encoding.items()}\n",
        "\n",
        "    return batch_encoding\n",
        "\n",
        "def get_data_files(train_path):\n",
        "    if os.path.isdir(train_path):\n",
        "        return [\n",
        "            os.path.join(train_path, file_name) for file_name in os.listdir(train_path)\n",
        "        ]\n",
        "    elif os.path.isfile(train_path):\n",
        "        return train_path\n",
        "\n",
        "    raise ValueError(\"Please pass in a proper train path\")\n",
        "\n",
        "class RegressionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer, file_path: str, block_size = 515, x_col = None, y_col = []):\n",
        "        super().__init__()\n",
        "        print(\"init dataset\")\n",
        "        self.tokenizer = tokenizer\n",
        "        self.file_path = file_path\n",
        "        self.block_size = block_size\n",
        "\n",
        "        data_files = get_data_files(file_path)\n",
        "        self.dataset = load_dataset(\"csv\", data_files=data_files)[\"train\"]\n",
        "        dataset_columns = list(self.dataset.features.keys())\n",
        "        self.smiles_column = x_col if x_col else dataset_columns[-1]\n",
        "        self.label_columns = y_col if len(y_col) > 0 else dataset_columns[1:-1]\n",
        "        self.num_labels = len(self.label_columns)\n",
        "\n",
        "        print(\"Loaded Dataset\")\n",
        "        self.len = len(self.dataset)\n",
        "        print(\"Number of lines: \" + str(self.len))\n",
        "        print(\"Block size: \" + str(self.block_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        dd = preprocess(self.dataset[i], self.tokenizer, self.block_size, self.smiles_column, self.label_columns)\n",
        "        return dd\n",
        "\n",
        "class RobertaForRegression(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.num_outputs = config.num_outputs if config.num_outputs else 1\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.regression_heads = nn.ModuleList([RobertaRegressionHead(config) for i in range(self.num_outputs)])\n",
        "        self.loss_fct = MSELoss()\n",
        "        self.init_weights()\n",
        "\n",
        "        self.is_classification = True if config.is_classification else False\n",
        "\n",
        "\n",
        "        self.do_norm = False\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "\n",
        "        return_dict = (\n",
        "            return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        )\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = (\n",
        "            outputs.last_hidden_state\n",
        "        )\n",
        "\n",
        "        logits = [rh(sequence_output) for rh in self.regression_heads]\n",
        "\n",
        "        if labels is None:\n",
        "            return self.unnormalize_logits(logits)\n",
        "\n",
        "        if labels is not None:\n",
        "            normalized_labels = self.normalize_logits(labels)\n",
        "\n",
        "            loss = self.loss_fct(logits[0].squeeze(), normalized_labels[0].squeeze())\n",
        "            for i in range(normalized_labels.shape[1] - 1):\n",
        "              loss = loss + self.loss_fct(logits[i].squeeze(), normalized_labels[i].squeeze())\n",
        "\n",
        "            if not return_dict:\n",
        "                output = (logits,) + outputs[2:]\n",
        "                return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return RegressionOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "    def normalize_logits(self, tensor):\n",
        "        if self.do_norm:\n",
        "            return [(tensor[0] - self.mean) / self.std]\n",
        "        return tensor#(tensor - self.norm_mean) / self.norm_std\n",
        "\n",
        "    def unnormalize_logits(self, tensor):\n",
        "        if self.do_norm:\n",
        "            return [(tensor[0] * self.std) + self.mean]\n",
        "        return tensor\n",
        "\n",
        "    def freeze_unfreeze(self, action = 'f'):\n",
        "        # action can be either 'f' - Freeze or 'u' - Unfreeze\n",
        "        flag_to_set = False\n",
        "        if action == 'u':\n",
        "            flag_to_set = True\n",
        "        for param in self.roberta.parameters():\n",
        "            param.requires_grad = flag_to_set\n",
        "\n",
        "    def save_model(self, roberta_path, head_path = None):\n",
        "\n",
        "        torch.save(self.roberta.state_dict() , roberta_path)\n",
        "        if head_path:\n",
        "            torch.save(self.regression_heads.state_dict() , head_path)\n",
        "\n",
        "        print(f'Saved the model at : {roberta_path} and {head_path}')\n",
        "\n",
        "    def load_model(self, roberta_path, head_path = None):\n",
        "        # if there is name, load from appropriate path, if not name, check for path and load it. If not both, then throw an error\n",
        "        self.roberta.load_state_dict(torch.load(roberta_path))\n",
        "        if head_path:\n",
        "            self.regression_heads.load_state_dict(torch.load(head_path))\n",
        "\n",
        "        print(f'Sucesfully loaded model from : {roberta_path}, {head_path}')\n",
        "\n",
        "class RobertaRegressionHead(nn.Module):\n",
        "    \"\"\"Head for multitask regression models.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        self.is_classification = config.is_classification\n",
        "\n",
        "        super(RobertaRegressionHead, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "\n",
        "        if self.is_classification:\n",
        "            x = torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class RegressionOutput(ModelOutput):\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: torch.FloatTensor = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "\n",
        "def train_regression(model, train_dataloader, val_dataloader, learning_rate, epochs, wd = 1e-02):\n",
        "\n",
        "    hyperparameters = {\n",
        "        \"batch_size\": train_dataloader.batch_size,\n",
        "        \"num_epochs\": epochs,\n",
        "    }\n",
        "    wandb.config.update(hyperparameters)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion_1 = nn.MSELoss()\n",
        "    optimizer = AdamW(model.parameters(), lr = learning_rate, weight_decay = wd)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = learning_rate, steps_per_epoch = len(train_dataloader), epochs = epochs)\n",
        "\n",
        "    if use_cuda:\n",
        "        criterion_1 = criterion_1.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "        total_loss_train = []\n",
        "        c = 0\n",
        "        for dd in tqdm(train_dataloader):\n",
        "            try:\n",
        "                mask = dd['attention_mask']\n",
        "                input_id = dd['input_ids']\n",
        "                train_label = dd['label'].float()\n",
        "\n",
        "                if(input_id.shape[0] != train_dataloader.batch_size):\n",
        "                    continue\n",
        "\n",
        "                if use_cuda:\n",
        "                    input_id = input_id.to(device)\n",
        "                    train_label = train_label.to(device)\n",
        "                outputs = model(input_id)\n",
        "                # st()\n",
        "                loss = criterion_1(outputs[0].squeeze(), train_label[:, 0].squeeze())\n",
        "                for i in range(train_label.shape[1] - 1):\n",
        "                  loss = loss + criterion_1(outputs[i].squeeze(), train_label[:, i].squeeze())\n",
        "                # outputs = torch.stack(outputs, dim = 1).squeeze()\n",
        "                # loss = criterion_1(outputs, train_label)\n",
        "                total_loss_train.append(loss.item())\n",
        "                c += 1\n",
        "                if (c%10 == 0):\n",
        "                    wandb.log({\n",
        "                      \"Train Loss Running Mean\": np.mean(total_loss_train)\n",
        "                    })\n",
        "\n",
        "                model.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "            except Exception as e:\n",
        "                st()\n",
        "                print(e)\n",
        "\n",
        "\n",
        "\n",
        "        total_loss_val = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            c = 0\n",
        "            for vdd in val_dataloader:\n",
        "                try:\n",
        "                    if use_cuda:\n",
        "\n",
        "                        val_label = vdd['label'].float()\n",
        "                        val_label = val_label.to(device)\n",
        "                        input_id = vdd['input_ids'].to(device)\n",
        "\n",
        "                    if(input_id.shape[0] != val_dataloader.batch_size):\n",
        "                        continue\n",
        "\n",
        "                    outputs = model(input_id)\n",
        "\n",
        "                    loss = criterion_1(outputs[0].squeeze(), val_label[:, 0].squeeze())\n",
        "                    for i in range(train_label.shape[1] - 1):\n",
        "                        loss = loss + criterion_1(outputs[i].squeeze(), val_label[:, i].squeeze())\n",
        "\n",
        "                    # outputs = torch.stack(outputs, dim = 1).squeeze()\n",
        "                    # loss = criterion_1(outputs, val_label)\n",
        "                    c += 1\n",
        "                    total_loss_val.append(loss.item())\n",
        "                    if (c%3 == 0):\n",
        "                        wandb.log({\n",
        "                          \"Validation Loss Running Mean\": np.mean(total_loss_val)\n",
        "                        })\n",
        "                except Exception as e:\n",
        "                    st()\n",
        "                    print(e)\n",
        "\n",
        "\n",
        "\n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Train Loss: {np.mean(total_loss_train): .3f} \\\n",
        "            | Val Loss: {np.mean(total_loss_val): .3f}')\n",
        "\n",
        "def train_classification(model, train_dataloader, val_dataloader, learning_rate, epochs, wd = 1e-02):\n",
        "\n",
        "    hyperparameters = {\n",
        "        \"batch_size\": train_dataloader.batch_size,\n",
        "        \"num_epochs\": epochs,\n",
        "    }\n",
        "    wandb.config.update(hyperparameters)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    if model.device != device:\n",
        "        model.to(device)\n",
        "    criterion_1 = nn.BCELoss()\n",
        "    optimizer = AdamW(model.parameters(), lr = learning_rate, weight_decay = wd)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = learning_rate, steps_per_epoch = len(train_dataloader), epochs = epochs)\n",
        "\n",
        "    if use_cuda:\n",
        "        criterion_1 = criterion_1.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "        total_loss_train = []\n",
        "        c = 0\n",
        "        for dd in tqdm(train_dataloader):\n",
        "            try:\n",
        "                mask = dd['attention_mask']\n",
        "                input_id = dd['input_ids']\n",
        "                train_label = dd['label'].float()\n",
        "\n",
        "                if(input_id.shape[0] != train_dataloader.batch_size):\n",
        "                    continue\n",
        "\n",
        "                if use_cuda:\n",
        "                    input_id = input_id.to(device)\n",
        "                    train_label = train_label.to(device)\n",
        "                outputs = model(input_id)\n",
        "\n",
        "                loss = criterion_1(outputs[0].squeeze(), train_label[:, 0].squeeze())\n",
        "                for i in range(1, train_label.shape[1]):\n",
        "                  loss = loss + criterion_1(outputs[i].squeeze(), train_label[:, i].squeeze())\n",
        "\n",
        "                total_loss_train.append(loss.item())\n",
        "                c += 1\n",
        "                if (c%10 == 0):\n",
        "                    wandb.log({\n",
        "                      \"Train Loss Running Mean\": np.mean(total_loss_train)\n",
        "                    })\n",
        "\n",
        "                model.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "            except Exception as e:\n",
        "                st()\n",
        "                print(e)\n",
        "\n",
        "        total_loss_val = []\n",
        "        with torch.no_grad():\n",
        "            c = 0\n",
        "            for vdd in val_dataloader:\n",
        "                try:\n",
        "                    if use_cuda:\n",
        "\n",
        "                        val_label = vdd['label'].float()\n",
        "                        val_label = val_label.to(device)\n",
        "                        input_id = vdd['input_ids'].to(device)\n",
        "\n",
        "                    if(input_id.shape[0] != val_dataloader.batch_size):\n",
        "                        continue\n",
        "\n",
        "                    outputs = model(input_id)\n",
        "\n",
        "                    loss = criterion_1(outputs[0].squeeze(), val_label[:, 0].squeeze())\n",
        "                    for i in range(1, val_label.shape[1]):\n",
        "                        loss = loss + criterion_1(outputs[i].squeeze(), val_label[:, i].squeeze())\n",
        "\n",
        "                    c += 1\n",
        "                    total_loss_val.append(loss.item())\n",
        "                    if (c%3 == 0):\n",
        "                        wandb.log({\n",
        "                          \"Validation Loss Running Mean\": np.mean(total_loss_val)\n",
        "                        })\n",
        "                except Exception as e:\n",
        "                    st()\n",
        "                    print(e)\n",
        "\n",
        "\n",
        "\n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Train Loss: {np.mean(total_loss_train): .3f} \\\n",
        "            | Val Loss: {np.mean(total_loss_val): .3f}')"
      ],
      "metadata": {
        "id": "iDa7SbanREHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Functions and classes for using this models.**"
      ],
      "metadata": {
        "id": "XlTBTiMnRVAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project = \"Deleney Test\")"
      ],
      "metadata": {
        "id": "oumiKbMPRu0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_outputs = 1, is_classification = False):\n",
        "  config = RobertaConfig(\n",
        "        vocab_size = 600,m\n",
        "        max_position_embeddings = 515,\n",
        "        num_attention_heads = 6,\n",
        "        num_hidden_layers = 6,\n",
        "        type_vocab_size = 515,\n",
        "        num_labels = 1,\n",
        "        is_gpu = True,\n",
        "        num_outputs = num_outputs,\n",
        "        position_embedding_type = 'random',\n",
        "        is_classification = is_classification\n",
        "    )\n",
        "  model = RobertaForRegression(config)\n",
        "  return model\n",
        "\n",
        "def load_model(model, roberta_path, head_path):\n",
        "  model.load_model(roberta_path, head_path)\n",
        "  return model\n",
        "\n",
        "def tokenize_smiles(smiles_string):\n",
        "  data = smiles_tokenizer(\n",
        "        smiles_string,\n",
        "        add_special_tokens = True,\n",
        "        truncation = True,\n",
        "        padding = \"max_length\",\n",
        "        max_length = 515,\n",
        "    )\n",
        "  return data['input_ids']\n",
        "\n",
        "# Use batch_size as 2 if you are on local machine. On GPU server, you can use batch_size up to 32\n",
        "def prepare_batch_loader(smiles_list = [], batch_size = 16):\n",
        "\n",
        "  tokenized_smiles = [tokenize_smiles(i) for i in smiles_list]\n",
        "  ds = torch.Tensor(tokenized_smiles).long()\n",
        "  return DataLoader(ds, batch_size = batch_size)\n",
        "\n",
        "def predict(model, dl):\n",
        "  out = torch.Tensor()\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for x in dl:\n",
        "      if model.device != x.device:\n",
        "        x = x.to(model.device)\n",
        "      outputs = model(x)\n",
        "      yhat = outputs[0].squeeze()\n",
        "      yhat = yhat.to(torch.device('cpu'))\n",
        "      out = torch.cat([out, yhat], dim = -1)\n",
        "  return out.numpy()"
      ],
      "metadata": {
        "id": "LD7AcJcWRP8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Model\n",
        "model = create_model(num_outputs = 1, is_classification = False)\n",
        "\n",
        "# Load pre trained model\n",
        "model = load_model(model, 'models/delaney_finetuned_base.pth', 'models/delaney_finetuned_heads_v2.pth')\n",
        "\n",
        "# Get a smiles_list - Example Smiles String list below.\n",
        "smiles_list = ['ClCC(Cl)(Cl)Cl','CC(Cl)(Cl)Cl', 'ClC(Cl)C(Cl)Cl', 'ClCC(Cl)Cl', 'FC(F)(Cl)C(F)(Cl)Cl', 'CC(Cl)Cl', 'ClC(=C)Cl', 'CCOC(C)OCC', 'BrCCBr', 'ClCCCl', 'CC(Cl)CCl', 'FC(F)(Cl)C(F)(F)Cl', 'CCOCCOCC', 'C=CC=C', 'ClCCCCl', 'CCNC(=S)NCC', 'C=CCC=C', 'C=CCCC=C', 'CC(C)CBr', 'CCCCBr', 'CCCCCCCBr', 'CCCCCCBr', 'CCCCCCCCBr', 'CCCCCBr', 'CCCBr', 'CCCCO', 'CCC=C', 'CCC#C', 'ClCCBr', 'ClCC(C)C', 'CCCCCl', 'CCCCCCCCl', 'CCCCCCCl', 'CCCCCCl', 'CCCCl', 'CCCCCCCCCCO', 'CCCCCCCCC=C', 'CCCCCCCCCCCCO', 'CCCCCCCO', 'CCCCCC=C', 'CCCCCC#C', 'CCCCCCCCCCCCCCCCO', 'CCCCCCO', 'CCCCC=C', 'CCCC(O)C=C', 'CCCCC#C', 'CCCCI', 'CCCCCCCI', 'CCCI', 'CCCN(=O)=O', 'CCCCCCCCCO', 'CCCCCCCC=C', 'CCCCCCCC#C', 'CCCCCCCCCCCCCCCCCCO', 'CCCCCCCCO']\n",
        "\n",
        "# Prepare a Batch\n",
        "batch_loader = prepare_batch_loader(smiles_list = smiles_list)\n",
        "\n",
        "# check if GPU is available and if available, load the model to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Get Predictions\n",
        "predictions = predict(model = model, dl = batch_loader)"
      ],
      "metadata": {
        "id": "TFFTGh9tRiME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Model on a new Dataset** - And required functions."
      ],
      "metadata": {
        "id": "j8b-ecALRyB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to prepare train and valid data loaders. Please use batch size of 32 only if you are running on server, else use batch_size = 4. Training is not recommended on local machines.\n",
        "def get_data_loaders(f_path, smiles_col, label_col, batch_size = 32):\n",
        "  ds = RegressionDataset(smiles_tokenizer, file_path = f_path, block_size = 515, x_col = smiles_col, y_col = [label_col])\n",
        "\n",
        "  train_ds, valid_ds = random_split(ds, lengths= (int(len(ds)*0.8), len(ds) - int(len(ds)*0.8)))\n",
        "  train_loader = DataLoader(train_ds, batch_size = 32, shuffle = True)\n",
        "  valid_loader = DataLoader(valid_ds, batch_size = 32, shuffle = True)\n",
        "  return train_loader, valid_loader\n",
        "\n",
        "# This\n",
        "def train_one_epoch(model, train_dataloader, val_dataloader, criterion_1, optimizer, scheduler, device, is_classification = False):\n",
        "\n",
        "    total_loss_train = []\n",
        "    rmse_train = []\n",
        "    rmse_valid = []\n",
        "    c = 0\n",
        "    for dd in tqdm(train_dataloader):\n",
        "        try:\n",
        "            mask = dd['attention_mask']\n",
        "            input_id = dd['input_ids']\n",
        "            train_label = dd['label'].float()\n",
        "\n",
        "            if(input_id.shape[0] != train_dataloader.batch_size):\n",
        "                continue\n",
        "\n",
        "            input_id = input_id.to(device)\n",
        "            train_label = train_label.to(device)\n",
        "            outputs = model(input_id)\n",
        "            # st()\n",
        "            loss = criterion_1(outputs[0].squeeze(), train_label[:, 0].squeeze())\n",
        "            for i in range(train_label.shape[1] - 1):\n",
        "              loss = loss + criterion_1(outputs[i].squeeze(), train_label[:, i].squeeze())\n",
        "            # outputs = torch.stack(outputs, dim = 1).squeeze()\n",
        "            # loss = criterion_1(outputs, train_label)\n",
        "            total_loss_train.append(loss.item())\n",
        "            if is_classification:\n",
        "                # Calculate ROC_AUC\n",
        "                rmse_train.append(roc_auc(train_label[:, 0].to(torch.device('cpu').numpy()), outputs[0].squeeze().detach().to(torch.device('cpu')).numpy()))\n",
        "            else:\n",
        "                rmse_train.append(np.sqrt(loss.item()))\n",
        "\n",
        "            c += 1\n",
        "            if (c%10 == 0):\n",
        "                wandb.log({\n",
        "                  \"Train Loss Running Mean\": np.mean(total_loss_train)\n",
        "                })\n",
        "\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    total_loss_val = []\n",
        "    with torch.no_grad():\n",
        "        c = 0\n",
        "        for vdd in val_dataloader:\n",
        "            try:\n",
        "                val_label = vdd['label'].float()\n",
        "                val_label = val_label.to(device)\n",
        "                input_id = vdd['input_ids'].to(device)\n",
        "\n",
        "                if(input_id.shape[0] != val_dataloader.batch_size):\n",
        "                    continue\n",
        "\n",
        "                outputs = model(input_id)\n",
        "\n",
        "                loss = criterion_1(outputs[0].squeeze(), val_label[:, 0].squeeze())\n",
        "                for i in range(train_label.shape[1] - 1):\n",
        "                    loss = loss + criterion_1(outputs[i].squeeze(), val_label[:, i].squeeze())\n",
        "\n",
        "                # outputs = torch.stack(outputs, dim = 1).squeeze()\n",
        "                # loss = criterion_1(outputs, val_label)\n",
        "                c += 1\n",
        "                total_loss_val.append(loss.item())\n",
        "\n",
        "                if is_classification:\n",
        "                    # Calculate ROC_AUC\n",
        "                    rmse_valid.append(roc_auc(val_label[:, 0].to(torch.device('cpu').numpy()), outputs[0].squeeze().detach().to(torch.device('cpu')).numpy()))\n",
        "                else:\n",
        "                    rmse_valid.append(np.sqrt(loss.item()))\n",
        "                if (c%3 == 0):\n",
        "                    wandb.log({\n",
        "                      \"Validation Loss Running Mean\": np.mean(total_loss_val)\n",
        "                    })\n",
        "                scheduler.step(np.mean(total_loss_val))\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "    return np.mean(total_loss_train), np.mean(total_loss_val), np.mean(rmse_train), np.mean(rmse_valid)\n",
        "\n",
        "def train_regression_v2(model, train_dataloader, val_dataloader, learning_rate = 1e-04, epochs = 50, wd = 1e-02):\n",
        "\n",
        "    hyperparameters = {\n",
        "        \"batch_size\": train_dataloader.batch_size,\n",
        "        \"num_epochs\": epochs,\n",
        "    }\n",
        "    wandb.config.update(hyperparameters)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    if model.device != device:\n",
        "        model.to(device)\n",
        "\n",
        "    criterion_1 = nn.MSELoss()\n",
        "    optimizer = AdamW(model.parameters(), lr = learning_rate, weight_decay = wd)\n",
        "    # scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = learning_rate, steps_per_epoch = len(train_dataloader), epochs = epochs)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                            optimizer,\n",
        "                            mode = 'min',\n",
        "                            factor = 0.7,\n",
        "                            patience = 10,\n",
        "                            min_lr = 1e-04\n",
        "                )\n",
        "\n",
        "    if use_cuda:\n",
        "        criterion_1 = criterion_1.cuda()\n",
        "\n",
        "    model.freeze_unfreeze()\n",
        "    for epoch_num in range(5):\n",
        "\n",
        "        train_loss, valid_loss, train_rmse, valid_rmse = train_one_epoch(model, train_loader, valid_loader, criterion_1, optimizer, scheduler, device)\n",
        "\n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Train Loss: {train_loss: .3f} \\\n",
        "            | Val Loss: {valid_loss: .3f} \\\n",
        "            | Train RMSE: {train_rmse: .3f} \\\n",
        "            | Val RMSE: {valid_rmse: .3f}')\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                            optimizer,\n",
        "                            mode = 'min',\n",
        "                            factor = 0.7,\n",
        "                            patience = 10,\n",
        "                            min_lr = 1e-06\n",
        "                )\n",
        "    model.freeze_unfreeze('u')\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "        train_loss, valid_loss, train_rmse, valid_rmse = train_one_epoch(model, train_loader, valid_loader, criterion_1, optimizer, scheduler, device)\n",
        "\n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Train Loss: {train_loss: .3f} \\\n",
        "            | Val Loss: {valid_loss: .3f} \\\n",
        "            | Train RMSE: {train_rmse: .3f} \\\n",
        "            | Val RMSE: {valid_rmse: .3f}')\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_classification_v2(model, train_dataloader, val_dataloader, learning_rate = 1e-04, epochs = 50, wd = 1e-02):\n",
        "\n",
        "    hyperparameters = {\n",
        "        \"batch_size\": train_dataloader.batch_size,\n",
        "        \"num_epochs\": epochs,\n",
        "    }\n",
        "    wandb.config.update(hyperparameters)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    if model.device != device:\n",
        "        model.to(device)\n",
        "\n",
        "    criterion_1 = nn.BCELoss()\n",
        "    optimizer = AdamW(model.parameters(), lr = learning_rate, weight_decay = wd)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                            optimizer,\n",
        "                            mode = 'min',\n",
        "                            factor = 0.7,\n",
        "                            patience = 10,\n",
        "                            min_lr = 1e-04\n",
        "                )\n",
        "\n",
        "    if use_cuda:\n",
        "        criterion_1 = criterion_1.cuda()\n",
        "    model.freeze_unfreeze()\n",
        "\n",
        "    for epoch_num in range(5):\n",
        "\n",
        "        train_loss, valid_loss, train_roc_auc, valid_roc_auc = train_one_epoch(model, train_loader, valid_loader, criterion_1, optimizer, scheduler, device, is_classification = True)\n",
        "\n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Train Loss: {train_loss: .3f} \\\n",
        "            | Val Loss: {valid_loss: .3f} \\\n",
        "            | Train ROC AUC: {train_roc_auc: .3f} \\\n",
        "            | Val ROC AUC: {valid_roc_auc: .3f}')\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                            optimizer,\n",
        "                            mode = 'min',\n",
        "                            factor = 0.7,\n",
        "                            patience = 10,\n",
        "                            min_lr = 1e-06\n",
        "                )\n",
        "    model.freeze_unfreeze('u')\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "        train_loss, valid_loss, train_roc_auc, valid_roc_auc = train_one_epoch(model, train_loader, valid_loader, criterion_1, optimizer, scheduler, device, is_classification = True)\n",
        "\n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Train Loss: {train_loss: .3f} \\\n",
        "            | Val Loss: {valid_loss: .3f} \\\n",
        "            | Train ROC AUC: {train_rmse: .3f} \\\n",
        "            | Val ROC AUC: {valid_rmse: .3f}')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Nd00xwAzRw3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loader objects\n",
        "train_loader, valid_loader = get_data_loaders(\"./chem_datasets/dataset-delaney.csv\", 'SMILES', 'measured log(solubility:mol/L)')\n",
        "\n",
        "# Create a model\n",
        "model = create_model()\n",
        "\n",
        "# Train the model\n",
        "model = train_regression_v2(model, train_loader, valid_loader)\n",
        "\n",
        "# save the model, update the path here.\n",
        "model.save_model('./models/deleney_roberta_06_14.pth', './models/deleney_head_06_14.pth')"
      ],
      "metadata": {
        "id": "g4bhUzhwScur"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}